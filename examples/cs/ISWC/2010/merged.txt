scientists produce a large amount of quantitative  data consisting of measurements ranging from the surface temperatures of an ocean to the viscosity of a sample of mayonnaise
governmental agencies produce a large amount of quantitative  data consisting of measurements ranging from the surface temperatures of an ocean to the viscosity of a sample of mayonnaise
companies produce a large amount of research  data consisting of measurements ranging from the surface temperatures of an ocean to the viscosity of a sample of mayonnaise
companies produce a large amount of quantitative  data consisting of measurements ranging from the surface temperatures of an ocean to the viscosity of a sample of mayonnaise
scientists produce a large amount of research  data consisting of measurements ranging from the surface temperatures of an ocean to the viscosity of a sample of mayonnaise
governmental agencies produce a large amount of research  data consisting of measurements ranging from the surface temperatures of an ocean to the viscosity of a sample of mayonnaise
such measurements are stored in tables in spreadsheet files
such measurements are stored in tables in research reports
to integrate such data it is necessary to have a semantic description of the data
to reuse such data it is necessary to have a semantic description of the data
the notation used
however the notation is often ambiguous making automatic interpretation and conversion to other suitable format difficult
however the notation is often ambiguous making automatic interpretation and conversion to rdf difficult
for example the table header cell  f  hz   refers to frequency the symbol  f  can also refer to the quantities force
for example the table header cell  f  hz   refers to frequency the symbol  f  can also refer to the unit farad
for example the table header cell  f  hz   refers to frequency the symbol  f  can also refer to luminous flux
for example the table header cell  f  hz   refers to frequency measured in hertz
current annotation tools for this task perform a more limited task
current annotation tools for this task either work on less ambiguous data
we introduce new disambiguation strategies based on an ontology
an ontology which allows to improve performance on  sloppy  datasets not yet targeted by existing systemsthe ontology underlying the source dataset in the linked data cloud
building applications over linked data often requires the ontology
building applications over linked data often requires a mapping between the application model
a mapping between the application model can be defined in many ways
a mapping between the ontology can be defined in many ways
the ontology underlying the source dataset in the linked data cloud
for instance by inference rules that infer the application model from the source dataset
for instance by describing the application model as a view over the source dataset by giving mappings in the form of dependencies between the two datasets that infer the application model from the source dataset
explicitly formulating these mappings demands a comprehensive understanding of rdf ontologies  of the target datasets
explicitly formulating these mappings demands a comprehensive understanding of the underlying schemas  of the source datasets
explicitly formulating these mappings demands a comprehensive understanding of rdf ontologies  of the source datasets
explicitly formulating these mappings demands a comprehensive understanding of the underlying schemas  of the target datasets
this task can help the application designer with finding the implicit relationships that the application designer wants to map
this task can be supported by integrating the process of schema exploration into the mapping process
this paper describes fusion a framework for closing the underlying ontologies in the linked data cloud
this paper describes fusion a framework for closing the gap between the application model
a visual user interface that integrates the exploratory process
fusion simplifies the definition of mappings by providing a visual user interface
a visual user interface that integrates the mapping process
the mapping process architecture allows the creation of new applications through the extension of existing linked data with additional datathe web of data currently coming into existence through the linked open data effort is a major milestone in realizing the the web of data currently coming into existence through the linked open data effort vision
the web of data currently coming into existence through the linked open data effort is a major milestone in realizing the the web of data currently coming into existence through the linked open data effort vision
applications based on linked open data
however the development of applications faces difficulties due to the fact that the different linked open data datasets are rather loosely connected pieces of information
in particular
schemalevel information is being ignored
links between linked open data datasets are almost exclusively on the level of instances
in this paper we therefore present a system for finding schemalevel links between linked open data datasets in the sense of ontology alignment
our system called blooms
our system is based on the idea of bootstrapping information already present on the linked open data cloud
our also present a comprehensive evaluation
present a comprehensive evaluation which shows that blooms outperforms stateoftheart ontology alignment systems on linked open data datasets
at the same time blooms is also competitive compared with these other systems on the ontology evaluation alignment initiative benchmark datasetswe extend we recent work on evaluating incomplete reasoners by introducing strict testing bases
we show how they can be used in practice to identify queries where applications can exploit highly scalable incomplete query answering systems while enjoying completeness guarantees normally available only when using computationally intensive reasoning systems
we show how they can be used in practice to identify ontologies where applications can exploit highly scalable incomplete query answering systems while enjoying completeness guarantees normally available only when using computationally intensive reasoning systemsin linked data the use of owl is ubiquitous in interlinking datasets
in linked data the use of sameas is ubiquitous in interlinking datasets
there is however ongoing discussion about owl sameas use
there is however ongoing discussion about owl potential misuse particularly with regards to interactions with inference
in fact owl can be viewed as encoding only one point on a scale of one 
in fact owl can be viewed as encoding only one point on a scale of similarity 
in fact owl can be viewed as encoding only one point on a scale of sameas current uses
in fact sameas can be viewed as encoding only one point on a scale of similarity 
in fact sameas can be viewed as encoding only one point on a scale of sameas current uses
one that is often too strong for many of owl
in fact sameas can be viewed as encoding only one point on a scale of one 
referentially opaque contexts that do not allow inference
we describe how referentially opaque contexts exist
outline some varieties of referentiallyopaque alternatives to owl
outline sameas
finally we report on an empirical experiment over sameas statements from the web of data
finally we report on an empirical experiment over randomly selected owl 
this theoretical apparatus she would light upon how sameas is being used  and misused  on the web of data
experiment she would light upon how owl is being used  and misused  on the web of data
experiment she would light upon how sameas is being used  and misused  on the web of data
this theoretical apparatus she would light upon how owl is being used  and misused  on the web of datamuch of the research on automated web service composition relates the research on automated web service composition automated web service composition to an ai planning task
an ai planning task where automated web service composition automated web service composition is primarily done offline prior to execution
recent research on automated web service composition has argued convincingly for the importance of optimizing quality of user preferences
recent research on automated web service composition has argued convincingly for the importance of optimizing quality of service
recent research on automated web service composition has argued convincingly for the importance of optimizing quality of trust
while some of this optimization can be done offline many useful optimizations are datadependent
while some of this optimization can be done offline many interesting optimizations are datadependent
while some of this optimization can be done offline many interesting optimizations must be done following execution of at least some informationgathering services
while some of this optimization can be done offline many useful optimizations must be done following execution of at least some informationgathering services
in this paper we examine this class of automated web service composition problems attempting to balance the tradeoff between online information gathering with a view to producing highquality compositions efficiently gathering
in this paper we examine this class of automated web service composition problems attempting to balance the tradeoff between offline composition with a view to producing without excessive data gathering
in this paper we examine this class of automated web service composition problems attempting to balance the tradeoff between offline composition with a view to producing highquality compositions efficiently gathering
in this paper we examine this class of automated web service composition problems attempting to balance the tradeoff between online information gathering with a view to producing without excessive data gathering
we investigation is performed in the context of the semantic web
the semantic web employing an existing preferencebased hierarchical task network automated web service composition system
composition generation afforded by we
we experiments illustrate the potential improvement in both the quality and speed of composition generation approachmany applications make use of named entity classification
many named entity classification methods where the choice of features is critical to final performance
the preferred technique adopted for many named entity classification methods
machine learning is the preferred technique
existing approaches explore only the features the named entity linguistic context
only the features derived from the characteristic of the named entity
existing approaches explore only the features the named entity linguistic context
with the development of the semantic web a large number of data sources are published across the web as linked open data
with the development of the semantic web a large number of data sources are connected across the web as linked open data
knowledge that can be a valuable asset when used in connection with named entity classification
linked open data provides rich a priori knowledge about entity type information knowledge
in this paper we explore the use of linked open data to enhance named entity classification
we builds pair
we method extracts information from linked open data
a type knowledge base which is used to score a
we builds a type knowledge base  named entity string type 
this score is then injected as one features into the existing classifier in order to improve this score performance
this score is then injected as more features into the existing classifier in order to improve this score performance
the results which confirm the effectiveness of we proposed method
we report the results
we conducted a thorough experimental studyontologies are often collaboratively developed
ontologies are used for sharing information
ontologies are adapted for different applications and domains
an ontology that are caused by changes
different applications and domains resulting in multiple versions of an ontology
an ontology that are caused by refactorings
quite often ontology versions  are syntactical very different
quite often ontology versions  are semantically equivalent
quite often or parts of ontologies  are syntactical very different
quite often or parts of ontologies  are semantically equivalent
while there is existing work on detecting syntactical changes in ontologies there is still a need in recognizing ontology changes and refactorings by a semantically comparison of ontology versions
while there is existing work on detecting syntactical changes in ontologies there is still a need in analyzing ontology changes and refactorings by a semantically comparison of ontology versions
while there is existing work on detecting structural changes in ontologies there is still a need in recognizing ontology changes and refactorings by a semantically comparison of ontology versions
while there is existing work on detecting structural changes in ontologies there is still a need in analyzing ontology changes and refactorings by a semantically comparison of ontology versions
owl ontologies using dl reasoning to recognize such refactorings
in our approach our start with a classification of model refactorings found in software engineering for identifying such refactorings in owl ontologiesmillions of owl sameas statements have been published on the web of data
due to the web of heavy usage in linked data integration sameas has become a topic of increasing debate
due to the web of heavy usage in linked data integration owl has become a topic of increasing interest
due to the web of heavy usage in linked data integration owl has become a topic of increasing debate
due to the web of heavy usage in linked data integration sameas has become a topic of increasing interest
due to the web of data unique role sameas has become a topic of increasing debate
due to the web of data unique role owl has become a topic of increasing debate
due to the web of data unique role sameas has become a topic of increasing interest
due to the web of data unique role owl has become a topic of increasing interest
the web of data uses these statistics to focus discussion around the web of data usage in linked data
the web of data provides sameas deployment status
the web of data provides a quantitative analysis of owlin its core the semantic web is about the creation collection and interlinking of metadata on which agents can perform tasks for human users
while many tools and approaches support either the creation or usage of semantic metadata there is neither a related theory of guidance which metadata should be created
while many tools and approaches support either the creation or usage of semantic metadata there is neither a proper notion of metadata need
in this paper we propose to analyze structured queries to help identifying missing metadata
semantic mediawiki one of the most popular the semantic web applications to date analyzing structured  ask  queries in public semantic mediawiki instances
we conduct a study on semantic mediawiki one of the most popular the semantic web applications to date
based on that we describe semantic need an extension for semantic mediawiki which guides contributors to provide semantic annotations
based on that we describe semantic need an extension for semantic mediawiki which guides contributors to summarize feedback from an online survey among 30 experienced semantic mediawiki usersin this paper we discuss optimisations of rulebased materialisation approaches for reasoning over large static rdf datasets
we generalise what we call the  partialindexing  approach to the  partialindexing  approach to scalable rulebased materialisation is based on a separation of terminological data which has been shown in previous works to enable highly scalable in so doing we provide some completeness propositions with respect to seminaive evaluation
we reformalise what we call the  partialindexing  approach to the  partialindexing  approach to scalable rulebased materialisation is based on a separation of terminological data which has been shown in related works to enable highly scalable in so doing we provide some completeness propositions with respect to seminaive evaluation
we generalise what we call the  partialindexing  approach to scalable rulebased materialisation is based on a separation of terminological data which has been shown in previous works to enable highly scalable in so doing we provide some completeness propositions with respect to seminaive evaluation
we reformalise distributable reasoning for specific rulesets in so doing we provide some completeness propositions with respect to seminaive evaluation
we reformalise what we call the  partialindexing  approach to scalable rulebased materialisation is based on a separation of terminological data which has been shown in previous works to enable highly scalable in so doing we provide some completeness propositions with respect to seminaive evaluation
we reformalise what we call the  partialindexing  approach to the  partialindexing  approach to scalable rulebased materialisation is based on a separation of terminological data which has been shown in previous works to enable highly scalable in so doing we provide some completeness propositions with respect to seminaive evaluation
we reformalise what we call the  partialindexing  approach to scalable rulebased materialisation is based on a separation of terminological data which has been shown in related works to enable highly scalable in so doing we provide some completeness propositions with respect to seminaive evaluation
we generalise what we call the  partialindexing  approach to the  partialindexing  approach to scalable rulebased materialisation is based on a separation of terminological data which has been shown in related works to enable highly scalable in so doing we provide some completeness propositions with respect to seminaive evaluation
we generalise what we call the  partialindexing  approach to scalable rulebased materialisation is based on a separation of terminological data which has been shown in related works to enable highly scalable in so doing we provide some completeness propositions with respect to seminaive evaluation
we generalise distributable reasoning for specific rulesets in so doing we provide some completeness propositions with respect to seminaive evaluation
tboxspecific dynamic rulesets created by binding the terminological patterns in the static ruleset
we then show how related work on template rules tboxspecific dynamic rulesets can be optimised for the partialindexing approach
we then show how related work on template rules tboxspecific dynamic rulesets can be incorporated for the partialindexing approach
methods using owl horst 
methods using owl 2 rl
thereafter demonstrate pragmatic distributed reasoning over 112 billion linked data statements for a subset of owl 2 rlrdf rules we argue to be suitable for web reasoning
methods using pd 
we evaluate we methods
methods using lubm for rdfsthe rule interchange format production rule dialect is a w3c recommendation to define production rules for the semantic web
the semantic web whose semantics is defined operationally via labeled terminal transition systemsfinding an ontology
finding an ontology to replace another
measuring similarity between ontologies can be very useful for different purposes
an ontology in which queries can be translated
classical measures compute distances in an ontology space by directly comparing the content of ontologies
classical measures compute similarities in an ontology space by directly comparing the content of ontologies
ontology measures computed in an alignment space
ontology measures computed in an alignment space
ontology measures computed in an alignment space a new family of ontology measures evaluate the similarity between two ontologies with regard to the available alignments between a new family of ontology measures
we introduce a new family of ontology measures
we define two sets of such measures
such measures relying on the existence of a path between ontologies
the ontology entities that are preserved by the alignments
such measures relying on the ontology entities
the former accounts for known relations between ontologies while the latter reflects the possibility to perform actions such as instance import
the former accounts for known relations between ontologies while the latter reflects the possibility to perform actions such as query translation
the ontosim library that has been used in experiments
all these measures have been implemented in the ontosim library
experiments which showed that entity are comparable to the best ontology space measures
entity preserving measures
moreover all these measures showed a robust behaviour with respect to the alteration of the alignment spacetwitter enjoys enormous popularity as a microblogging service largely due to twitter simplicity
on the downside
everyone involved
messages passing through the system
making sense of the stream of messages has become a significant challenge for everyone
there is little organization to the twitterverse
as a solution twitter users have adopted the convention of adding a hash at the beginning of a word to turn a hash into a hashtag
hashtags have become the means in twitter to build communities around particular interests
hashtags have become the means in twitter to create threads of conversationwe study the problem of evolution for knowledge bases
knowledge bases expressed in description logics of the description logic
knowledge bases expressed in lite family
description logic lite is at the basis of owl 2 ql one of the tractable fragments of owl 2 
description logic lite is at the basis of owl 2 ql one of the tractable fragments of the recently proposed revision of the web ontology language
we propose some fundamental principles that knowledge base evolution should respect
we review formulabased approaches for evolution of propositional theories
we review known model for evolution of propositional theories
we exhibit limitations of a number of modelbased approaches besides the fact that a number of modelbased approaches are either hard to compute a number of modelbased approaches intrinsically ignore the structural properties of knowledge bases
we exhibit limitations of a number of modelbased approaches besides the fact that a number of modelbased approaches are either not expressible in lite a number of modelbased approaches intrinsically ignore the structural properties of knowledge bases
knowledge bases which leads to undesired properties of knowledge bases
we exhibit limitations of a number of modelbased approaches besides the fact that a number of modelbased approaches are either not expressible in description logic a number of modelbased approaches intrinsically ignore the structural properties of knowledge bases
knowledge bases resulting from such an evolution
we also examine proposals on revision of description logic knowledge bases
we also examine proposals on update
description logic knowledge bases that adopt the modelbased approaches
description logic knowledge bases that discuss the modelbased approaches drawbacks
we show that known formulabased approaches are also not appropriate for description logic
we show that known formulabased approaches are also due to high complexity of computation
we show that known formulabased approaches are also not appropriate for lite evolution
we show because the result of such an action of evolution is not expressible in lite
we show because the result of such an action of evolution is not expressible in description logic
building upon the insights gained we propose two novel formulabased approaches for which evolution is expressible in description logic
building upon the insights gained we propose two novel formulabased approaches that respect we principles
building upon the insights gained we propose two novel formulabased approaches for which evolution is lite
for we approaches we also developed polynomial time algorithms to compute lite knowledge bases
for we approaches we also developed polynomial time algorithms to compute evolution of description logic starting from the general framework for annotated rdfs which we presented in previous work  extending udrea et als annotated rdf  we address the development of a query language
several features of sparql 11
starting from the general framework for annotated rdfs which we presented in previous work  extending udrea et als annotated rdf  we address the development of a query language anql that is inspired by sparql
anql that is inspired by sparql
formal definitions of the semantics of these features  which could serve as a basis for the ongoing work in sparql 11
as a side effect we propose formal definitions of the semantics of subqueries aggregates assignment solution modifiers 
as a side effect we propose formal definitions of the semantics of these features 
formal definitions of the semantics of subqueries aggregates assignment solution modifiers  which could serve as a basis for the ongoing work in sparql 11
we demonstrate the value of such a framework by comparing we approach to previously proposed extensions of sparql
we demonstrate the value of such a framework by comparing we show that anql generalises and extends themareas that rely upon everyday users to create knowledge bases
debugging is an important prerequisite for the widespread application of ontologies especially in areas such as the semantic web
areas that rely upon everyday users to maintain knowledge bases
most recent approaches use diagnosis methods to identify sources of inconsistency
however in most debugging cases these methods return many alternative diagnoses thus placing the burden of fault localization on the user
the user demonstrates how the target diagnosis can be identified by performing a sequence of observations that is by querying an oracle about entailments of the target ontology
we exploit probabilities of typical user errors to formulate information theoretic concepts for query selection
we evaluation showed that the suggested method reduces the number of required observations compared to myopic strategiesfacilitating the seamless evolution of rdf knowledge bases on the semantic web presents still a major challenge
in this work we devise evopat a patternbased approach for the evolution and refactoring of knowledge bases
basic evolution patterns which can capture refactoring operations on both schema levels
the approach is based on the definition of basic evolution patterns
basic evolution patterns which are represented declaratively
basic evolution patterns which can capture simple evolution on both data
basic evolution patterns which can capture refactoring operations on both data
basic evolution patterns which can capture simple evolution on both schema levels
for more advanced several simple evolution patterns can be combined into a compound one
for domainspecific evolution several simple evolution patterns can be combined into a compound one
for refactorings several simple evolution patterns can be combined into a compound one
we performed a comprehensive survey of possible evolution patterns with a combinatorial analysis of all possible beforeafter combinations resulting in an extensive catalog of usable evolution patterns
we approach was implemented as an extension for framework
we approach was implemented as an extension for the ontowiki semantic collaboration platformincreasingly huge rdf data sets are being published on the web
currently the web contain high levels of redundancy
currently the web use different syntaxes of rdf
currently the web have a plain indivisible structure
all this leads to complex processing
all this leads to inefficient management
all this leads to lack of scalability
all this leads to fuzzy publications
a novel rdf representation which takes advantage of the structural properties of rdf graphs for splitting efficiently three components of rdf data header structure
a novel rdf representation which takes advantage of the structural properties of rdf graphs for splitting efficiently three components of rdf data triples structure
a novel rdf representation which takes advantage of the structural properties of rdf graphs representing efficiently three components of rdf data dictionary structure
a novel rdf representation which takes advantage of the structural properties of rdf graphs representing efficiently three components of rdf data header structure
a novel rdf representation which takes advantage of the structural properties of rdf graphs for splitting efficiently three components of rdf data dictionary structure
all this presents a novel rdf representation
a novel rdf representation which takes advantage of the structural properties of rdf graphs representing efficiently three components of rdf data triples structure
ondemand management operations can be implemented on top of a novel rdf representation representation
more than fifteen times the current naive representation improving parsing while keeping a consistent publication scheme
more than fifteen times the current naive representation improving processing while keeping a consistent publication scheme
experiments show that data sets can be compacted in a novel rdf representation by more than fifteen times the current naive representation
for exchanging specific compression techniques over a novel rdf representation improve current compression solutionssources using a term index
in order to effectively answer queries in environments with distributed rdfowl we present a query optimization algorithm to identify the potentially relevant semantic web data sources
in order to quickly answer queries in environments with distributed rdfowl we present a query optimization algorithm to identify the potentially relevant semantic web data sources
sources using structural query features
these two patterns treated independently
a query optimization algorithm is based on the observation that the join selectivity of a pair of query triple patterns is often higher than the overall selectivity of these two patterns
a rule goal tree that expresses the reformulation of a conjunctive query
given a rule goal tree a query optimization algorithm uses a bottomup approach to estimate the selectivity of each node
a query optimization algorithm then prioritizes loading of selective nodes and uses the information from these sources to further constrain other nodes
finally
the selected sources
we use an owl reasoner to answer queries over the selected sources
the selected sources corresponding ontologies
system using both a synthetic data set data
we have evaluated we system
system using a subset of the realworld billion triple challenge datasituations in which usage of certain entities enable the automatic evaluation whether a situation is compliant
situations in which usage of certain entities are allowed
formal policies allow the nonambiguous definition of situations
this is useful for example in applications using data provided via standardized interfaces
the low technical barriers of integrating such data sources is in contrast to the manual evaluation of natural language policies as natural language policies currently exist
policies which can be restricted by the policy of a used entity
usage situations can usage situations be regulated by policies
applications using the google maps api
consider for example the google maps api
the google maps api which requires that applications must be available without a fee the applications policy must not require a payment
a policy language that can express such constraints on other policies a selfpolicing policy language
in this paper we present a policy language
we validate we approach by realizing a use case scenario using a policy engine developed for our languagepolicies are declarations of constraints on the behaviour of components within distributed systems and are often used to capture norms within agentbased systems
a few machineprocessable representations for policies have been proposed
policies that can be limited by the complexity of associated reasoning mechanisms
policies that can be expressed by the complexity of associated reasoning mechanisms
a few machineprocessable representations for policies tend to be either limited in the types of policies
in this paper we argue for a language which enables both policy analysis within the bounds of decidability
in this paper we argue for a language which enables both policygoverned decisionmaking within the bounds of decidability
in this paper we argue for a language that sufficiently expresses the types of policies essential in practical systems
a reasoning mechanism that uses a novel combination of query answering
a reasoning mechanism that uses a novel combination of ontology consistency checking
we then propose an owlbased representation of a reasoning mechanism
these criteria using
we then propose an owlbased representation of policies
policies that meets these criteria
in this way agentbased systems can be developed that operate flexibly
in this way agentbased systems can be effectively in policyconstrainted environmentsthe web of data is increasingly becoming an important infrastructure for such diverse sectors as government
the web of data is increasingly becoming an important infrastructure for such diverse sectors as entertainment
the web of data is increasingly becoming an important infrastructure for such diverse sectors as science
the web of data is increasingly becoming an important infrastructure for such diverse sectors as ecommerce
as a result the robustness of the web of data is now crucial
prior studies show that the web of data is strongly dependent on a small number of central hubs making the web of data highly vulnerable to single points of failure
in this paper we present concepts to analyse the brittleness of the web of data
in this paper we present algorithms to analyse the brittleness of the web of data
in this paper we present algorithms to repair the brittleness of the web of data
in this paper we present concepts to repair the brittleness of the web of data
we apply these on a substantial subset of it the 2010 billion triple challenge dataset
we first distinguish the physical structure of the web of data from the web of data semantic structure
for both of these structures we then calculate both of these structures robustness
both of these structures robustness taking betweenness centrality as a robustnessmeasure
to the best of we knowledge this is the first time that such robustnessindicators have been calculated for the web of data
finally we determine which links should be added to the web of data in order to improve the web of data robustness most effectively
we are able to determine such links by deploying an evolutionary algorithm to solve a very large optimisation problem
we are able to determine such links by interpreting the question as a very large optimisation problem
we believe that with this work we offer an effective method to analyse the most important structure that the the web of data community has constructed to date
we believe that with this work we offer an effective method to improve the most important structure that the the web of data community has constructed to dateowl 2 rl was standardized as a less expressive subset of owl 2
owl 2 rl was standardized as a less scalable subset of owl 2
owl 2 that allows a forwardchaining implementation
2  efficiently update inference for additions remains a challenge
an enterprisescale forwardchaining based inference engine
however
building an enterprisescale forwardchaining that can 1  take advantage of modern multicore computer architectures
in this paper we present an owl 2 rl inference engine implemented inside the oracle database system using novel techniques for parallel processing
parallel processing that can readily scale on multicore machines and clusters
additionally we have added support for efficient incremental maintenance of the inferred graph after triple additions
a hybrid inmemorydisk based approach to efficiently compute compact equivalence closures
finally to handle sameas relationships present in semantic web datasets we have provided a hybrid inmemorydisk
finally to handle the increasing number of owl  we have provided a hybrid inmemorydisk
we have done extensive testing to evaluate these new techniques the test results demonstrate that we inference engine is capable of performing efficient inference over ontologies with billions of triples
triples using a modest hardware configurationblogging platforms remain primarily limited to text
several projects have brought rich data semantics to collaborative wikis
as blogs comprise a significant portion of the semantic web content engagement of the blogging community is crucial to the development of the semantic web
we provide a study of blog content to show a latent need for better data publishing in blogging software
we provide a study of blog content to show visualization support in blogging software
we then present datapress an extension to the wordpress blogging platform
the wordpress blogging platform that enables users to publish share aggregate and visualize structured information using the same workflow that users already apply to textbased content
in particular we aim to preserve those attributes
those attributes that make blogs such a successful publication medium from other sources
those attributes that make easy copy and paste of information  and visualizations  from other sources
those attributes that make oneclick publishing of the information from other sources
those attributes that make oneclick access to the information from other sources
those attributes that make oneclick publishing of natural authoring interfaces from other sources
we reflect on how we designs make progress toward these goals with a study of how users made use of various features
users who installed datapresseffective communication in open environments relies on the ability of agents to reach a mutual understanding of the exchanged message by reconciling the vocabulary
the vocabulary used
various approaches have considered how mutually acceptable mappings between corresponding concepts in the agents own ontologies may be determined dynamically through argumentationbased negotiation  such as meaningbased argumentation mba 
mutually acceptable mappings that allows agents to express a private acceptability threshold over the types of mappings agents
in this paper we present a novel approach to the dynamic determination of mutually acceptable mappings prefer
we empirically compare this approach with the meaningbased argumentation
we empirically demonstrate that the proposed approach produces larger agreed alignments thus better enabling agent communication
we compare the fitness for purpose of the generated alignments
furthermore
we empirically demonstrate that the proposed approach has comparable performance to the mba approach
we evaluate the fitness for purpose of the generated alignmentswe study the problem of sparql query optimization on top of distributed hash tables
existing works on sparql query processing in such environments have never been do not utilize any optimization techniques
existing works thus exhibit poor performance
existing works on sparql query processing in such environments have never been implemented in a real system
we goal in this paper is to propose efficient algorithms for optimizing sparql basic graph pattern queries
we goal in this paper is to propose scalable algorithms for optimizing sparql basic graph pattern queries
we augment a known distributed query processing algorithm with query optimization strategies
query optimization strategies that improve performance in terms of query response time
query optimization strategies that improve performance in terms of bandwidth usage
we implement we study we performance experimentally in a local cluster
we implement we techniques in the system atlas experimentally in a local clustersemantic web policy languages  provide fresh motivations for extending a topic
semantic web policy languages based on description logics
dls  provide fresh motivations for extending a topic
biomedical ontologies provide fresh motivations for extending dls with nonmonotonic inferences
dls  provide fresh motivations for extending dls with nonmonotonic inferences
a topic that has attracted a significant amount of attention along the years
biomedical ontologies provide fresh motivations for extending a topic
semantic web policy languages  provide fresh motivations for extending dls with nonmonotonic inferences
despite this nonmonotonic inferences are not yet supported by the existing dl engines
one reason is the high computational complexity of the existing decidable fragments of nonmonotonic dls
in this paper we identify a fragment of circumscribed el ot
circumscribed el ot that supports attribute inheritance with much like an objectoriented language  that reasoning about default attributes is in p
circumscribed el ot that supports attribute inheritance with specificitybased overriding  that reasoning about default attributes is in p
circumscribed el ot that supports attribute inheritance with such that reasoning about default attributes is in panalysing the performance of owl reasoners on expressive owl ontologies is an ongoing challenge
in this paper we present a new approach to performance analysis
performance analysis based on justifications for entailments of owl ontologies
justifications are minimal subsets of an ontology
an ontology that are commonly used to debug owl ontologies
an ontology that are sufficient for an entailment to hold
test which means that individual justifications are tested for reasoner performance instead of random subsets
test which means that individual justifications are tested for reasoner performance instead of entire ontologies
test which means that individual justifications are tested for correctness performance instead of entire ontologies
in justbench justifications form the key unit of test
test which means that individual justifications are tested for correctness performance instead of random subsets
justifications are generally small which makes justifications very suitable for transparent analytic microbenchmarks
justifications are relatively easy to analyse which makes justifications very suitable for transparent analytic microbenchmarks
furthermore the justbench approach also allows us to isolate reasoner errors
furthermore the justbench approach also allows us to isolate inconsistent behaviour
us present the results of initial experiments
the results of initial experiments using justbench with fact
the results of initial experiments using justbench with pellet
the results of initial experiments using justbench with hermit
finally us show how justbench can be used by reasoner developers
ontology engineers seeking to understand the performance characteristics of reasoners
ontology engineers seeking to improve the performance characteristics of ontologies
reasoner developers seeking to improve the performance characteristics of reasoners
reasoner developers seeking to understand the performance characteristics of reasoners
ontology engineers seeking to understand the performance characteristics of ontologies
reasoner developers seeking to improve the performance characteristics of ontologies
reasoner developers seeking to understand the performance characteristics of ontologies
finally us show how justbench can be used by ontology engineers
ontology engineers seeking to improve the performance characteristics of reasonerssystems based on statistical learning methods have been shown to be extremely effective and scalable for the analysis of large amount of textual data
systems based on machine learning methods have been shown to be extremely effective and scalable for the analysis of large amount of textual data
however in the recent years it becomes evident that one of the most important directions of improvement in natural language processing  nlp  tasks like coreference resolution is by exploiting semantics
however in the recent years it becomes evident that one of the most important directions of improvement in natural language processing  nlp  tasks like word sense disambiguation is by exploiting semantics
however in the recent years it becomes evident that one of the most important directions of improvement in natural language processing  nlp  tasks like other tasks is by exploiting semantics
other tasks related to knowledge extraction
however in the recent years it becomes evident that one of the most important directions of improvement in natural language processing  nlp  tasks like relation extraction is by exploiting semantics
ontologies rdf data  which constitutes a valuable source of semantics
while in the past the unavailability of rich semantic descriptions constituted a serious limitation of the unavailability of rich and complete semantic descriptions applicability nowadays the semantic web encoded information 
while in the past the unavailability of complete semantic descriptions constituted a serious limitation of the unavailability of rich and complete semantic descriptions applicability nowadays the semantic web encoded information 
while in the past the unavailability of complete semantic descriptions constituted a serious limitation of the unavailability of rich and complete semantic descriptions applicability nowadays the semantic web encoded ontologies rdf data 
the semantic web made available a large amount of logically
while in the past the unavailability of rich semantic descriptions constituted a serious limitation of the unavailability of rich and complete semantic descriptions applicability nowadays the semantic web encoded ontologies rdf data 
information  which constitutes a valuable source of semantics
ontologies rdf data linked data
however web semantics can not be easily plugged into machine learning systems
therefore the objective of this paper is to define a reference methodology for combining semantic information available in the web under the form of logical theories with statistical methods for nlp
the rules that combine knowledge
the major problems that we have to solve to implement we methodology concern the selection of the minimal knowledge among the large amount available in the web the representation of uncertain knowledge and the resolution of the rules retrieved from semantic web sources with semantics in the text
the major problems that we have to solve to implement we methodology concern the selection of the correct knowledge among the large amount available in the web the representation of uncertain knowledge and the encoding of the rules retrieved from semantic web sources with semantics in the text
the major problems that we have to solve to implement we methodology concern the selection of the minimal knowledge among the large amount available in the web the representation of uncertain knowledge and the encoding of the rules retrieved from semantic web sources with semantics in the text
the major problems that we have to solve to implement we methodology concern the selection of the correct knowledge among the large amount available in the web the representation of uncertain knowledge and the resolution of the rules retrieved from semantic web sources with semantics in the text
in order to evaluate our present an application of the methodology to the problem of intradocument coreference resolution
in order to evaluate the appropriateness of our approach
in order to evaluate our show by means of some experiments on the standard dataset
the standard dataset how the injection of knowledge leads to the improvement of this task performancea key problem in ontology alignment is that different ontological features  vary widely in lexical importance for different ontology comparisons
a key problem in ontology alignment is that different ontological features  vary widely in structural importance for different ontology comparisons
a key problem in ontology alignment is that semantic  vary widely in structural importance for different ontology comparisons
a key problem in ontology alignment is that lexical  vary widely in semantic importance for different ontology comparisons
a key problem in ontology alignment is that  vary widely in semantic importance for different ontology comparisons
a key problem in ontology alignment is that structural  vary widely in different ontological features importance for different ontology comparisons
a key problem in ontology alignment is that  vary widely in lexical importance for different ontology comparisons
a key problem in ontology alignment is that semantic  vary widely in semantic importance for different ontology comparisons
a key problem in ontology alignment is that structural  vary widely in structural importance for different ontology comparisons
a key problem in ontology alignment is that semantic  vary widely in lexical importance for different ontology comparisons
a key problem in ontology alignment is that structural  vary widely in lexical importance for different ontology comparisons
a key problem in ontology alignment is that different ontological features  vary widely in different ontological features importance for different ontology comparisons
a key problem in ontology alignment is that lexical  vary widely in different ontological features importance for different ontology comparisons
a key problem in ontology alignment is that semantic  vary widely in different ontological features importance for different ontology comparisons
a key problem in ontology alignment is that different ontological features  vary widely in semantic importance for different ontology comparisons
a key problem in ontology alignment is that  vary widely in different ontological features importance for different ontology comparisons
a key problem in ontology alignment is that lexical  vary widely in lexical importance for different ontology comparisons
a key problem in ontology alignment is that structural  vary widely in semantic importance for different ontology comparisons
a key problem in ontology alignment is that lexical  vary widely in structural importance for different ontology comparisons
a key problem in ontology alignment is that  vary widely in structural importance for different ontology comparisons
in this paper we present a set of principled techniques
principled techniques that exploit user feedback to customize the alignment process for a given pair of ontologies
specifically we propose an iterative supervisedlearning approach to determine the weights
the weights assigned to each alignment strategy
the weights assigned to each alignment strategy to combine each alignment strategy for matching ontology entities
specifically we propose an iterative supervisedlearning approach to use the weights
the weights assigned to each alignment strategy to determine the degree to which the information from such matches should be propagated to such matches neighbors along different relationships for collective matching
we demonstrate the utility of these techniques with standard benchmark datasets showing improvements in fscores of up to 70
we demonstrate the utility of these techniques with large realworld ontologies showing improvements in fscores of up to 70we develop query relaxation techniques for regular path queries
we combine regular path queries with query approximation in order to support flexible querying of rdf data where its full structure is irregular
we combine regular path queries with query approximation in order to support flexible querying of rdf data when the users lacks knowledge of the users full structure
in such circumstances it is helpful if the querying system can perform both approximate matching and relaxation of the users query and can rank the answers according to how closely the users match the original query
our framework incorporates both standard notions of approximation
approximation based on edit distance
approximation based on rdfsbased inference rules
querying paths using regular expressions
the query language we adopt comprises conjunctions of regular path queries thus including extensions
extensions proposed for sparql to allow for querying paths
we provide an incremental query evaluation algorithm
an incremental query evaluation algorithm which runs in polynomial time to the users in ranked order
an incremental query evaluation algorithm which runs in returns answers to the users in ranked orderdifferent sources using equivalence statements such as owl
different sources using equivalence statements such as other types of linked properties
the web of linked data is characterized by linking structured data from different sources
different sources using equivalence statements such as sameas
the ontologies behind different sources however remain unlinked
different sources using equivalence statements such as sameas
different sources using equivalence statements such as other types of linked properties
different sources using equivalence statements such as owl
this paper describes an extensional approach to generate alignments between the ontologies behind these sources
specifically our algorithm produces subsumption relationships between classes from ontologies of different linked data sources by exploring the space of hypotheses
specifically our algorithm produces equivalence relationships between classes from ontologies of different linked data sources by exploring the space of hypotheses
hypotheses supported by the existing equivalence statements
we are also able to generate new classes for a second source where the ontology is not as refined as the first
we are also able to generate a complementary hierarchy of derived classes within an existing ontology where the ontology is not as refined as the first
we demonstrate empirically we approach using linked data sources from the geospatial
we demonstrate empirically we approach using linked data sources from genetics
we demonstrate empirically we approach using linked data sources from zoology domains
our algorithm discovered about 29000 subset relationships in the alignment of five source pairs from these domains
our algorithm discovered about 800 equivalences in the alignment of five source pairs from these domains
thus we are able to understand the semantic relationships between the two sources
thus we are able to model one linked data source in terms of another by aligning linked data ontologiesontological metamodeling has a variety of applications yet only very restricted forms are supported by owl 2 directly
a novel encoding scheme enabling classbased metamodeling inside the domain ontology with full reasoning support through standard owl 2 reasoning systems
we propose a novel encoding scheme
we demonstrate the usefulness of we method by applying our method to the ontoclean methodology
performance problems arising from the inconsistency diagnosis strategy originally proposed for ontoclean by introducing an alternative technique
en passant we address performance problems predicates
an alternative technique where sources of conflicts are indicated by means of markerrecent technology developments in the area of services on the web are marked by the proliferation of web applications and apis
automation that can be achieved with current technologies
applications based on web apis
the implementation and evolution of applications is however hampered by the lack of automation
research on semantic web services is therefore trying to adapt the principles and technologies
the principles and technologies that were devised for traditional web services to deal with this new kind of services
in this paper we show that currently more than 80ontology classification the computation of subsumption hierarchies for properties  is one of the most important tasks for owl reasoners
ontology classification the computation of subsumption hierarchies for classes  is one of the most important tasks for owl reasoners
based on the algorithm by shearer we present a new classification procedure that uses several novel optimisations in order to achieve superior performance
based on the algorithm by horrocks we present a new classification procedure that addresses several open issues of the original algorithm
based on the algorithm by shearer we present a new classification procedure that addresses several open issues of the original algorithm
based on the algorithm by horrocks we present a new classification procedure that uses several novel optimisations in order to achieve superior performance
we also consider the classification of properties
we show that algorithms commonly used to implement that task are incomplete even for relatively weak ontology languages
furthermore we show how to reduce the property classification problem into a standard classification problem
a standard classification problem which allows reasoners to classify properties using we optimised procedure
we implemented we algorithms in the owl hermit reasoner
we present the results of a performance evaluationthere are ontology domain concepts
ontology domain concepts that can be represented according to multiple alternative classification criteria
current ontology modeling guidelines do not explicitly consider this aspect in the representation of such concepts
facet analysis used in library science
to assist with this issue we examined a domainspecific simplified model for facet analysis
a faceted classification scheme which accounts for the multiple alternative classification criteria of the domain concept under scrutiny
a domainspecific simplified model for facet analysis produces a faceted classification scheme
facet analysis used in library science
a comparative analysis between a comparative analysis between a fcs  indicates the existence of key similarities between the elements in the generic structure of both knowledge representation models
a comparative analysis between a comparative analysis between the normalisation ontology design pattern  indicates the existence of key similarities between the elements in the generic structure of both knowledge representation models
a comparative analysis between a a faceted classification scheme and the normalisation ontology design pattern  indicates the existence of key similarities between the elements in the generic structure of both knowledge representation models
a a faceted classification scheme into an owl dl ontology applying the normalisation a comparative analysis between the normalisation ontology design pattern
a a faceted classification scheme into an owl dl ontology applying the normalisation a comparative analysis between a fcs
as a result a mapping is identified that allows to transform a a faceted classification scheme into an owl dl ontology
our contribution is illustrated with an existing a faceted classification scheme example in the domain of  dishwashing detergent  that benefits from the outcome of this studythe social semantic web has begun to provide connections between users within social networks social networks produce across the whole of the social semantic web
the social semantic web has begun to provide connections between users within the content social networks produce across the whole of the social semantic web
thus the social semantic web provides a basis to analyze both the communication behavior of users together with the content of users communication
little research combining the tools to study social network analysis
little research combining the tools to study namely
little research combining the tools to study communication behaviour
little research combining the tools to study communication content
little research combining the tools to study content analysis
however there is little research
furthermore there is even less work addressing the longitudinal characteristics of such a combination
this paper presents a general framework for measuring the dynamic bidirectional influence between social networks
this paper presents a general framework for measuring the dynamic bidirectional influence between communication content
we apply this framework in two usecases online conference publications
we apply this framework in two usecases online forum discussions
this paper provide a new perspective over the dynamics involving both communication content
this paper provide a new perspective over the dynamics involving both social networksthe availability of streaming data sources is progressively increasing thanks to the development of ubiquitous data capturing technologies such as sensor networks
the heterogeneity of streaming data sources introduces the requirement of providing data access in a unified manner whilst allowing the user to express streaming data sources needs at an ontological level
the heterogeneity of streaming data sources introduces the requirement of providing data access in a coherent manner whilst allowing the user to express streaming data sources needs at an ontological level
in this paper we describe an ontologybased streaming data access service
sources link sources data content to ontologies through s 2o mappings
the ontology using an extension of sparql for streaming data
users can query the ontology
the ontology using sparql stream
a preliminary implementation of the approach is also presented
with a preliminary implementation of the approach we expect to set the basis for future efforts in ontologybased streaming data integrationwe extend the semantic web query language sparql by defining the semantics of sparql queries under the entailment regimes of rdf
we extend the semantic web query language sparql by defining the semantics of sparql queries under the entailment regimes of owl
we extend the semantic web query language sparql by defining the semantics of sparql queries under the entailment regimes of rdfs
the proposed extensions are part of the sparql 11 entailment regimes working draft
draft which is currently being developed as part of the w3c standardization process of sparql 11
we review the conditions that sparql imposes on such extensions discuss the practical difficulties of this task
we explicate the design choices underlying we proposals
in addition we include current implementations
current implementations underlying techniques
in addition we include an overview of current implementationsrelatedness measures between ontology concepts are useful in many research areas
semantic similarity between ontology concepts are useful in many research areas
while similarity only considers subsumption relations to assess how two objects are alike relatedness takes into account a broader range of relations 
while similarity only considers subsumption relations to assess how two objects are alike relatedness takes into account a broader range of partof 
in this paper we present the proposed framework
a new way of computing ic values directly from an ontology structure is also introduced
this new model takes into account the whole set of semantic relations
this new model called extended information content
semantic relations defined in an ontology
the proposed framework enables to rewrite existing similarity measures
existing similarity measures that can be augmented to compute semantic relatedness
upon the proposed framework a new measure has been devised
a new measure called faith 
a new measure called feature 
a new measure called information theoretic 
extensive experimental evaluations confirmed the suitability of the proposed frameworkthe semantic web graph is growing at an incredible pace enabling opportunities to discover new knowledge by interlinking previously unconnected data sets
the semantic web graph is growing at an incredible pace enabling opportunities to discover new knowledge by analyzing previously unconnected data sets
the programming models that facilitate the infrastructure to run various algorithms on the semantic web graph
this confronts researchers with a conundrum whilst the data is available the programming models are missing
the programming models that facilitate scalability to run various algorithms on the semantic web graphontologies underpin the semantic web ontologies define the concepts contained in a data source
ontologies underpin the semantic web ontologies define ontologies relationships contained in a data source
an increasing number of ontologies are available online
an ontology can grow extremely large
an ontology that combines information from many different sources
an ontology response time becomes slower
as an ontology grows larger more resources are required to use an ontology
an owl ontology that are no longer used
thus we present an online approach in terms of resources
an owl ontology that are infrequently used
an online approach that are cheap to relearn
an online approach that forgets fragments from an owl ontology
thus we present an online approach in terms of time
thus we evaluate an online approach in terms of time
thus we evaluate an online approach in terms of resources
robocup owlrescue which is an extension of the widely used robocup rescue
in order to evaluate our approach our situate an ontology in a controlled simulation environment robocup owlrescue platform agents are required to perform
platform which enables agents to build ontologies automatically based on the tasks
agents using our approach
agents using our approach ontology
our benchmark show that agents spend less time forgetting concepts from agents allowing agents using our approach to spend more time deliberating agents using our approach actions to achieve a higher average score in the simulation environment
our benchmark our approach against other comparable techniqueson the semantic web decision makers  humans alike  are faced with the challenge of examining large volumes of information
information originating from heterogeneous sources with the goal of ascertaining trust in various pieces of information
on the semantic web decision makers  software agents alike  are faced with the challenge of examining large volumes of information
while previous work has focused on simple models for review systems we introduce a new trust model for rich informationwe present the challenges raised by the new model and the results of an evaluation of the first prototype implementation under a variety of scenarios
while previous work has focused on simple models for rating systems we introduce a new trust model for rich informationwe present the challenges raised by the new model and the results of an evaluation of the first prototype implementation under a variety of scenarios
while previous work has focused on simple models for rating systems we introduce a new trust model for uncertain informationwe present the challenges raised by the new model and the results of an evaluation of the first prototype implementation under a variety of scenarios
while previous work has focused on simple models for rating systems we introduce a new trust model for complex informationwe present the challenges raised by the new model and the results of an evaluation of the first prototype implementation under a variety of scenarios
while previous work has focused on simple models for review systems we introduce a new trust model for uncertain informationwe present the challenges raised by the new model and the results of an evaluation of the first prototype implementation under a variety of scenarios
while previous work has focused on simple models for review systems we introduce a new trust model for complex informationwe present the challenges raised by the new model and the results of an evaluation of the first prototype implementation under a variety of scenariosas knowledge bases move into the landscape of larger ontologies we must work on optimizing the performance of we tools
as knowledge bases have terabytes of related data we must work on optimizing the performance of we tools
we are easily tempted to fill rooms with armies of little ones to address the scalability problem
we are easily tempted to buy bigger machines
yet careful analysis and evaluation of the characteristics of we data using metrics often leads to dramatic improvements in performance
firstly are current scalable systems scalable enough
we found that for large ontologies  it is hard to say because benchmarks obscure the loadtime costs for materialization
we found that for deep ontologies  it is hard to say because benchmarks obscure the loadtime costs for materialization
we found that for some as large as 500000 classes  it is hard to say because benchmarks obscure the loadtime costs for materialization
therefore to expose the loadtime costs for materialization we have synthesized a set of more representative ontologies
scalability how do we manage knowledge over time
secondly in designing for scalability
the ncbo resource index a knowledge base of 164 billion annotations linking 24 million terms from 200 ontologies to 35 million data elements from one week to less than one hour for one of the large datasets on the same machine
by optimizing for data distribution we have reduced the population time including materialization for the ncbo resource index a knowledge base of 164 billion annotations
by optimizing for ontology evolution we have reduced the population time including materialization for the ncbo resource index a knowledge base of 164 billion annotationsrecently processing of queries on linked data has gained attention
a mixed strategy that assumes some incomplete knowledge
a mixed strategy that discovers new sources at runtime
a topdown strategy that relies on complete knowledge about new sources to select relevant sources
a bottomup strategy that discovers new sources during query processing by following links between sources
we systematically discuss three main strategies a mixed strategy
we systematically discuss three main strategies a topdown strategy
we identify
we systematically discuss three main strategies a bottomup strategy
a topdown strategy that relies on complete knowledge about new sources to process relevant sources
knowledge discovered at runtime
to exploit knowledge we propose an additional step explicitly scheduled during query processing called correct source ranking
additionally we propose the adoption of streambased query processing to deal with the unpredictable nature of data access in the distributed linked data environment
in experiments we show that our implementation of the mixed strategy leads to early reporting of results and thus more responsive query processing while not requiring complete knowledgeexplanation provided by ontology engineering environments
justifications that is minimal entailing subsets of an ontology are currently the dominant form of explanation especially those
those focused on the web ontology language 
those focused on owl 
justifications that can be very difficult to understand
despite this there are naturally occurring justifications
in essence justifications are merely the premises of a proof and as such do not articulate the  often nonobvious  reasoning which connect merely the premises of a proof with the conclusion
this paper presents justification oriented proofs as a potential solution to this problemowl 2 currently support only static ontologies
rdf  currently support only static ontologies
such rdf  currently support only static ontologies
semantic web applications often need to represent such changes and reason about semantic web applications
in practice
however the truth of statements often changes with time
in this paper we present a logicbased approach for representing validity time in rdf
in this paper we present a logicbased approach for representing validity time in owl
unlike the existing proposals we approach is applicable to entailment relations such as the rdfbased semantics of owl 2
unlike the existing proposals we approach is applicable to entailment relations such as the direct semantics
entailment relations that are not deterministic
we also present a query evaluation algorithm
we also extend sparql to temporal rdf graphs
entailment relations characterized by a set of such rdf
entailment relations characterized by a set of deterministic rules
we algorithm that is applicable to entailment relations
finally we present an optimization of we algorithm
entailment relations characterized by a set of owl 2 rlrdf entailmentincreasingly usergenerated content is being utilised as a source of information however each individual piece of content tends to contain low levels of information
in addition such information tends to be imperfect in nature containing imprecise subjective ambiguous expressions
in addition such information tends to be informal in nature containing imprecise subjective ambiguous expressions
increasingly usergenerated content may be grouped with similar content
however increasingly usergenerated content does not have to be interpreted in isolation as increasingly usergenerated content is linked either implicitly to a network of interrelated content
increasingly usergenerated content may be related to other content posted at the same time or by members of the same author social network
however increasingly usergenerated content does not have to be interpreted in isolation as increasingly usergenerated content is linked either explicitly to a network of interrelated content
increasingly usergenerated content may be tagged with similar content
comments may be added by other users
increasingly usergenerated content may be related to other content posted at the same time or by the same author social network
increasingly usergenerated content generally examines how ambiguous concepts within usergenerated content can be assigned a specificformal meaning by considering the expanding context of the information other information specifically considers the issue of toponym resolution of locations
increasingly usergenerated content generally examines how ambiguous concepts within usergenerated content can be assigned a specificformal meaning by considering the expanding context of the information other information contained within indirectly related content
increasingly usergenerated content generally examines how ambiguous concepts within usergenerated content can be assigned a specificformal meaning by considering the expanding context of the information other information contained within directly related contentservices that can be created from online sources by using scalable approaches to solve the service selection problem
services that can be created from online sources by using existing annotation tools widely available to the users
given the large number of semantic web services are required to make the large number of semantic web services
services that can be created from online sources by using efficient approaches to solve the service selection problem
services that can be created from online sources by using existing annotation tools
services that can be created from online sources by using expressive formalisms
a framework that is grounded on the localasview approach for representing instances of the service selection problem
in this paper we propose a framework
a framework that is grounded on logic for representing instances of the service selection problem
in we approach web services are semantically described using lav mappings in terms of generic concepts from an ontology
user requests correspond to conjunctive queries on the generic concepts
 in addition the user may specify a set of preferences
preferences that are used to rank the possible solutions to the given request
the ranks induced by the preferences
a query rewriting problem that must consider the relationships among the ranks
a query rewriting problem that must consider the relationships among the concepts in the ontology
the lav formulation allows us to cast the service selection problem as a query rewriting problem
then building on related work we devise an encoding of the resulting query rewriting problem as a logical theory whose models are in correspondence with the solutions of the user request
preferences whose best models are in correspondence with the bestranked solutions
then building on related work we devise an encoding of the resulting query rewriting problem as a logical theory whose models are in correspondence in presence of preferences
thus by exploiting known properties of modern sat solvers we provide an efficient solution to the service selection problem
thus by exploiting known properties of modern sat solvers we provide an scalable solution to the service selection problem
the approach provides the basis to represent a large number of realworld situations
the approach provides the basis to represent a large number of interesting user requestsapplications that exploit heterogeneous data from different sources
the proliferation of linked data on the web paves the way to a new generation of applications
however because the web is continuously evolving it is nontrivial to express some given information needs as structured queries against the relevant link data sources
however because the web is continuously evolving it is nontrivial to identify the relevant link data sources
however because the web is large it is nontrivial to identify the relevant link data sources
however because the web is large it is nontrivial to express some given information needs as structured queries against the relevant link data sources
in this work we allow users to express needs in terms of simple keywords
keyword query routing
given simple keywords we define the problem of finding the relevant sources as the one of keyword query
as a solution we present a family of the proposed models
the proposed models capture information at different levels representing summaries of varying granularity
the proposed models represent different tradeoffs between effectiveness
the proposed models represent different tradeoffs between efficiency
we also verify these tradeoffs in experiments carried out in a realworld setting using more than 150 publicly available datasets
we provide a theoretical analysis of these tradeoffs carried out in a realworld setting using more than 150 publicly available datasets
