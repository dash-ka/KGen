As knowledge bases move into the landscape of larger ontologies we must work on optimizing the performance of we tools
As knowledge bases have terabytes of related data we must work on optimizing the performance of we tools
we are easily tempted to fill rooms with armies of little ones to address the scalability problem
we are easily tempted to buy bigger machines
Yet careful analysis and evaluation of the characteristics of we data using metrics often leads to dramatic improvements in performance
Firstly are current scalable systems scalable enough
we found that for large ontologies  it is hard to say because benchmarks obscure the loadtime costs for materialization
we found that for deep ontologies  it is hard to say because benchmarks obscure the loadtime costs for materialization
we found that for some as large as 500000 classes  it is hard to say because benchmarks obscure the loadtime costs for materialization
Therefore to expose the loadtime costs for materialization we have synthesized a set of more representative ontologies
Secondly in designing for scalability
scalability how do we manage knowledge over time
By optimizing for ontology evolution we have reduced the population time including materialization for the NCBO Resource Index a knowledge base of 164 billion annotations
the NCBO Resource Index a knowledge base of 164 billion annotations linking 24 million terms from 200 ontologies to 35 million data elements from one week to less than one hour for one of the large datasets on the same machine
By optimizing for data distribution we have reduced the population time including materialization for the NCBO Resource Index a knowledge base of 164 billion annotations