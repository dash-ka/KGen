the paper determines the algebraic structure of the multiset semantics of the core patterns of sparql
the paper determines the logic structure of the multiset semantics of the core patterns of sparql
we prove that the fragment union minus corresponds precisely to both the intuitive multiset relational algebra 
we prove that the fragment union minus corresponds precisely to both projection selection natural join except 
we prove that the fragment union minus corresponds precisely to both projection selection natural join arithmetic union 
the fragment formed by and
we prove that the fragment union filter corresponds precisely to both the intuitive multiset relational algebra 
we prove that the fragment union select corresponds precisely to both the intuitive multiset relational algebra 
we prove that the fragment union filter corresponds precisely to both the multiset nonrecursive datalog with safe negation
we prove that the fragment union minus corresponds precisely to both the multiset nonrecursive datalog with safe negation
we prove that the fragment union optional corresponds precisely to both projection selection natural join except 
we prove that the fragment union select corresponds precisely to both the multiset nonrecursive datalog with safe negation
we prove that the fragment union filter corresponds precisely to both projection selection natural join except 
we prove that the fragment union select corresponds precisely to both projection selection natural join except 
we prove that the fragment union optional corresponds precisely to both the multiset nonrecursive datalog with safe negation
we prove that the fragment union filter corresponds precisely to both projection selection natural join arithmetic union 
we prove that the fragment union optional corresponds precisely to both the intuitive multiset relational algebra 
we prove that the fragment union optional corresponds precisely to both projection selection natural join arithmetic union 
we prove that the fragment union select corresponds precisely to both projection selection natural join arithmetic union onthefly generation of integrated representations of linked data search results is challenging because linked data search requires successfully automating a number of complex subtasks such as structure inference and matching of both instances and concepts
both instances and concepts each of which gives rise to uncertain outcomes
onthefly generation of integrated representations of linked data search results is challenging because linked data search requires successfully automating a number of complex subtasks such as structure inference and matching of both instances and concepts
such uncertainty is unavoidable given the semantically heterogeneous nature 
such uncertainty is unavoidable given linked data ones
this paper approaches the problem of structuring linked data search results as an evidencebased one
in particular this paper shows how viz probabilistic soft logic  can be exploited to assimilate different sources of evidence to beneficial effect for users
in particular this paper shows how one formalism  can be exploited to assimilate different sources of evidence to beneficial effect for users
in particular this paper shows how one formalism  can be exploited to assimilate different sources of evidence in a principled way
in particular this paper shows how viz probabilistic soft logic  can be exploited to assimilate different sources of evidence in a principled way
this paper considers user evidence in the form of feedback
syntactic evidence derived from matching algorithms
this paper considers syntactic evidence
this paper considers semantic evidence
semantic evidence derived from linked data vocabularies
probabilistic soft logic rules that model the uniform assimilation of diverse kinds of an empirical evaluation of how the resulting probabilistic soft logic programs perform in terms of the resulting probabilistic soft logic programs ability to infer structure for integrating linked data search results
the main contributions are sets of probabilistic soft logic rules and finally a concrete example of how populating such inferred structures for presentation to the end user is beneficial besides enabling the collection of feedback whose assimilation further improves search result presentation
probabilistic soft logic rules that model the uniform assimilation of diverse kinds of evidence with the success of open data a huge amount of tabular data sources became available that could potentially be mapped into the web of  linked  data
with the success of open data a huge amount of tabular data sources became available that could potentially be linked into the web of  linked  data
most rely on mappings of textual information to classes properties
most rely on mappings of textual information to instances in rdf knowledge bases in order to link
most transform tabular data into rdf
most existing approaches to  semantically label  such tabular data
solutions that solely focus on textual  cues  are only partially applicable for mapping such data sources
however as we will illustrate open data tables typically contain a large portion of numerical columns andor nontextual headers therefore solutions
we propose an approach to rank candidates of context descriptions for a given bag of numerical values
we propose an approach to find candidates of context descriptions for a given bag of numerical values
we propose an approach to find candidates of semantic labels for a given bag of numerical values
we propose an approach to rank candidates of semantic labels for a given bag of numerical values
to this end we apply a hierarchical clustering over information
information taken from dbpedia to build a background knowledge graph of possible  semantic contexts  for bags of numerical values over which we perform a nearest neighbour search to rank the most likely candidates
our evaluation shows that our approach can assign finegrained semantic labels
finegrained semantic labels when there is enough supporting evidence in the background knowledge graph
in other cases our approach can nevertheless assign high level contexts to the data
the data which could potentially be used in combination with other approaches to narrow down the search space of possible labelsstatistical data in the form of rdf data cubes is becoming increasingly valuable as statistical data in the form of rdf data cubes influences decisions in areas such as policy
statistical data in the form of rdf data cubes is becoming increasingly valuable as statistical data in the form of rdf data cubes influences decisions in areas such as finance
statistical data in the form of rdf data cubes is becoming increasingly valuable as statistical data in the form of rdf data cubes influences decisions in areas such as health care
while a growing amount is becoming freely available through the open data movement statistical data in the form of rdf data cubes is opaque to laypersons
semantic question answering technologies provide intuitive access via freeform natural language queries
general semantic question answering systems can not process rdf data cubes
on the intersection between semantic question answering we create a new subfield of semantic question answering called rdcqa
on the intersection between rdf data cubes we create a new subfield of semantic question answering called rdcqa
we create an rdqca benchmark as task 3 of the qald6 evaluation challenge to stimulate further research
we create an rdqca benchmark as task 3 of the qald6 evaluation challenge to enable quantitative comparison between rdcqa systems
independent cubeqa algorithm which achieves a global f  1 score of 043 on the qald6t3test benchmark
we design independent cubeqa algorithm
independent cubeqa algorithm which is the first rdcqa system
1 score of 043 on the qald6t3test benchmark showing that rdcqa is feasible
we evaluate the domain independent cubeqa algorithmrelated entity recommendation where the task is to retrieve a ranked list of related entities
related entities given a keyword query
in recent years there has been an increasing effort to develop techniques for related entity recommendation
account when assessing the relevance of documents
another trend in the area of information retrieval is to take temporal aspects of a given query into account
however while this has become an established functionality in document search engines the significance of time has not yet been recognized for entity recommendation
in this paper we address this gap by introducing the task of timeaware entity recommendation
entities extracted from different data sources publicly available on the web
the first probabilistic model that takes timeawareness into consideration for entity recommendation by leveraging heterogeneous knowledge of entities
we propose the first probabilistic model
we extensively evaluate we experimental results show considerable improvements compared to timeagnostic entity recommendation approaches
we extensively evaluate the proposed approachthe unprecedented growth in mobile devices combined with advances in semantic web technologies has given birth to opportunities for more intelligent systems onthego
approaches that make mobile reasoning more applicable
limited resources of mobile devices demand approaches
while mobilecloud integration is a promising method for harnessing the power of semantic technologies in the mobile infrastructure mobilecloud integration is an open question how to decide when to reason over ontologies on mobile devices
mobile devices that allows an analysis of the feasibility of performing an ontology reasoning on a mobile device with respect to energy consumption
in this paper we introduce an energy consumption prediction mechanism for ontology reasoning on mobile devices
the developed prediction model helps to improve further developments in semantic reasoning in general
the developed prediction model contributes to mobilecloud integrationrecently owl have become the most common knowledge representation languages in use on the web
the web propelled by the recommendation of the w3c
recently rdf have become the most common knowledge representation languages in use on the web
in this paper we examine an alternative way to represent knowledge based on prototypes
this prototypebased representation has different properties
different properties which we argue to be more suitable for data
this prototypebased representation has reuse on the web
data sharing
prototypes provide a means for objectbased data sharing and reuse
prototypes avoid the distinction between classes
prototypes avoid the distinction between instancesduring recent years more data has been published as native rdf datasets
during recent years more data has been published as native rdf datasets
in this setup both the size of the datasets represent challenges for standard sparql query processing techniques
in this setup both the need to process aggregate queries represent challenges for standard sparql query processing techniques
to overcome these limitations materialized views can be used as a source of precomputed partial results during query processing
to overcome these limitations materialized views can be created as a source of precomputed partial results during query processing
however materialized view techniques as proposed for rdf do not support rdf specifics such as incompleteness to support implicit information
however materialized view techniques as proposed for rdf do not support rdf specifics such as the need to support implicit information
to overcome these challenges this paper proposes marvel  materialized rdf views with entailment 
to overcome these challenges this paper proposes marvel  materialized rdf views with incompletness 
an algorithm for rewriting sparql queries materialized rdf views
a view selection algorithm based on an associated rdfspecific cost model a view definition syntax
the approach consists of a view selection algorithm
rewriting sparql queries using
the experimental evaluation shows that marvel can improve query response time by more than an order of magnitude while effectively handling rdf specificsalignments between ontologies usually come with numerical attributes
numerical attributes expressing the confidence of each correspondence
semantics supporting such confidences
semantics must generalise the semantics of alignments without confidence
a semantics which satisfies this
there exists a semantics
a semantics which introduces a discontinuity between nonweighted interpretations
a semantics which introduces a discontinuity between weighted interpretations
moreover this does not provide a calculus for reasoning with weighted ontology alignments
this paper introduces a calculus for such alignments
this paper is given by an infinite relationtype algebra the elements of which are weighted taxonomic relations
in addition this paper approximates the nonweighted case in a continuous mannerannotations are useful to semantically enrich documents
annotations are useful to semantically enrich other datasets with concepts of standardized vocabularies and ontologies
manual annotation is a difficult process
in the medical domain
a difficult process making automatic annotation methods highly desirable to support human annotators
many documents are not annotated at all
a reusebased annotation approach that utilizes previous annotations to annotate similar medical documents
we propose a reusebased annotation approach
the approach clusters items in documents such as medical forms according to previous ontologybased annotations
the approach clusters items in documents such as medical forms uses similar medical documents to determine candidate annotations for new items
a new contextbased strategy that considers the cooccurrence relatedness of annotating concepts
a new contextbased strategy that considers the semantic relatedness of annotating concepts
the final annotations are selected according to a new contextbased strategy
the evaluation based on previous umls annotations of medical forms shows that the new approaches outperform a baseline approach for finding umls concepts in medical documents
the evaluation based on previous umls annotations of medical forms shows that the new approaches outperform the use of the metamap tool for finding umls concepts in medical documentslinked open data has been recognized as a valuable source for background information in data mining
however most data mining tools require a vector of numerical features associated with an instance while linked open data sources are graphs by nature
however most data mining tools require features in propositional form associated with an instance while linked open data sources are graphs by nature
however most data mining tools require a vector of nominal features associated with an instance while linked open data sources are graphs by nature
in this paper we present rdf2vec an approach
an approach that uses adapts language modeling approaches for unsupervised feature extraction to rdf graphs
an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words for unsupervised feature extraction to rdf graphs
graph walks
graph learn latent numerical representations of entities in rdf graphs
we generate sequences by leveraging local information from graph substructures
graph substructures harvested by weisfeilerlehman subtree rdf graph kernels
we evaluation shows that feature vector representations of general knowledge graphs such as wikidata can be easily reused for different tasks
we evaluation shows that feature vector representations of general knowledge graphs such as dbpedia can be easily reused for different tasks
we evaluation shows that such vector representations outperform existing techniques for the propositionalization of rdf graphs on a variety of different predictive machine learning tasksfinding associations between entities is a common information need in many areas
it has been facilitated by the increasing amount of graphstructured data on the web
the web describing relations between entities
a minimal connected subgraph containing all of them
in this paper we define an association
an association connecting multiple entities in a graph as a minimal connected subgraph
entities computed based on a distance oracle
associations which prunes the search space by exploiting distances between entities
we propose an efficient graph search algorithm for finding associations
a conceptual abstract summarizing notable subgroups to be explored
a conceptual abstract summarizing notable subgroups to present an efficient mining algorithm based on canonical codes and partitions
having found a possibly large group of associations we propose to mine frequent association patterns as a conceptual abstract
extensive experiments on large real rdf datasets demonstrate the efficiency of the proposed algorithmsmapping data to a shared domain ontology is a key step in publishing semantic content on the web
most of the work on automatically mapping semistructured sources to ontologies focuses on semantic labeling properties
most of the work on automatically mapping semistructured sources to ontologies focuses on annotating data fields with ontology classes andor properties
most of the work on automatically mapping structured sources to ontologies focuses on semantic labeling properties
most of the work on automatically mapping structured sources to ontologies focuses on annotating data fields with ontology classes andor properties
however a precise mapping needs to describe the semantic relations between the data fields too
a precise mapping that fully recovers the intended meaning of the data
we present a novel approach to automatically discover the semantic relations within a given data source
we mine the small graph patterns occurring in linked open data
we combine the small graph patterns to build a graph
a graph that will be used to infer semantic relations
we evaluated we approach on datasets from different domains
mining patterns of maximum length five we method achieves an average precision of 75the challenges facing the ontology alignment community
user validation is one of the challenges as there are limits to the quality of automated alignment algorithms
ontology alignments that encompasses the alignment system user interface
in this paper we present a broad study on user validation of ontology alignments
ontology alignments that encompasses the services of the alignment system
ontology alignments that encompasses three distinct aspects
ontology alignments that encompasses three interrelated aspects
ontology alignments that encompasses the profile of the user
we discuss the profile of the user and provide an overview of how current systems address the services of the alignment system
we discuss key issues and provide an overview of how current systems address three interrelated aspects
we discuss the profile of the user and provide an overview of how current systems address the profile of the user
we discuss key issues and provide an overview of how current systems address the profile of the user
we discuss its user interface and provide an overview of how current systems address the services of the alignment system
we discuss the profile of the user and provide an overview of how current systems address its user interface
we discuss the profile of the user and provide an overview of how current systems address three distinct aspects
we discuss key issues and provide an overview of how current systems address its user interface
we discuss the services of the alignment system and provide an overview of how current systems address the profile of the user
we discuss the services of the alignment system and provide an overview of how current systems address the services of the alignment system
we discuss the services of the alignment system and provide an overview of how current systems address its user interface
we discuss its user interface and provide an overview of how current systems address the profile of the user
key issues pertaining to the alignment validation process under each of three distinct aspects
we discuss the profile of the user and provide an overview of how current systems address three interrelated aspects
we discuss key issues and provide an overview of how current systems address the services of the alignment system
we discuss the services of the alignment system and provide an overview of how current systems address three distinct aspects
we discuss the services of the alignment system and provide an overview of how current systems address three interrelated aspects
we discuss key issues and provide an overview of how current systems address three distinct aspects
we discuss its user interface and provide an overview of how current systems address its user interface
we discuss its user interface and provide an overview of how current systems address three interrelated aspects
key issues pertaining to the alignment validation process under each of three interrelated aspects
we discuss its user interface and provide an overview of how current systems address three distinct aspects
finally we use experiments from the interactive matching track of the ontology alignment evaluation initiative  oaei  how systems cope with systems as function of systems services
finally we use experiments from the interactive matching track of the ontology alignment evaluation initiative  oaei  2015 to assess the impact of errors in alignment validationcombinatorial creativity combines existing concepts in a novel way in order to produce new concepts
jewelry that measures blood pressure
for example we can imagine jewelry
for this we would combine the concept of jewelry with the capabilities of medical devices
in this paper we concentrate on creating new concepts in the description logic el
we propose a novel language to this effect
we study a novel language properties and complexity
we show that we language can be used to model existing inventions and  to a limited degree  to generate new conceptsthe traversalbased approach to execute queries over linked data on the www fetches data by traversing data links and thus is able to make use of uptodate data from initially unknown data sources
while the downside of the traversalbased approach to execute queries over linked data on the www is the delay before the query engine completes a query execution user may be improved significantly by returning as many elements of the result
the result set as soon as possible
user perceived response time
to this end the query engine requires a traversal strategy
a traversal strategy that enables the engine to fetch resultrelevant data as early as possible
the challenge for such a strategy is that the query engine does not know a priori which of resultrelevant data sources will contain resultrelevant data
sources discovered during the query execution
in this paper we investigate 14 different approaches to achieve a variety of traversal strategies
in this paper we investigate 14 different approaches to rank traversal steps
a baseline that resembles a breadthfirst traversal
we experimentally study traversal strategies impact on response times
we experimentally compare traversal strategies to a baseline
response times that are worse than the baseline
while we experiments show that some of the approaches can achieve noteworthy improvements over the baseline in a significant number of cases we also observe that for every approach there is a nonnegligible chance to achieve response timeswe build on we earlier finding that more than 95in several subject domains classes classes may be subject to categorization resulting in classes of classes 
in several subject domains classes classes may be subject to categorization resulting in classes of or  metaclasses  
when representing these domains one needs to capture not only these domains relations
when representing these domains one needs to capture not only entities of different classification levels
the constraints that apply to entities of these domains relations
the constraints that apply to entities of different classification levels
we observe that this is challenging in current semantic web languages as there is little support to guide the modeler in producing correct multilevel ontologies especially because of the nuances in the constraints
a vocabulary that can be used as a basis for multilevel ontologies in owl along with a number of integrity constraints to prevent the construction of inconsistent models
in order to address these representation challenges we propose a vocabulary
an axiomatic theory called a multilevel modeling theory 
an axiomatic theory called mlt 
in this process we employ an axiomatic theoryin this paper we study instancelevel update in the description logic
in this paper we study instancelevel update in extitdllite  a
the description logic underlying the owl 2 ql standard
in particular we focus on formulabased approaches to abox insertion and deletion
extitdllite  a which is wellknown for enjoying firstorder rewritability of query answering
we show that extitdllite  a enjoys a firstorder rewritability property also for updates
that is every update can be reformulated into a set of insertion instructions computable through a nonrecursive datalog program
that is every update can be reformulated into a set of deletion instructions computable through a nonrecursive datalog program
the abox considered as a database
the abox considered into sql
such a program is readily translatable into a firstorder query over the abox
some experiments showing that the approach works in practice
by exploiting this result we implement an update component for extitdllite  a perform some experiments
by exploiting this result we implement an update component for extitdllite  a based systemssemantic labeling is the process of mapping attributes in data sources to classes in an ontology
semantic labeling is a necessary step in heterogeneous data integration
variations in data formats attribute names make this a very challenging task
variations in data formats attribute even ranges of values of data make this a very challenging task
in this paper we present a novel domainindependent approach to automatic semantic labeling
automatic semantic labeling that uses machine learning techniques
machine learning to learn a model
previous approaches use machine
a model that extracts features related to the data of a domain
a domain which requires the model to be retrained for every new domain
our solution uses similarity metrics as features to compare against labeled domain data
our solution learns a matching function to infer the correct semantic labels for data
the learned the data the data
since our approach depends on the the data needs to be trained once to work effectively across multiple domains
since our approach depends on the the data is domainindependent
the learned similarity metrics
in our evaluation our achieves higher accuracy than other approaches
other approaches even when the learned models are trained on domains other than the test domainconjunctive query answering over expressive horn description logic ontologies is a challenging problem which in some cases can be addressed by application of the chase algorithm
conjunctive query answering over expressive horn description logic ontologies is a relevant problem which in some cases can be addressed by application of the chase algorithm
a novel acyclicity notion which provides a sufficient condition for termination of the restricted chase over horn
in this paper we define a novel acyclicity notion
a novel acyclicity notion which provides sriq tboxes
a novel acyclicity notion which provides sriq tboxes
a novel acyclicity notion which provides a sufficient condition for termination of the restricted chase over horn
we show that a novel acyclicity notion generalizes most of the existing acyclicity conditions  both theoretically 
we show that a novel acyclicity notion generalizes most of the existing acyclicity conditions  both empirically 
a novel acyclicity notion which provides a sufficient condition for termination of the restricted chase over horn
a novel acyclicity notion which provides sriq tboxes
furthermore a novel acyclicity notion gives rise to a very efficient reasoning procedure
we provide evidence for this by providing a materialization
acyclic ontologies which outperforms other stateoftheart systems
a materialization based reasoner for acyclic ontologieswe propose an obda approach for accessing geospatial data using r2rml mappings
geospatial data stored in relational databases
we propose an obda approach for accessing geospatial data using obda mappings
we propose an obda approach for accessing geospatial data using the ogc standard geosparql
we introduce extensions to an existing sparqltosql translation method to support geosparql features
we describe the implementation of we approach in the system ontopspatial an extension of the obda system ontop for creating virtual geospatial rdf graphs on top of geospatial relational databases
we present an experimental evaluation of we system using a stateoftheart benchmark
we present an experimental evaluation of we system extending a stateoftheart benchmark
to measure the performance of our system we confirm our system efficiency
to measure the performance of our system we compare our system to a stateoftheart geospatial rdf storequery containment is one of the building block of query optimization techniques
in the relational world query containment is a wellstudied problem
queries that capture navigation in the graph
graphstructured data where one is interested in expressing queries
at the same time it is wellunderstood that relational queries are not enough to cope with graphstructured data
navigational queries called extended property paths
this paper contributes a study on the problem of query containment for an expressive class of navigational queries
extended property paths are more expressive than previous navigational extension of sparql  as they allow to express path negation among others
extended property paths are more expressive than previous navigational extension of sparql  as they allow to express path conjunction among others
extended property paths are nested regular expressions  as they allow to express path conjunction among others
extended property paths are nested regular expressions  as they allow to express path negation among others
we attack the problem of extended property paths containment
we provide complexity boundsletters whose names are chosen arbitrarily
according to its modeltheoretic semantics modeltheoretic semantics semantic web iris are individual constants
according to its modeltheoretic semantics modeltheoretic semantics semantic web iris are carry no formal meaning
according to its modeltheoretic semantics modeltheoretic semantics semantic web iris are predicate letters
at the same time it is a wellknown aspect of semantic web pragmatics that iris are often constructed mnemonically in order to be meaningful to a human interpreter
a concept that has been not yet quantitatively studied by the semantic web community
a concept that has been discussed
the latter has traditionally been termed  social meaning  a concept
in this paper we use measures of mutual information content
the meaning that is  at least  encoded
in this paper we use methods from statistical model learning to quantify the meaning in semantic web names
we evaluate the approach over hundreds of thousands of datasets in order to illustrate the approach efficacy
we implement the approach
we experiments confirm that many semantic web names are indeed meaningful and more interestingly we provide a quantitative lower bound on how much meaning is encoded in names on a perdataset basis
to we knowledge this is the first paper about the interaction between formal meaning
to we knowledge this is the first paper
the first paper that uses statistical model learning as a method to quantify meaning in the semantic web context
to we knowledge this is the first paper about the interaction between social meaning
semantic web tools that take such social meaning into account
these insights are useful for the design of a new generation of semantic web toolsnavigational graph queries are an important class of queries
queries that can extract implicit binary relations over the nodes of input graphs
most of the navigational query languages nested regular expressions in nsparql are based on the regular expressions
most of the navigational query languages property paths in w3c sparql 11 are based on the regular expressions
the navigational query languages used in the rdf community
it is known that regular expressions have limited expressivity for instance some natural queries like same generationqueries are not expressible with regular expressions
sparql query language equipped with contextfree grammars
to overcome this limitation in this paper we present cfsparql
to overcome this limitation in this paper we present an extension of sparql query language
the cfsparql language is strictly more expressive than property paths
the cfsparql language is strictly more expressive than nested expressions
the additional expressivity can be used for modelling graph summarization
the additional expressivity can be used for modelling graph similarities
the additional expressivity can be used for modelling ontology alignment
despite the additional expressivity we show that cfsparql still enjoys a low computational complexity
despite the additional expressivity we show that cfsparql can be evaluated efficientlyfinding relevant concepts from a corpus of ontologies is useful in many scenarios such as web page annotation
finding relevant concepts from a corpus of ontologies is useful in many scenarios such as automatic ontology population
finding relevant concepts from a corpus of ontologies is useful in many scenarios such as document classification
many millions of concepts are contained in a large number of ontologies across diverse domains
a sparqlbased query demands the knowledge of the structure of the query language whereas userfriendlier suffer from false positives
a sparqlbased query demands the knowledge of the structure of ontologies whereas  simpler keywordbased approaches suffer from false positives
a sparqlbased query demands the knowledge of the structure of the query language whereas  simpler keywordbased approaches suffer from false positives
a sparqlbased query demands the knowledge of the structure of ontologies whereas userfriendlier suffer from false positives
this is because concept descriptions in ontologies may overlap
this is because concept descriptions in ontologies may be ambiguous
in this paper we balances the relevance and diversity of search results
a keywordbased concept search framework which exploits the structure
a keywordbased concept search framework which exploits semantics
in this paper we propose a keywordbased concept search framework in ontologies by constructing contexts for each concept generates the interpretations of a query
the generalpurpose falcons on widelyused performance metrics demonstrates that our system outperforms both
a comprehensive evaluation against the domainspecific bioportal demonstrates that our system outperforms boththe assessment of risk in medicine is a crucial task
scientific knowledge derived by systematic clinical studies on factors
the assessment of risk in medicine depends on scientific knowledge
factors affecting health
scientific knowledge derived by systematic clinical studies on particular knowledge about the current status of a particular patient
existing nonsemantic risk prediction tools are typically based on hardcoded scientific knowledge and only cover a very limited range of patient states
this makes patient states rapidly out of date
this limited in application particularly for patients with multiple cooccurring conditions
in this work we quantified self technologies to create the implemented framework for calculating clinical risk predictions for patients
patients based on selfgathered biometric data
in this work we propose an integration of semantic web
data represented according to generic reusable ontologies for representing clinical risk
data represented according to generic reusable ontologies for representing sensor readings
the implemented framework relies on generic reusable ontologies for representing clinical risk
the implemented framework relies on generic reusable ontologies for representing sensor readings
the implemented framework relies on reasoning to support the integration of data
the implemented framework shows a wide range of advantages over existing risk calculationadvances in information extraction have enabled the automatic construction of large knowledge graphs like yago
advances in information extraction have enabled the automatic construction of large knowledge graphs like dbpedia
advances in information extraction have enabled the automatic construction of large knowledge graphs like freebase
advances in information extraction have enabled the automatic construction of large knowledge graphs like wikidata
these large knowledge graphs are inevitably bound to be incomplete
to fill in the gaps data correlations in the large knowledge graph can be analyzed to predict new facts
to fill in the gaps data correlations in the large knowledge graph can be analyzed to infer horn rules
however horn rules do not take into account possible exceptions so that predicting facts via such rules introduces errors
to overcome this problem we present a method for effective revision of learned horn rules by adding negated atoms  into atoms bodies
to overcome this problem we present a method for effective revision of learned horn rules by adding exceptions  into atoms bodies
this way errors are largely reduced
we apply we method to discover rules with exceptions from realworld large knowledge graphs
we experimental results demonstrate the effectiveness of the developed method
we experimental results demonstrate the effectiveness of the improvements in accuracy for large knowledge graph completion by rulebased fact predictiondespite the emergence and growth of numerous large knowledge graphs many basic facts about our everyday world are not readily available on the web
despite the emergence and growth of numerous large knowledge graphs many important facts about our everyday world are not readily available on the web
commonsense knowledge that relies on joint learning from webscale data to fill gaps in the knowledge acquisition
to address this we present webbrain
to address this we present a new approach for harvesting commonsense knowledge
we train a neural network model to learn relations based on large numbers of textual patterns
textual patterns found on the web
at the same time a neural network model learns vector representations of general word semantics
commonsense knowledge that relies on joint learning from webscale data to fill gaps in the knowledge acquisition
a new approach for harvesting commonsense knowledge allows us to generalize beyond the explicitly extracted information
words that reflect experiments semantics
experiments show that we can obtain representations of words yet also allow us to capture conceptual relationships
experiments show that we can obtain representations of words yet also allow us to capture commonsense knowledgedata stream applications are becoming increasingly popular on the web
in data stream applications one query pattern is especially prominent a join between some background data
in data stream applications one query pattern is especially prominent a join between a continuous data stream
oftentimes the target some background data is large maintained externally costly to query  both in terms of money 
oftentimes the target some background data is large maintained externally costly to query  both in terms of time 
oftentimes the target some background data is large maintained externally changing slowly
hence practical applications usually maintain a local  cached  view of the original some background data
given that these caches are not updated as the original some background data these caches should be refreshed under realistic budget constraints  in terms of computation time  to avoid stale data
given that these caches are not updated as the original some background data these caches should be refreshed under realistic budget constraints  in terms of latency  to avoid stale data
stale data leading to wrong answers
given that these caches are not updated as the original some background data these caches should be refreshed under realistic budget constraints  in terms of possibly financial cost  to avoid stale data
this paper proposes to model the join between the original some background data as a bipartite graph
this paper proposes to model the join between streams as a bipartite graph
by exploiting the graph structure we keep the quality of results good enough without refreshing the entire cache for each evaluation
updates that have the longest effect
we also introduce two extensions to this method first we consider a continuous join between some some background data to focus on updates
we also introduce two extensions to this method first we consider a continuous join between recent portions of a data stream to focus on updates
second we consider the future impact of a query to the original some background data by proposing to delay some updates to provide fresher answers in future
by extending an existing stream processor with the proposed policies we empirically show that we can improve result freshness by 93entity resolution is the task of identifying all mentions that represent the same realworld entity within a knowledge base
entity resolution is the task of identifying all mentions that represent the same realworld entity across multiple knowledge bases
rdf graphs containing multiple types of nodes
we address the problem of performing entity resolution on rdf graphs using the links between instances of different types to improve the accuracy
for example in a graph of products the goal is to resolve all the manufacturers
for example in a graph of manufacturers the goal is to resolve all the products
for example in a graph of products the goal is to resolve all the products
for example in a graph of manufacturers the goal is to resolve all the manufacturers
we formulate this problem as a multitype graph summarization problem
a multitype graph summarization problem which involves clustering the nodes in each type
super nodes that summarize the intercluster links in the original graph
each type that refer to the same entity creating weighted links among super nodes
each type that refer to the same entity into one super node
experiments show that the proposed approach outperforms several stateoftheart generic entity resolution approaches especially in data sets with missing values
experiments show that the proposed approach outperforms several stateoftheart generic entity resolution approaches especially in data sets with onetomany manytomany relationsthe amount of entities in large knowledge bases available on the web has been increasing rapidly making the amount of entities in large knowledge bases available on the web possible to propose new ways of intelligent information access
technologies that can enable crosslingual information access
in addition there is an impending need for technologies
as a intuitive way of specifying information needs keyword queries suffer from the challenges
as a intuitive way of specifying information needs keyword queries suffer from
as a intuitive way of specifying information needs keyword queries enjoy widespread usage
as a simple way of specifying information needs keyword queries suffer from the challenges
ambiguity
as a simple way of specifying information needs keyword queries suffer from
as a simple way of specifying information needs keyword queries enjoy widespread usage
incompleteness
the challenges
crosslinguality
semantic representation which can facilitate query disambiguation and expansion
semantic representation which can facilitate bridge language barriers
in this paper we present a knowledge base approach to crosslingual keyword query interpretation by transforming keyword queries in different languages to different languages semantic representation
the experimental results show that we approach considerably outperforms the baselines
the experimental results show that we approach achieves both high efficiency and effectivenessdespite developments of the semantic web technologies the gap between nonexpert endusers still exists
despite developments of the semantic web technologies the gap between the semantic web still exists
in the field of semantic content authoring tools for interacting with semantic content remain directed at highly trained individuals
this paper adds to the challenges of bringing usergenerated content into the semantic web
in this paper we present seed short for semantic editor an extensible knowledgesupported natural language text composition tool for nonexperienced endusers
semantic editor enables semiautomatic creation of standards
semantic editor enables automatic creation of standards
standards based semantically annotated textual content with focus on the task of text composition
we point out the structure of seed explain how semantic editor excels at utilizing linked open data to realize userfriendly generation of textual content for the semantic web
we point out the structure of seed compare semantic editor with related work
we point out the structure of seed explain how semantic editor excels at utilizing state of the art natural language processing to realize userfriendly generation of textual content for the semantic web
we also present experimental evaluation results involving a diverse group of 120 participants
120 participants which showed that seed helped endusers easily interact with semantic content with nearly no prerequisite knowledge
120 participants which showed that seed helped endusers easily create with semantic content with nearly no prerequisite knowledgeto realise a semantic web of things the challenge of achieving efficient resource description format storage on internet of things devices with limited resources has to be addressed
to realise a semantic web of things the challenge of achieving efficient sparql query performance on internet of things devices with limited resources has to be addressed
stateoftheart sparqltosql engines have been shown to outperform resource description format stores on some benchmarks
in this paper we describe an optimisation to the sparqltosql approach based on a study of timeseries things data structures 
timeseries things data structures that employs efficient translation by reusing existing sparql engines to produce linked data justintime
timeseries things data structures that employs metadata abstraction by reusing existing sparql engines to produce linked data justintime
we evaluate we approach against streaming sparql engines in the context of things data
we evaluate we approach against stateoftheart sparqltosql engines in the context of scenarios
we evaluate we approach against resource description format stores in the context of scenarios
we evaluate we approach against resource description format stores in the context of things data
we evaluate we approach against streaming sparql engines in the context of scenarios
we evaluate we approach against stateoftheart sparqltosql engines in the context of things data
we show that storage efficiency can be improved from 2 times to 3 orders of magnitude
we show that query performance can be improved from 2 times to 3 orders of magnitude
we show that with succinct row storage can be improved from 2 times to 3 orders of magnitudeevaluating joins over rdf data stored in a sharednothing server cluster is key to processing truly large rdf datasets
to the best of our knowledge the existing approaches use a variant of the data exchange operator
the data exchange operator that is inserted into the query plan statically  at query compile time  to shuffle data between servers
distributed query answering that consists of two main components
our argue that such approaches often miss opportunities for local computation
we present a novel solution to distributed query
dynamic data exchange which exploits data locality to maximise the amount of computation on a single server
a query answering algorithm based on dynamic data exchange
first we present a query
graph partitioning whose aim is to increase data locality
rdf data based on graph partitioning
second we present a partitioning algorithm for rdf data
our performance evaluation suggests that our techniques outperform the state of the art by up to an order of magnitude in terms of query evaluation times
our performance evaluation suggests that our techniques outperform the state of the art by up to an order of magnitude in terms of network communication
our have implemented our approach in the rdfox system
our performance evaluation suggests that our techniques outperform the state of the art by up to an order of magnitude in terms of memory uselargescale knowledge graphs provide excellent usecases for ontologies
largescale knowledge graphs are widely used in industry
largescale knowledge graphs are widely used in academia
we find however that popular ontology languages such as owl can not express even the most basic relationships on the normalised data format of largescale knowledge graphs
we find however that popular ontology languages such as datalog can not express even the most basic relationships on the normalised data format of largescale knowledge graphs
existential rules may make reasoning undecidable
existential rules are more powerful
normalising existential rules to suit largescale knowledge graphs often also destroys syntactic restrictions
syntactic restrictions that ensure low complexity
syntactic restrictions that ensure decidability
we study this issue for several classes of existential rules
we derive new syntactic criteria to recognise wellbehaved rulebased ontologies over largescale knowledge graphsa measure supporting the detection of strong relationships between linked data entities
the goal of this work is to learn a measure
such relationships can be represented as paths of entities
such relationships can be represented as paths of properties
such relationships can be obtained through a blind graph search process
a blind graph search process traversing linked data
a costfunction that is able to detect the strongest relationship between two given entities
the challenge here is therefore the design of a costfunction by objectively assessing the value of a given path
to achieve this we use a genetic programming approach in a supervised learning method to generate path evaluation functions that compare well with human evaluations
we show how such a costfunction can be generated only using basic topological features of the nodes of the paths as the paths are being traversed  without knowledge of the whole graph 
we show how such a costfunction can be generated only using basic topological features of the nodes of the paths how it can be improved through introducing a very small amount of knowledge about the vocabularies of the properties
the properties that connect nodes in the graphresolving the semantic heterogeneity in the semantic web requires finding correspondences between ontologies
ontologies describing resources
in particular with the explosive growth of data sets in the linked open data linking multiple vocabularies and ontologies simultaneously known as holistic matching problem becomes necessary
currently most stateoftheart matching approaches are limited to pairwise matching
a linear program extending the maximumweighted graph matching problem with structural 
a linear program extending the maximumweighted graph matching problem with cardinality 
a linear program extending the maximumweighted graph matching problem with linear constraints 
a linear program extending the maximumweighted graph matching problem with coherence constraints 
in this paper we propose a holistic ontology matching approach
a holistic ontology matching approach that is modeled through a linear program
a holistic ontology guarantees the optimal solution with mostly coherent alignments
a holistic ontology matching approach
a linear program extending the maximumweighted graph matching problem with cardinality rrb
a linear program extending the maximumweighted graph matching problem with linear constraints rrb
approach that is modeled through a linear program
a linear program extending the maximumweighted graph matching problem with structural rrb
a linear program extending the maximumweighted graph matching problem with coherence constraints rrb
experiments performed on the conference track of the oaei 2015 under both holistic and pairwise matching settings
to evaluate we proposal we discuss the results of experimentsthe semantics can be used to intermediate heterogeneous users activity logs such information can be used to improve applications
the semantics distributed over largescale knowledge bases
intermediate heterogeneous users activity logs created in services
applications that can help users to decide the next activitiesservices
since user activities can be represented in terms of relationships involving a user tags movitems on a webpage  tensors are an attractive approach to represent tensors
since user activities can be represented in terms of relationships involving three things  tensors are an attractive approach to represent tensors
since user activities can be represented in terms of relationships involving more things  tensors are an attractive approach to represent tensors
the recently introduced semantic sensitive tensor factorization is promising as it achieves high accuracy in predicting users activities by basing tensor factorization on the semantics behind objects 
the recently introduced semantic sensitive tensor factorization is promising as it achieves high accuracy in predicting users activities by basing tensor factorization on the semantics behind item categories 
however sensitive tensor factorization currently thus has two problems the sparsity problem triggered by insufficient observations within a single service
however sensitive tensor factorization currently focuses on the factorization of a tensor for a single service the balance problem occurs when handling heterogeneous datasets simultaneously
however sensitive tensor factorization currently focuses on the factorization of a tensor for a single service the sparsity problem triggered by insufficient observations within a single service
however sensitive tensor factorization currently thus has two problems the balance problem occurs when handling heterogeneous datasets simultaneously
our solution semantic sensitive simultaneous tensor factorization tackles the problems by creating tensors for individual services does not force the creation of a tensor from multiple services
our solution semantic sensitive simultaneous tensor factorization tackles the problems by creating tensors for individual services factorize the single tensor
our solution semantic sensitive simultaneous tensor factorization tackles the problems by factorizing individual services simultaneously semantic sensitive semantic sensitive simultaneous tensor factorization factorize the single tensor
our solution semantic sensitive simultaneous tensor factorization tackles the problems by factorizing individual services simultaneously semantic sensitive simultaneous tensor factorization does not force the creation of a tensor from multiple services
our solution semantic sensitive simultaneous tensor factorization tackles the problems by factorizing individual services simultaneously semantic sensitive simultaneous tensor factorization factorize the single tensor
our solution semantic sensitive simultaneous tensor factorization tackles the problems by factorizing individual services simultaneously semantic sensitive semantic sensitive simultaneous tensor factorization does not force the creation of a tensor from multiple services
this avoids the low prediction accuracy
the low prediction accuracy caused by the balance problem
assigning semantic bias to each tensor factorization
utilizing shared semantics behind distributed activity logs
this avoids the sparsity problem by sharing semantics among services
experiments show that semantic sensitive simultaneous tensor factorization achieves higher accuracy in rating prediction than the current best tensor method
experiments using realworld datasets
the current best tensor method also extracts implicit relationships across services in the feature spaces by simultaneous factorization with shared semantics
