ontology mappings are often assigned a weight factor by matchers
ontology mappings are often assigned a confidence factor by matchers
nonetheless few semantic accounts have been given so far for such weights
this paper presents a formal semantics for weighted mappings between different ontologies
weights are interpreted to measure how complete reclassifications are
it is based on a classificational interpretation of mappings if o 2 are two ontologies then mappings between o 2 are interpreted to encode how elements of x are reclassified in the concepts of o 2
it is based on a classificational interpretation of mappings if o 1 are two ontologies then mappings between o 1 are interpreted to encode how elements of x are reclassified in the concepts of o 2
two ontologies used to classify a common set x
x classified in the concepts of o 1
weights are interpreted to measure how precise reclassifications are
it is based on a classificational interpretation of mappings if o 1 are two ontologies then mappings between o 2 are interpreted to encode how elements of x are reclassified in the concepts of o 2
it is based on a classificational interpretation of mappings if o 2 are two ontologies then mappings between o 1 are interpreted to encode how elements of x are reclassified in the concepts of o 2
this semantics is justifiable by extensional practice of ontology matching
this semantics is a conservative extension of a semantics of crisp mappings
properties that relate mapping entailment with description logic constructors
this paper also includes propertiesclassification is a fundamental reasoning task in ontology design
there is currently a wide range of reasoners highly optimised for classification of owl 2 ontologies
several reasoners that are complete for restricted fragments of owl 2 such as the owl 2 el profile
there are also several reasoners
several reasoners complete for ontologies
ontologies containing axioms outside the relevant fragment
several reasoners that are complete for restricted fragments of owl 2 such as the owl 2 el profile are much more efficient than several reasoners
several reasoners that are complete for restricted fragments of owl 2 such as the owl 2 el profile are much more efficient than fullyfledged owl 2 reasoners
several reasoners that are complete for restricted fragments of owl 2 such as the owl 2 el profile are not
a novel classification technique that combines an efficient reasoner for a given fragment in such a way that the bulk of the workload is assigned to the latter
in this paper we propose a novel classification technique
a novel classification technique that combines an owl 2 reasoner for a given fragment in such a way that the bulk of the workload is assigned to the latter
reasoners are combined in a blackbox modular manner
the specifics of reasoners implementation are irrelevant to we approachfor a number of years now we have seen the emergence of repositories of research data conceptualized according to a variety of ontologies
for a number of years now we have seen the emergence of repositories of research data specified using owlrdf as representation languages
this class of solutions promises both to facilitate the integration of research data with other relevant sources of information and also to support more intelligent forms of querying
this class of solutions promises both to facilitate the integration of research data with other relevant sources of information and also to support more intelligent forms of exploration
however an issue is that of characterizing semantically the relations
an issue which has only been partially addressed
however an issue is that of generating semantically the relations
the relations that exist between research areas
this problem has been traditionally addressed by manually creating these taxonomies
these taxonomies are very coarsegrained
however
these taxonomies do not cater for the finegrained research topics
this manual approach is inadequate for a number of reasons
the finegrained research topics which define the level at which typically and even more so phd students  operate
the finegrained research topics which define the level at which typically researchers  operate
therefore and even more so phd students tend not to cover the most recent research trends
moreover
researchers evolve slowly
therefore researchers tend not to cover the most recent research trends
and even more so phd students evolve slowly
in addition as we move towards a semantic characterization of these relations there is arguably a need for a more sophisticated characterization than a homogeneous taxonomy to reflect the different ways
the different ways in which research areas can be related
in this paper we propose klink a new approach to i  wikipedia
both machine learning methods
external knowledge which is drawn from a number of resources
in this paper we propose klink a new approach to i  automatically generating relations between research areas and ii  and external knowledge
research areas and ii  which combines both machine
in this paper we propose klink a new approach to i  google scholar
research areas and ii  populating a bibliographic ontology
we have tested a number of alternative algorithms evaluation shows that a method performs best with respect to a manually constructed standard
a method relying on the ability to detect temporal relations between research areas
we have tested a number of we evaluation shows that a method performs best with respect to a manually constructed standard
a method relying on both external knowledge to detect temporal relations between research areasthe main tasks when maintaining knowledge bases
the main tasks when creating knowledge bases
one of the main tasks is to provide sources for facts in order to ensure correctness of the provided knowledge
one of the main tasks is to provide sources for facts in order to ensure traceability of the provided knowledge
one of the main tasks is to validate facts
so far this task is often addressed by human curators in a threestep process screening those documents for relevant content
so far this task is often addressed by human curators in a threestep process retrieving potentially relevant documents
so far this task is often addressed by human curators in a threestep process issuing appropriate keyword queries for the statement to check using standard search engines
the drawbacks of a threestep process are manifold
most importantly it is very timeconsuming as the experts must often read several documents
most importantly it is very timeconsuming as the experts have to carry out several search processes
in this article we present an algorithm for validating facts by finding trustworthy sources for this article on the web
in this article we present deep fact validation  
in this article we present defacto  
defacto aims to provide an effective way of validating facts by supplying the user with relevant excerpts of webpages defacto has in the correctness of the input fact
defacto aims to provide defacto has in the correctness of the input fact
useful additional information
a score for the confidence
defacto aims to provide useful additional information defacto has in the correctness of the input factthis paper presents an approach to automatically extract relationships from textual documents
this paper presents an approach to automatically extract entities from textual documents
the main goal is to populate a knowledge base
a knowledge base that hosts this structured information about domain entities
the extracted entities expected relationships are verified using classification
the extracted entities expected relationships are verified linking
the extracted entities expected relationships are verified linking
the extracted entities expected relationships are verified using two evidence based techniques
the extracted entities expected relationships are verified using classification
the extracted entities expected relationships are verified using two evidence based techniques
this last process also enables the linking of our knowledge base to other sources
other sources which are part of the linked open data cloud
we demonstrate the benefit of we approach through series of experiments with realworld datasetsthe lightweight ontology language owl rl is used for reasoning with large amounts of data
to this end the w3c standard provides a simple system of deduction rules
deduction rules which operate directly on the rdf syntax of owl
several similar systems have been studied
however these approaches are usually complete for instance retrieval only
this paper asks how such methods could also be used for computing
computing entailed subclass relationships
this paper asks if such methods could also be used for computing
tractable rulebased reasoning is possible when restricting to subsumptions between atomic classes
checking entailment for arbitrary owl rl class subsumptions is conphard
the w3c calculus can not be extended to compute all atomic class subsumptions
however this can not be achieved in any rdfbased rule system
surprisingly
a rule system that is sound for many owl rl ontologies
a rule system that is complete for many owl rl ontologies
we identify syntactic restrictions to mitigate this problem and propose a rule systemthe primary challenge of machine perception is to define efficient computational methods to derive highlevel knowledge from lowlevel sensor observation data
perception which enable advanced integration and interpretation of heterogeneous sensor data
emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing
emerging solutions are using ontologies for expressive representation of concepts in perception
the computational complexity of owl however seriously limits the computational complexity of owl applicability and use within resourceconstrained environments such as mobile devices
to overcome this issue we employ owl to formally define the inference tasks discrimination and then provide efficient algorithms for these tasks using bitvector encodings and operations
the inference tasks needed for machine perception
to overcome this issue we employ owl to formally define the inference tasks explanation and then provide efficient algorithms for these tasks using bitvector encodings and operations
the applicability of our approach to machine perception is evaluated on a smartphone mobile device demonstrating dramatic improvements in both efficiency and scalerecent web service technology development owing to web apis simplicity of technology stack
web apis have gained increasing popularity in recent web service technology development
web apis have gained the proliferation of mashups
however efficiently discovering web apis is still a challenging task even with the best resources available on the web
however efficiently discovering the relevant documentations on the web is still a challenging task even with the best resources available on the web
in this paper we cast the problem of detecting the web api documentations as a text classification problem of classifying a given web page as web api associated or not
we propose a supervised generative topic model
feature latent dirichlet allocation which offers a generic probabilistic framework for automatic detection of web apis
a supervised generative topic model called feature latent dirichlet allocation
feature latent dirichlet allocation not only captures the correspondence between data automatically learned from data
feature latent dirichlet allocation not only also provides a mechanism for incorporating side information such as labelled features automatically learned from data
data that can effectively help improving classification performance
feature latent dirichlet allocation not only captures the correspondence between the associated class labels automatically learned from data
feature latent dirichlet allocation which offers a generic probabilistic framework for automatic detection of web apis
feature latent dirichlet allocation which offers a generic probabilistic framework for automatic detection of web apis
a supervised generative topic model called feature latent dirichlet allocation rrb
a supervised generative topic model called feature latent dirichlet allocation rrb
extensive experiments on our web apis documentation dataset shows that a supervised generative topic model outperforms
the maximum entropy model by over 3
three strong supervised baselines
extensive experiments on our web apis documentation dataset shows that a supervised generative topic model outperforms three strong supervised baselines
naive bayes support vector machines by over 3topk queries queries results ordered by a userdefined scoring function are an important category of queries
queries returning the top k
data that can be exploited to speed up query processing
order is an important property of data
a materializethensort processing scheme that computes all the
all the matching thousands  even if ten  are requested
all the matching solutions  even if ten  are requested
stateoftheart sparql engines underuse order
topk queries are mostly managed with a materializethensort processing scheme
all the matching thousands  even if only a limited number k  are requested
all the matching solutions  even if only a limited number k  are requested
an extended sparql algebra that treats order as a first class citizen enabling efficient splitandinterleave processing schemes
the sparqlrank algebra is an extended sparql algebra
efficient splitandinterleave processing schemes that can be adopted to improve the performance of topk sparql queries
in this paper we propose an incremental execution model for sparqlrank queries we compare the performance of alternative physical operators and we propose a rankaware join algorithm
algorithm optimized for native rdf stores
experiments conducted with an open source implementation of a sparqlrank query engine based on arq
experiments show that the evaluation of topk queries can be sped up by orders of magnituderecent developments in hardware have shown an increase in parallelism as opposed to clock rates
a way that allows for finegrained parallelism
in order to fully exploit these new avenues of performance improvement computationally expensive workloads have to be expressed in a way
in this paper we address the problem of describing rdfs entailment in such a way
different from previous work on parallel rdfs reasoning we assume a shared memory architecture
duplicates that naturally occur in rdfs reasoning and develop strategies towards rdfs reasoning mitigation exploiting all levels of our architecture
we analyze the problem of duplicates
our implement and evaluate our approach on two realworld datasets performance characteristics on different levels of parallelization
our implement and evaluate our approach on study rdfs reasoning performance characteristics on different levels of parallelization
careful optimizations that take into account intricacies of modern parallel hardware
our conclude that rdfs entailment lends rdfs entailment well to parallelization
our conclude that rdfs entailment can benefit even more from careful optimizationsexisting approaches for link prediction in the domain of network science exploit a networks topology to predict future connections by inducing links
existing approaches for link prediction in the domain of network science exploit a networks topology to predict future connections by assessing existing edges and connections
links given the presence of mutual nodes
despite the rise in popularity of the production of content within such platforms no existing work has attempted to exploit the semantics of published content when predicting network links
despite the rise in popularity of microblogging platforms  no existing work has attempted to exploit the semantics of published content when predicting network links
despite the rise in popularity of attentioninformation networks  no existing work has attempted to exploit the semantics of published content when predicting network links
in this paper we present models
an approach that fills this gap by a  significantly outperforming a random baseline
in this paper we present an approach
models that rely solely on network topology information and b 
an approach that fills this gap by a  predicting follower edges within a directed social network by exploiting concept graphs
network topology information and b  assessing the different behaviour that users exhibit when making followeeaddition decisions
the existence of a clear need for topical affinity between users for a follow link to be created
this latter contribution exposes latent factors within social networksthe heterogeneous nature of linked open data requires federated techniques for query evaluation
the distributed nature of linked open data requires flexible techniques for query evaluation
the heterogeneous nature of linked open data requires flexible techniques for query evaluation
the distributed nature of linked open data requires federated techniques for query evaluation
in order to evaluate current federation querying approaches a general methodology for conducting benchmarks is mandatory
in this paper we present a classification methodology for federated sparql queries
a classification methodology for federated sparql queries can be used by developers of federated querying approaches to compose a set of test benchmarks
test benchmarks that cover diverse characteristics of different queries
test benchmarks that allows for comparability
benchmark queries that takes into account the number of sources to be queried
benchmark queries that takes into account the number of several complexity parameters
benchmark queries that is based on a classification methodology for federated sparql queries
a heuristic called splodge for automatic generation of benchmark queries
we further develop a heuristic
we evaluate the adequacy of the query generation strategy by applying the adequacy of our methodology on the 2011 billion triple challenge data
we evaluate the adequacy of we methodology by applying the adequacy of the query generation strategy on the 2011 billion triple challenge data
we evaluate the adequacy of we methodology by applying the adequacy of our methodology on the 2011 billion triple challenge data
the 2011 billion triple challenge data set
we evaluate the adequacy of the query generation strategy by applying the adequacy of the query generation strategy on the 2011 billion triple challenge datathe last decade of research in ontology alignment has brought a variety of computational techniques to discover correspondences between ontologies
domain knowledge that is used to augment
domain knowledge that is used to train the algorithms
domain knowledge that is used to validate
while the accuracy of automatic approaches has continuously improved human contributions remain a key ingredient of the process this input serves as a valuable source of domain knowledge automatically computed alignments
in this paper we introduce crowdmap 
in this paper we introduce a model to acquire such human contributions via microtask crowdsourcing
for a given pair of ontologies crowdmap a model to acquire such human contributions via microtask crowdsourcing translates the alignment problem into microtasks
the results obtained from the crowd
microtasks that publishes the microtasks on an online labor market
microtasks that address individual alignment questions
microtasks that evaluates the quality of the results
we evaluated the current implementation of a model to acquire such human contributions via microtask crowdsourcing in a series of the experiments the crowdsourcing platform crowdflower
a series of the experiments using ontologies from the ontology alignment evaluation
we evaluated the current implementation of a model to acquire such human contributions via microtask crowdsourcing in a series of the experiments initiative
we evaluated the current implementation of crowdmap
a series of the experiments using reference alignments from the ontology alignment evaluation
the experiments clearly can improve the accuracy of existing ontology alignment solutions in a costeffective manner
the experiments clearly demonstrated that the overall approach is feasible
the experiments clearly can improve the accuracy of existing ontology alignment solutions in a fast manner
the experiments clearly can improve the accuracy of existing ontology alignment solutions in a scalable mannerfirst order logic  rewritability is a desirable feature for query answering over geothematic ontologies because in most geoprocessing scenarios one has to cope with large data volumes
fol  rewritability is a desirable feature for query answering over geothematic ontologies because in most geoprocessing scenarios one has to cope with large data volumes
hence there is a need for combined geothematic logics
combined geothematic logics that provide a sufficiently expressive query language
a sufficiently expressive query language allowing for fol rewritability
the dllite family of description logics is tailored towards fol rewritability of query answering for unions of conjunctive queries hence the dllite family of description logics is a suitable candidate for the thematic component of a combined geothematic logic
we show that a weak coupling of dllite with the expressive region connection calculus rcc8 allows for fol rewritability under a spatial completeness condition for the abox
stronger couplings are possible only for spatial calculi as weak as the lowresolution calculus rcc2
stronger couplings allowing for fol rewritability
already a strong combination of dllite with the lowresolution calculus rcc3 does not allow for fol rewritabilitya key issue in semantic reasoning is the computational complexity of inference tasks on expressive ontology languages such as owl dl
a key issue in semantic reasoning is the computational complexity of inference tasks on expressive ontology languages such as owl 2 dl
theoretical works have established worstcase complexity results for reasoning tasks for expressive ontology languages such as owl dl and owl 2 dl
however hardness of reasoning about individual ontologies has not been adequately characterised
in this paper we conduct a systematic study to tackle this problem using machine learning techniques covering over four stateoftheart widelyused owl 2 reasoners
in this paper we conduct a systematic study to tackle this problem using machine learning techniques covering over 350 realworld ontologies
we main contributions are twofold
various classifiers that accurately predict classification time for an ontology based on an ontology metric values
firstly we learn various classifiers
secondly we identify a number of metrics
metrics that can be used to effectively predict reasoning performance
we prediction models have been shown to be highly effective achieving an accuracy of over 80sentiment analysis over twitter offer organisations a effective way to monitor the publics feelings towards the publics brand for training sentiment classifiers for twitter datasets have been researched in recent years with varying results
sentiment analysis over twitter offer organisations a fast way to monitor the publics feelings towards the publics methods for training sentiment classifiers for twitter datasets have been researched in recent years with varying results
sentiment analysis over twitter offer organisations a effective way to monitor the publics feelings towards the publics methods for training sentiment classifiers for twitter datasets have been researched in recent years with varying results
sentiment analysis over twitter offer organisations a effective way to monitor the publics feelings towards the publics business directors a wide range of features for training sentiment classifiers for twitter datasets have been researched in recent years with varying results
sentiment analysis over twitter offer organisations a fast way to monitor the publics feelings towards the publics brand for training sentiment classifiers for twitter datasets have been researched in recent years with varying results
sentiment analysis over twitter offer organisations a fast way to monitor the publics feelings towards the publics business directors a wide range of features for training sentiment classifiers for twitter datasets have been researched in recent years with varying results
the training set for sentiment analysis
in this paper we introduce a novel approach of adding semantics as additional features into the training
for each we add apple product   as an additional feature
for each we measure the correlation of the representative concept with negativepositive sentiment
for each we add iphone semantic concept  as an additional feature
each extracted entity  from tweets
each extracted iphone  from tweets
we apply this approach to predict sentiment for three different twitter datasets
we results show an average increase of f harmonic accuracy score for identifying both negative sentiment of around 65
we results show an average increase of f harmonic accuracy score for identifying both positive sentiment of around 65a new all other geospatial rdf store that supports the state of the art semantic geospatial query languages stsparql
a new all other geospatial rdf store that supports the state of the art semantic geospatial query languages geosparql
we present strabon a new all other geospatial rdf store
the expressive power offered by the art the art semantic geospatial query languages geosparql implementation in strabon semantic geospatial query languages stsparql
a new rdf store that supports the state of the art
the expressive power offered by the art the art semantic geospatial a new rdf store semantic geospatial query languages stsparql
the expressive power offered by the art the art semantic geospatial query languages stsparql implementation in strabon semantic geospatial query languages stsparql
the expressive power offered by the art the art semantic geospatial a new rdf store semantic geospatial query languages geosparql
the expressive power offered by the art the art semantic geospatial query languages geosparql implementation in strabon semantic geospatial query languages geosparql
to illustrate the expressive power we concentrate on the new version of the query language stsparql that we have developed we
to illustrate the expressive power we concentrate on the new version of the data model strdf that we have developed we
the expressive power offered by the art semantic geospatial query languages stsparql geosparql
the expressive power offered by the art semantic geospatial query languages geosparql stsparql
the expressive power offered by the art semantic geospatial query languages geosparql geosparql
the expressive power offered by the art the art semantic geospatial query languages stsparql implementation in strabon semantic geospatial query languages geosparql
the expressive power offered by the art semantic geospatial query languages stsparql stsparql
like geosparql these new versions use ogc standards to represent geometries where the original versions used linear constraints
we show that the performance of strabon experimentally scales to very large data volumes most of the times better than all other geospatial rdf stores the performance of strabon experimentally has been compared with
we study the performance of strabon experimentally most of the times better than all other geospatial rdf stores the performance of strabon experimentally has been compared with
we show that the performance of strabon performs most of the times better than all other geospatial rdf stores the performance of strabon experimentally has been compared withontology are built around the idea that axioms enable the inference of new facts about the available data
other logical languages are built around the idea that axioms enable the inference of new facts about the available data
in some circumstances
deducing new facts may be undesirable
however the available data is meant to be complete in certain ways
previous approaches to this issue have relied on syntactically specifying certain axioms as constraints or adding in new constructs for constraints and providing a extended meaning for constraints
constraints that reduces their ability to infer new facts without requiring the data to be complete ability to infer new facts without requiring the available data to be complete
constraints that eliminates constraints
constraints that reduces constraints
previous approaches to this issue have relied on syntactically specifying certain axioms as constraints or adding in new constructs for constraints and providing a different meaning for constraints
constraints that eliminates their ability to infer new facts without requiring the data to be complete ability to infer new facts without requiring the available data to be complete
we propose to instead directly state that the extension of certain concepts and roles are complete by making certain concepts and roles dbox predicates which eliminates the distinction between regular axioms and constraints for these concepts and roles
this proposal avoids problems of previous proposals
this proposal eliminates the need for special semanticsontology alignment using instance
instance based matching of types
in this paper we describe a mechanism for ontology alignment
instancebased matching is known to be a useful technique for matching ontologies
ontologies that have different names
ontologies that have different structures
a key problem in instance matching of types however is scaling the matching algorithm to handle types with a large number of instances
a key problem in instance matching of types however is scaling the matching algorithm to match a large number of type pairs
we propose the use of locality sensitive hashing techniques to vastly improve the scalability of instance matching across multiple types
we show the feasibility of we approach with freebase with hundreds of types respectively
we show the feasibility of we approach with two different type systems with thousands of types respectively
we show the feasibility of we approach with freebase with thousands of types respectively
we show the feasibility of we approach with dbpedia with thousands of types respectively
we show the feasibility of we approach with dbpedia with hundreds of types respectively
we show the feasibility of we approach with two different type systems with hundreds of types respectively
we describe how these techniques can be used to estimate containment relations between two type systems
we compare two different locality sensitive hashing techniques for computing instance similarity
we describe how these techniques can be used to estimate equivalence relations between two type systemsmost of the semantic content available has been generated automatically by using annotation services for existing content
automatic annotation is not of sufficient quality to enable focused search and retrieval
too few terms are semantically annotated
too many terms are semantically annotated
userdefined semantic enrichment allows for a more targeted approach
we conducted an enduser study to evaluate a tool acceptance by
we conducted usability for nonexpert users
we developed a tool for semantic annotation of digital documents
an enduser study to evaluate its acceptance by presents the results of usability for nonexpert users
usability for nonexpert users presents the results of usability for nonexpert users
an enduser study to evaluate its acceptance by presents the results of an enduser study to evaluate its acceptance by
usability for nonexpert users discusses the lessons learned about both our methodology of exposing nonexperts to semantic enrichment
an enduser study to evaluate its acceptance by discusses the lessons learned about both the semantic enrichment process of exposing nonexperts to semantic enrichment
usability for nonexpert users discusses the lessons learned about both the semantic enrichment process of exposing nonexperts to semantic enrichment
an enduser study to evaluate its acceptance by discusses the lessons learned about both our methodology of exposing nonexperts to semantic enrichment
usability for nonexpert users presents the results of an enduser study to evaluate its acceptance bydespite the increase in the number of linked instances in the linked data cloud in recent times the absence of links at the concept level has resulted in heterogenous schemas challenging the interoperability goal of the semantic web
in this paper we address this problem by finding alignments between concepts from multiple linked data sources
instead of only considering the existing concepts present in each ontology we hypothesize new composite concepts
new composite concepts defined as disjunctions of conjunctions of rdf types which we generate alignments between these composite concepts
new composite concepts defined as disjunctions of conjunctions of value restrictions which we generate alignments between these composite concepts
new composite concepts defined as disjunctions of conjunctions of rdf types which we call restriction classes
new composite concepts defined as disjunctions of conjunctions of value restrictions which we call restriction classes
those that are simple renderings of relational databases
this extended concept language enables us to even align sources
this extended concept language enables us to find more complete definitions
sources that have rudimentary ontologies such as those
these concepts linked instances
our concept alignment approach is based on analyzing these concepts
our concept alignment approach is based on analyzing the extensions of these concepts
having explored the alignment of conjunctive concepts in our previous work in this paper 
having explored the alignment of conjunctive concepts in our previous work in our focus on concept coverings
our present an evaluation of this new algorithm to genetics domains
our present an evaluation of this new algorithm to biological classification
our present an evaluation of this new algorithm to geospatial
determining the alignments between concepts in refining existing ontologies thus increasing the interoperability in the linked open data cloud
the resulting alignments are useful for refining existing ontologieswe present an algorithm
we present tipalo
we present tool for automatically typing dbpedia entities
tipalo natural language definition which is extracted from tipalo corresponding wikipedia page abstract
tipalo identifies the most appropriate types for an entity by interpreting tipalo natural language definition
types are aligned to a subset of dolcedns ultra lite classes
types are aligned to two toplevel ontologies
types are aligned to wordnet supersenses
types are identified by means of a set of heuristics based on graph patterns
types are disambiguated to wordnet
an algorithm has been tuned against a golden standard
a golden standard that has been evaluated in a user study
a golden standard that has been built online by a group of selected usersunderstanding various contexts of users is important to enable personalised selection of web apis in directories such as programmable web
modelling various contexts of users is important to enable personalised selection of web apis in directories such as programmable web
currently relationships between web apis are not clearly understood by existing selection approaches
currently relationships between users are not clearly utilized by existing selection approaches
currently relationships between web apis are not clearly utilized by existing selection approaches
currently relationships between users are not clearly understood by existing selection approaches
a web api directory graph that captures relationships such as developers
in this paper we present a semantic model of a web api directory graph
a web api directory graph that captures relationships such as categories
a web api directory graph that captures relationships such as web apis
a web api directory graph that captures relationships such as mashups
we describe a novel configurable graphbased method for selection of web apis with personalised aspects
we describe a novel configurable graphbased method for selection of web apis with temporal aspects
a novel configurable graphbased method for selection of web apis allows users to get more control over users preferences and recommended web apis while users can exploit information about users social links and preferences
we evaluate a novel configurable graphbased method for selection of web apis on a realworld dataset from programmablewebcom
we show that a novel configurable graphbased method for selection of web apis provides more contextualised results than currently available popularitybased rankingsdue to the high worst case complexity of the core reasoning problem for the expressive profiles of owl 2 ontology engineers are confused by the performance behaviour of reasoners on ontology engineers ontologies
due to the high worst case complexity of the core reasoning problem for the expressive profiles of owl 2 ontology engineers are often surprised
even very experienced modellers with a sophisticated grasp of reasoning algorithms do not have a good mental model of reasoner performance behaviour
seemingly innocuous changes to an owl ontology can degrade classification time from instantaneous to too long to wait for
similarly switching to take advantage of specific features  can result in wildly different classification times
similarly switching reasoners  can result in wildly different classification times
an ontology which are performancedegrading for a given reasoner
in this paper we investigate performance variability phenomena in owl ontologies
present methods to identify subsets of an ontology
when the remainder is much easier for the given reasoner to reason over we designate ideally small subsets  hot spots 
when such  ideally small  subsets are removed from an ontology we designate ideally small subsets  hot spots 
when such  ideally small  subsets are removed from an ontology we designate such subsets  hot spots 
when the remainder is much easier for the given reasoner to reason over we designate such subsets  hot spots 
the identification of these hot spots allows users to isolate difficult portions of the ontology in a principled way
the identification of these hot spots allows users to isolate difficult portions of the ontology in a systematic way
moreover we compare various methods for approximate reasoning based on hot spots
moreover we devise various methods for approximate reasoning based on hot spots
moreover we devise various methods for knowledge compilation based on hot spots
moreover we compare various methods for knowledge compilation based on hot spots
classification time using approximate reasoning based on hot spots
we were able to firstly successfully identify performance hot spots against the major freely available dl reasoners and secondly significantly improve classification time
we verify we techniques with a select set of varyingly difficult ontologies from the ncbo bioportal firstly successfully identify performance hot spots against the major freely available dl reasoners and secondly significantly improve classification timewe propose a framework for querying probabilistic instance data in the presence of an owl2 ql ontology arguing that the interplay of ontologies is fruitful in many applications such as managing data
we propose a framework for querying probabilistic instance data in the presence of an owl2 ql ontology arguing that the interplay of probabilities is fruitful in many applications such as managing data
managing data that was extracted from the web
it can be implemented using standard probabilistic database systems
the prime inference problem is computing answer probabilities
we establish a ptime vs  p dichotomy for the data complexity of the prime inference problem by lifting a corresponding result from probabilistic databases
we also demonstrate that query rewriting  backwards chaining  is an important tool for our framework
we also briefly discuss approximation of answer probabilities
we also show that nonexistence of a rewriting into firstorder logic implies  phardnesswe introduce srbench  completely based on realworld data sets from the linked open data cloud
we introduce a generalpurpose benchmark primarily designed for streaming rdfsparql engines completely based on realworld data sets from the linked open data cloud
solutions in which semantic web technologies are extended for publishing sharing understanding streaming data
with enough tools to gain knowledge from too much streaming data researchers have set out for solutions
solutions in which semantic web technologies are adapted for publishing sharing understanding streaming data
with the increasing problem of too much streaming data researchers have set out for solutions
solutions in which semantic web technologies are adapted for publishing sharing analysing streaming data
solutions in which semantic web technologies are extended for publishing sharing analysing streaming data
srbench with which one can assess the abilities of a strrs engine to cope with a broad range of use cases typically encountered in realworld scenarios
to help researchers comparing streaming rdfsparql  engines in a standardised application scenario we have designed srbench
to help researchers comparing strrs  engines in a standardised application scenario we have designed srbench
to help users comparing strrs  engines in a standardised application scenario we have designed srbench
to help users comparing streaming rdfsparql  engines in a standardised application scenario we have designed srbench
the data sets used in the benchmark
the data sets have been carefully chosen such that the data sets represent a realistic usage of streaming data
the data sets have been carefully chosen such that the data sets represent a relevant usage of streaming data
the data sets used in the benchmark
the benchmark defines a concise yet comprehensive set of queries
queries that cover the major aspects of strrs processing
finally we work is complemented with a functional evaluation on three representative strrs engines cqels
finally we work is complemented with a functional evaluation on three representative strrs engines csparql
finally we work is complemented with a functional evaluation on three representative strrs engines sparqlstream
the presented results illustrate the stateoftheart
the presented results are meant to give a first baselinedecentralised approaches that can provide fresher results over the entire web at the cost of slower response times
centralised approaches that can efficiently answer queries over data
data cached from parts of the web
centralised approaches that can efficiently live decentralised approaches
for linked data query engines there are inherent tradeoffs between centralised approaches
a hybrid query execution approach that returns fresher results from a broader range of sources vs the centralised scenario
herein we propose a hybrid query execution approach while speeding up results vs the live scenario
we first compare results from two public sparql stores against current versions of the linked data sources two public sparql stores against current versions of the linked data sources they cache cache results are often missing
we first compare results from two public sparql stores against current versions of the linked data sources two public sparql stores against current versions of the linked data sources they cache cache results are outofdate
a subquery that should instead be run live
we thus propose using coherence estimates to split a query into a subquery
the cached data
a subquery for which the have good fresh coverage
we thus propose using coherence estimates to split a query into a subquery
finally we evaluate split positions in a realworld setup
finally we evaluate different hybrid query plans
the time taken vs fully live execution
we results show that hybrid query execution can improve freshness vs fully cached results while reducing the timere  system which learns grammarbased re rules from the web by utilizing large numbers of relation instances as seed
we present a largescale relation extraction  system
a largescale relation extraction  system which learns grammarbased re rules from the web by utilizing large numbers of relation instances as seed
we present re  system
we goal is to obtain rule sets large enough to cover the actual range of linguistic variation thus tackling the longtail problem of realworld applications
a variant of distant supervision learns several relations in parallel
several relations in parallel enabling a new method of rule filtering
a largescale relation extraction system which learns grammarbased re rules from the web by utilizing large numbers of relation instances as seed
re system which learns grammarbased re rules from the web by utilizing large numbers of relation instances as seed
a largescale relation extraction system detects both binary relations
a largescale relation extraction system detects both nary relations
re system detects both binary relations
re system detects both nary relations
3m sentences extracted from 20m web pages
we target 39 relations from freebase
freebase for which 3m sentences serve as the basis for learning an average of 40k distinctive rules per relation
employing an efficient dependency parser the average run time for each relation is only 19 hours
ones learned from local corpora of different sizes
we demonstrate that the web is indeed needed for a good coverage of linguistic variation
we compare these rules with onesthese matching tools have emerged that automate this task
the inherent heterogeneity of datasets on the semantic web has created a need to interlink the inherent heterogeneity of datasets on the semantic web
in this paper we are interested in what happens if we enrich these matching tools with knowledge of the domain of the ontologies
we examine various methods to decide what constitutes the domain of a given dataset
we explore how to express the notion of a domain in terms usable for an ontology matching tool
we show how we can use this in a matching tool
we show how we can study the effect of domain knowledge on the quality of the alignmentthe paper presents an approach for costbased query planning for sparql queries issued over an owl ontology using the owl direct semantics entailment regime of sparql 11
a model abstraction built by an owl reasoner
the costs are based on information about the instances of properties
properties that are extracted from a model abstraction
the costs are based on information about the instances of classes
classes that are extracted from a model abstraction
a static are presented which use the costs to find optimal optimal execution orders for the atoms of a query
a dynamic algorithm are presented which use the costs to find near optimal execution orders for the atoms of a query
a static are presented which use the costs to find near optimal execution orders for the atoms of a query
a dynamic algorithm are presented which use the costs to find optimal optimal execution orders for the atoms of a query
an individual clustering approach that allows for computing the cost functions based on one individual sample from a cluster
for the dynamic case we improve the performance by exploiting an individual clustering approach
we experimental study shows that the static outperforms the dynamic one when accurate statistics are available
the static ordering usually
this changes however when the statistics are due to nondeterministic reasoning decisions
this changes however when the statistics are less accuratedetecting the difference between two description logic is challenging for ontology engineers due in part to the possibility of complex nonlocal logic effects of axiom changes
detecting much less understanding is challenging for ontology engineers due in part to the possibility of complex nonlocal logic effects of axiom changes
two description logic based ontologies
first it is often quite difficult to even determine which concepts have had concepts
concepts meaning altered by a change
second once a concept change is pinpointed the problem of distinguishing whether the concept is indirectly affected by a change has yet to be tackled
second once a concept change is pinpointed the problem of distinguishing whether the concept is directly affected by a change has yet to be tackled
to address the first issue various principled notions of  semantic diff   based on deductive inseparability  have been shown to be computationally practical for the expressively restricted case of elhrterminologies
to address the first issue various principled notions of  semantic diff   based on deductive inseparability  have been proposed in the literature
sroiq which underly
however problems arise even for such limited logics as alc first computation gets more difficult becoming undecidable for logics such as sroiq the web ontology language
second the presence of disjunction make the standard semantic difference too sensitive to change essentially any logically effectual change always affects all terms in the ontology
second the presence of negation make the standard semantic difference too sensitive to change essentially any logically effectual change always affects all terms in the ontology
in order to tackle these issues we formulate the central notion of finding the minimal change set based on model inseparability
changes which are specific to  thus directly affect 
present a method to differentiate changes particular concept names
compare the variously approximated change sets over a series of versions of the nci thesaurus
subsequently we devise a series of computable approximationsa system that incrementally executes sparql queries on a hadoop cluster
a system that incrementally translates sparql queries to pig latin
we describe a system
a system that incrementally translates sparql queries to pig latin
a system is designed to work efficiently on complex queries with many selfjoins over huge datasets avoiding job failures even in the case of joins with unexpected highvalue skew
a system that incrementally executes them on a hadoop cluster
statistics gathered during the previous step
to be robust against cost estimation errors a system interleaves query optimization with query execution determining the next steps to take based on statistics
a system that incrementally executes them on a hadoop cluster
a system that incrementally translates sparql queries to pig latin
to be robust against cost estimation errors a system interleaves query optimization with query execution determining the next steps to take based on data samples
furthermore we have developed a novel skewresistant join algorithm
tuples corresponding to popular keys
algorithm that replicates tuples
we evaluate the effectiveness of we approach both on a synthetic benchmark as well as on a yahoo case of data analysis
a synthetic benchmark known to generate complex queries
a yahoo case of data analysis using rdf data crawled from the web
we results indicate that a system is indeed capable of processing huge datasets without precomputed statistics while exhibiting good loadbalancing properties
a system that incrementally translates sparql queries to pig latin
a system that incrementally executes them on a hadoop clusterdetermining trust of data available in the semantic web is fundamental for applications in particular for linked open data endpoints
determining trust of data available in the semantic web is fundamental for users in particular for linked open data endpoints
linked open data obtained from sparql
there exist several proposals in the literature to annotate sparql query results with values from abstract models adapting the seminal works on provenance for annotated relational databases
we provide an approach capable of providing provenance information for for the first time the major nonmonotonic constructs under multiset semantics
we provide an approach capable of providing provenance information for a large fragment 
we provide an approach capable of providing provenance information for a significant fragment 
the approach is based on the translation of sparql into relational queries over annotated relations with values of the most general msemiring also refuting a claim in the literature that the optional construct of sparql can not be captured appropriately with the known abstract models
the approach is based on the translation of sparql into relational queries over annotated relations in this way also refuting a claim in the literature that the optional construct of sparql can not be captured appropriately with the known abstract modelsin rdf a blank node  is a node in an rdf graph
in rdf or bnode  is a node in an rdf graph
an rdf graph which is not identified by a uri
an rdf graph which is not a literal
in rdf or anonymous resource  is a node in an rdf graph
several rdfs knowledge bases attributes  either associations with other resources  are known
several rdfs knowledge bases rely heavily on blank nodes as several rdfs knowledge bases several rdfs knowledge bases are convenient for representing complex attributes or resources
complex attributes or resources whose identity is unknown
several rdfs knowledge bases attributes  either literals with other resources  are known
several rdfs knowledge bases attributes  either literals with other resources  are known
several rdfs knowledge bases attributes  either associations with other resources  are known
in this paper we show how we can exploit blank nodes anonymity in order to reduce diff  size when comparing such several rdfs knowledge bases
in this paper we show how we can exploit blank nodes anonymity in order to reduce the delta  size when comparing such several rdfs knowledge bases
the main idea of the proposed method is to build a mapping between the bnodes of the compared several rdfs knowledge bases for reducing the delta size
we prove that finding the optimal mapping is nphard in the general case
we prove that finding the optimal mapping is polynomial in case there are not directly connected bnodes
subsequently we present various polynomial algorithms returning approximate solutions for the general casedata provenance is the history of derivation of a data artifact from data provenance original sources
as the reallife provenance records can likely cover thousands of derivation steps one of the pressing challenges becomes development of formal frameworks for thousands of derivation steps automated verification
as the reallife provenance records can likely cover thousands of data items one of the pressing challenges becomes development of formal frameworks for thousands of data items automated verification
as the reallife provenance records can likely cover thousands of derivation steps one of the pressing challenges becomes development of formal frameworks for thousands of data items automated verification
as the reallife provenance records can likely cover thousands of data items one of the pressing challenges becomes development of formal frameworks for thousands of derivation steps automated verificationan increasing amount of data is consumed on the web according to the linked data paradigm
an increasing amount of data is published on the web according to the linked data paradigm
in consideration of both publishers and consumers the temporal dimension of data is important
in this paper we investigate the characterisation and availability of temporal information in linked data at large scale
based on an abstract definition of temporal information we conduct experiments to evaluate the availability of such information using linked data at large scale from the 2011 billion triple challenge  btc  dataset
focusing in particular on temporal information we investigate the approaches proposing guidelines for data consumers
focusing in particular on the representation of temporal metainformation we investigate the approaches proposing guidelines for publishers
temporal information associated with rdf statements and graphs
focusing in particular on the representation of temporal metainformation we investigate the approaches performing both a quantitative
focusing in particular on temporal information we investigate the approaches performing both a qualitative analysis
focusing in particular on the representation of temporal metainformation we investigate the approaches performing both a qualitative analysis
the approaches proposed in the literature
focusing in particular on temporal information we investigate the approaches performing both a quantitative
focusing in particular on the representation of temporal metainformation we investigate the approaches proposing guidelines for data consumers
focusing in particular on temporal information we investigate the approaches proposing guidelines for publishers
we experiments show that the amount of temporal information available in the lod cloud is still very small several different models have been used on different datasets with rdfthe linking open data project is an ongoing effort to construct a global data space the web of data
one important part of the linking open data project is to establish owl 
one important part of the linking open data project is to establish sameas links among structured data sources
one important part of the linking open data project is to establish sameas links among structured data sources
one important part of the linking open data project is to establish owl 
such links indicate equivalent instances
equivalent instances that refer to the same realworld object
the problem of discovering owl is called instance matching
the problem of discovering sameas links between pairwise data sources is called instance matching
most of the existing approaches rely on the quality of prior schema matching
prior schema matching which is not always good enough in the open data scenario
the existing approaches addressing the problem of discovering sameas links between pairwise data sources
the existing approaches addressing the problem of discovering owl
in this paper we propose a schemaindependent instancepair similarity metric based on several general descriptive features
we solve the binary classification problem by machine learning algorithms
we transform the instance matching problem to the binary classification problem
some transfer learning methods to utilize sameas links in open data
furthermore we employ some transfer to reduce the demand for labeled data
some transfer learning methods to utilize the existing owl
we carry out experiments on some datasets of oaei2010
the results show that we method performs well on realworld open data data
the results show that we method outperforms the participants of oaei2010tracking user interests over time is important for making accurate recommendations
however the widelyused timedecaybased approach worsens the sparsity problem because the widelyused timedecaybased approach deemphasizes old item transactions
we introduce two ideas to solve the sparsity problem
first we divide the users transactions into epochs time periods
first we identify epochs
epochs that are dominated by interests similar to the current interests of the active user
thus the users can eliminate dissimilar transactions while making use of similar transactions
similar transactions that exist in prior epochs
second we use a taxonomy of items to model user item transactions in each epoch
this well captures the interests of users in each epoch even if there are few transactions
this suits the situations dynamically change over time do not change so often while individual items often appear
this suits the situations dynamically change over time do not change so often while individual items often disappear
the semantics behind classes do not change so often while individual items often disappear
the situations in which the items transacted by users
the semantics behind classes do not change so often while individual items often appear
fortunately many taxonomies are now available on the web because of the spread of the linked open data vision
we can now use those to understand dynamic user interests semantically
a restaurant visit history gathered from a gourmet guide site
one containing a restaurant visit history
we evaluate we method a music extracted from users tweets
method using a dataset
we evaluate we one
a music listening history
the results show that our method predicts user interests much more accurately than the previous timedecaybased methodthe amount of data available in the linked data cloud continues to grow
yet few services produce linked data
yet few services consume linked data
producing linked data requires specialized knowledge of sparql
there is recent work
building such models requires specialized knowledge of sparql
producing linked data requires specialized knowledge of rdf
building such models is time consuming
producing linked data is time consuming
building such models requires specialized knowledge of rdf
an online service which includes the specifications for consuming
recent work that allows a user to define a linked service from an online service
this paper presents a new approach
a new approach that allows domain experts to rapidly create semantic models of services by demonstration in an interactive webbased interface
first the user provides examples of the service request urls
then the system automatically proposes a service model the user can refine interactively
a service specification using a new expressive vocabulary
a new expressive vocabulary that includes lifting rules
a new expressive vocabulary that includes lowering rules
finally the system saves a service specification
a new approach empowers end users to use end users to produce linked data
a new approach empowers end users to use end users to consume linked data
a new approach that allows domain experts to rapidly create semantic models of services by demonstration in an interactive webbased interface
a new approach empowers end users to rapidly model existing servicestimeefficient algorithms are essential to address the complex
tasks that arise when trying to discover links on the web of data
the complex linking tasks
although several lossless approaches have been developed for this exact purpose timeefficient algorithms do not offer theoretical guarantees with respect to timeefficient algorithms performance
in this paper we address this drawback by presenting the first link discovery approach with theoretical quality guarantees
in particular we prove that given an achievable reduction ratio r we link discovery approach hr3 can achieve a reduction ratio r    r in a metric space
  r in a metric space where distances are measured by the means of a minkowski metric of any order p   2
we compare hr3
the hyppo algorithm implemented in limes 05 with respect to the number of comparisons they carry out
in addition we compare we approach with timeefficient algorithms limes silk 25 with respect to runtime
in addition we compare we approach with timeefficient algorithms limes 05 to runtime
timeefficient algorithms implemented in the stateoftheart frameworks
we show that hr3 outperforms these previous approaches with respect to runtime in each of we four experimental setupsthe size of the extracted base of the extracted base with the original one
the size of the corresponding signature of the extracted base with the original one
the syntactic similarity of the extracted base with the original one
three conflicting requirements arise in the context of knowledge base extraction of the extracted base with the original one
minimal module extraction assign an absolute priority to one of these requirements thereby limiting the possibilities to influence the other two
uniform interpolation assign an absolute priority to one of these requirements thereby limiting the possibilities to influence the other two
el that does not require such an extreme prioritization
we propose a novel technique for el
empirically compare a novel technique for el
el that does not require such an extreme prioritization with existing approaches with encouraging results
we propose a tractable rewriting approach
