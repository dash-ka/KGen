the work on integrating sources in the semantic web assumes that the source data is available through a semantic web service
the work on integrating sources in the semantic web assumes that the source data is either already represented in rdf
the work on integrating sources in the semantic web assumes that the source data is either already represented in owl
the work on integrating services in the semantic web assumes that the source data is either already represented in rdf
the work on integrating services in the semantic web assumes that the source data is available through a semantic web service
the work on integrating services in the semantic web assumes that the source data is either already represented in owl
in practice there is a tremendous amount of data on the web
the web that is not available through the semantic web
in this paper we present an approach to automatically discover and create new semantic web services
the idea behind an approach is to discover similar sources extract the source data build semantic descriptions of the sources and then turn similar sources into semantic web services
the idea behind an approach is to start with a set of known sources extract the source data build semantic descriptions of the sources and then turn similar sources into semantic web services
the idea behind an approach is to start with a set of the corresponding semantic descriptions extract the source data build semantic descriptions of the sources and then turn similar sources into semantic web services
we implemented an endtoend solution to this problem in a system
a system called deimos across five different domains
a system called deimos
we evaluated a system
the results demonstrate that a system can automatically build semantic web services with only example sources descriptions as input
the results demonstrate that a system can automatically build semantic only example sources descriptions as input
the results demonstrate that a system can automatically discover
a system called deimos
the results demonstrate that a system can automatically learn semantic descriptionsan important issue for the semantic web is how to combine openworld ontology languages with nonmonotonic  rule paradigms
an important issue for the semantic web is how to combine openworld ontology languages with closedworld  rule paradigms
predicates defined by the rules
several proposals for hybrid languages allow concepts to be simultaneously defined by rules where rules may refer to concepts in the ontology
several proposals for hybrid languages allow concepts to be simultaneously defined by an ontology where rules may refer to concepts in the ontology
several proposals for hybrid languages allow concepts to be simultaneously defined by an ontology where the ontology may also refer to predicates
several proposals for hybrid languages allow concepts to be simultaneously defined by rules where the ontology may also refer to predicates
one such proposal for which both a wellfounded semantics have been defined
one such proposal for which both a stable have been defined
hybrid mknf knowledge bases are one such proposal
the definition of hybrid mknf knowledge bases is parametric on the ontology language in the sense that nonmonotonic rules can extend any decidable ontology language
in this paper we define a querydriven procedure for hybrid mknf knowledge bases
hybrid mknf knowledge bases that is sound with respect to the original stable modelbased semantics
hybrid mknf knowledge bases that is correct with respect to the wellfounded semantics
hybrid mknf knowledge bases that is correct with respect to the wellfounded semantics
a querydriven procedure for hybrid mknf knowledge bases is parametric on an inference engine for reasoning in the on
hybrid mknf knowledge bases that is sound with respect to the original stable modelbased semantics
a querydriven procedure for hybrid mknf knowledge bases is parametric on an inference engine for reasoning in tology language
a querydriven procedure for hybrid mknf knowledge bases is able to answer conjunctive queries
a querydriven procedure for hybrid mknf knowledge bases is based on an extension of a and with some assumptions on the complexity of the oracle compared to the complexity of the ontology language maintains the data complexity of the wellfounded semantics for hybrid mknf knowledge bases
hybrid mknf knowledge bases that is correct with respect to the wellfounded semantics as an interaction with an external oracle
hybrid mknf knowledge bases that is sound with respect to the original stable modelbased semantics
a tabled rule evaluation to capture reasoning within an ontology by modeling a querydriven procedure for hybrid mknf knowledge bases
hybrid mknf knowledge bases that is correct with respect to the wellfounded semantics
hybrid mknf knowledge bases that is sound with respect to the original stable modelbased semanticsconjunctive query answering for el ontologies has recently drawn much attention as the description logic el captures the expressivity of many large ontologies in the biomedical domain and is the foundation for the owl 2 el profile
namely acyclic el that supports role inclusions
in this paper we propose a practical approach for conjunctive query answering in a fragment of el namely acyclic el
a practical approach for conjunctive query answering can be implemented with query answering
a practical approach for conjunctive query answering can be implemented with low cost by leveraging any existing relational database management system to do the abox data completion
a large clinical data set
we conducted a preliminary experiment to evaluate we approach using a large clinical data is practical
we conducted a preliminary experiment to show our approach is practicalrdf schema as a lightweight ontology language is querying are needed
rdf schema as a lightweight ontology language is gaining popularity and consequently tools for scalable rdf schema inference are needed
sparql mostly provides means for querying simple rdf graphs only whereas querying with respect to other entailment regimes is left outside the current specification
sparql mostly provides means for querying simple rdf graphs only whereas querying with respect to rdf schema is left outside the current specification
sparql has become recently a w3c standard for querying rdf data
in this paper
rdf datasets that comprise multiple named graphs
we provide an extension for sparql that remedies these effects
we show that sparql faces certain unwanted ramifications when querying ontologies in conjunction with rdf datasets
moreover since rdf schema inference has a close relationship with logic rules we generalize our approach to select a custom ruleset for specifying inferences to be taken into account in a sparql query
our show that our extensions are technically feasible by providing benchmark results for rdf schema as a backend for implementing sparql with dynamic rulebased inference
rdf schema querying in our prototype system giabata
giabata which uses datalog coupled with a persistent relational database
by employing different optimization techniques like magic set rewriting our system remains competitive with stateoftheart rdf schema querying systemsthe discovery of all subclass relationships between class names occurring in an ontology
the core services provided by owl reasoners
one of the core services is classification the discovery of all subclass relationships between class names
discovering these relations can be computationally expensive if the number of class names is large
discovering these relations can be computationally expensive particularly if individual subsumption tests are costly
we present a classification algorithm
a classification algorithm which exploits partial information about subclass relationships to reduce both the number of individual tests
a classification algorithm which exploits partial information about subclass relationships to reduce both the cost of working with large ontologies
we also describe techniques for extracting such partial information from existing reasoners
empirical results from a prototypical implementation demonstrate substantial performance improvements compared to implementations
empirical results from a prototypical implementation demonstrate substantial performance improvements compared to existing algorithmsthe focus of web search is moving away from returning relevant documents towards returning structured data as results to user queries
linkbased ranking algorithms which however are targeted towards hypertext documents
a vital part in the architecture of search engines are linkbased ranking algorithms
cases where data exhibits enormous variance in vocabularies
existing ranking algorithms for structured data on the other hand require manual input of a domain expert
vocabularies used
data integrated from a large number of sources
existing ranking algorithms for structured data on the other hand are thus not applicable in cases
in such environments the authority of data sources is an important signal that the ranking algorithm has to take into account
this paper presents algorithms for prioritising data
prioritising data returned by queries over web datasets
web datasets expressed in rdf
these identifiers which can speak authoritatively for these identifiers
authority which provides a correspondence between the sources
we introduce the notion of naming authority
the sources which can speak authoritatively for these identifiers
authority which provides a correspondence between these identifiers
we then propagates the authority values to identifiers
identifiers referenced in the sources
we uses the original pagerank method to assign authority values to data sources based on a naming authority graph
we conduct performance evaluations of the method on a large web dataset
we conduct quality evaluations of the method on a large web dataset
the method on a large web dataset has applications in search query processing reasoning over integrated datasets
the method on a large web dataset has applications in user interfaces over integrated datasets
the method on a large web dataset requires no manual input
the method on a large web dataset is schemaindependentscalable query answering over description logic plays an important role for the success of the semantic web
description logic based ontologies
towards tackling the scalability problem we propose a decompositionbased approach to optimizing existing owl description logic reasoners in evaluating conjunctive queries in owl description logic ontologies
the main idea is to decompose a given owl description logic ontology into a set of target ontologies without duplicated abox axioms so that the evaluation of a given conjunctive query can be separately performed in every target ontology by applying existing owl description logic reasoners
the applied owl description logic reasoner correctly
a decompositionbased approach to optimizing existing owl description logic reasoners in evaluating conjunctive queries in owl description logic ontologies guarantees sound results for the category of conjunctive queries that the evaluates
a decompositionbased approach to optimizing existing owl description logic reasoners in evaluating conjunctive queries in owl description logic ontologies guarantees complete results for the category of conjunctive queries that the evaluates
experimental results on benchmark queries show that a decompositionbased approach to optimizing existing owl description logic reasoners in evaluating conjunctive queries in owl description logic ontologies can significantly improve efficiency in evaluating general conjunctive queries
experimental results on large benchmark ontologies show that a decompositionbased approach to optimizing existing owl description logic reasoners in evaluating conjunctive queries in owl description logic ontologies can significantly improve efficiency in evaluating general conjunctive queries
experimental results on large benchmark ontologies show that a decompositionbased approach to optimizing existing owl description logic reasoners in evaluating conjunctive queries in owl description logic ontologies can significantly improve scalability in evaluating general conjunctive queries
experimental results on benchmark queries show that a decompositionbased approach to optimizing existing owl description logic reasoners in evaluating conjunctive queries in owl description logic ontologies can significantly improve scalability in evaluating general conjunctive queriesthis paper presents a decidable fragment for combining rules in ordersorted logic programming
this paper presents a decidable fragment for combining ontologies in ordersorted logic programming
we describe ordersorted logic programming with sort for deriving predicate assertions
we describe ordersorted logic programming with metapredicate hierarchies for deriving metapredicate assertions
we describe ordersorted logic programming with sort for deriving metapredicate assertions
we describe ordersorted logic programming with predicate for deriving predicate assertions
we describe ordersorted logic programming with metapredicate hierarchies for deriving predicate assertions
we describe ordersorted logic programming with predicate for deriving metapredicate assertions
metalevel predicates  are useful for representing relationships between predicate formulas and further predicates of predicates  conceptually yield a hierarchy similar to the hierarchies of sorts
metalevel predicates  are useful for representing relationships between predicate formulas and further metalevel predicates  conceptually yield a hierarchy similar to the hierarchies of sorts
predicates of predicates  are useful for representing relationships between predicate formulas and further metalevel predicates  conceptually yield a hierarchy similar to the hierarchies of predicates
predicates of predicates  are useful for representing relationships between predicate formulas and further predicates of predicates  conceptually yield a hierarchy similar to the hierarchies of sorts
metalevel predicates  are useful for representing relationships between predicate formulas and further metalevel predicates  conceptually yield a hierarchy similar to the hierarchies of predicates
predicates of predicates  are useful for representing relationships between predicate formulas and further metalevel predicates  conceptually yield a hierarchy similar to the hierarchies of sorts
predicates of predicates  are useful for representing relationships between predicate formulas and further predicates of predicates  conceptually yield a hierarchy similar to the hierarchies of predicates
metalevel predicates  are useful for representing relationships between predicate formulas and further predicates of predicates  conceptually yield a hierarchy similar to the hierarchies of predicates
metaatoms generalized by containing predicate variables
a queryanswering system that can answer queries such as atoms
a queryanswering system that can answer queries such as metaatoms
by extending the ordersorted hornclause calculus we develop a queryanswering system
atoms generalized by containing predicate variables
we show that the expressive queryanswering system computes every generalized the complexity of we query system is equal to that of datalog
we show that the expressive queryanswering system computes every generalized query in single exponential time query system is equal to that of datalogthis paper describes how to generate compositions of semantic web services using social trust information from user ratings of the services
we present a taxonomy of features such as availability
we present a taxonomy of features such as privacy
we present a taxonomy of features such as interoperability
we present a taxonomy of features such as security
we present a taxonomy of features such as others
we describe a way to compute social trust in owls style semantic web services
we formalism exploits the users ratings of execution characteristics of owls style semantic web services
we formalism exploits the users ratings of the services of owls style semantic web services
servicecomposition algorithm called trusty
we describe we servicecomposition algorithm that is based on our formalism
we discuss the formal properties of we implementation of our servicecomposition algorithm called trusty that is based on this formalism
we discuss the formal properties of trusty implementation of our servicecomposition algorithm called trusty that is based on this formalism
we present we experiments
experiments in which we compared trusty with a wellknown ai planning algorithm
experiments in which we compared trusty with shop2
a wellknown ai planning algorithm that has been successfully used for owls style service composition
we results demonstrate that trusty generates more trustworthy compositions than shop2ontology modularization techniques identify often reusable regions within an ontology
ontology modularization techniques identify coherent regions within an ontology
the ability to identify such modules thus potentially reducing the size or complexity of an ontology for a given task is increasingly important in the semantic web as domain ontologies increase in terms of expressivity
the ability to identify such modules thus potentially reducing the size or complexity of an ontology for a given task is increasingly important in the semantic web as domain ontologies increase in terms of complexity
the ability to identify such modules thus potentially reducing the size or complexity of an ontology for set of concepts is increasingly important in the semantic web as domain ontologies increase in terms of expressivity
the ability to identify such modules thus potentially reducing the size or complexity of an ontology for set of concepts is increasingly important in the semantic web as domain ontologies increase in terms of complexity
the ability to identify such modules thus potentially reducing the size or complexity of an ontology for set of concepts is increasingly important in the semantic web as domain ontologies increase in terms of size
the ability to identify such modules thus potentially reducing the size or complexity of an ontology for a given task is increasingly important in the semantic web as domain ontologies increase in terms of size
to date many techniques have been developed
evaluation of the results of many techniques is sketchy and somewhat ad hoc
theoretical properties of modularization algorithms have only been studied in a small number of cases
this paper presents an empirical analysis of the modules a number of modularization techniques
the modules they identify over a number of diverse ontologies by utilizing objective taskoriented measures to evaluate the fitness of the modules for a number of statistical classification problems identify over a number of diverse ontologies by utilizing objective taskoriented measures to evaluate the fitness of the modules for a number of statistical classification problems
this paper presents an empirical analysis of a number of modularization techniques of modularization techniquesan increasing number of scientific communities rely on semantic web ontologies to interpret data within and across research domains
an increasing number of scientific communities rely on semantic web ontologies to share data within and across research domains
these common knowledge representation resources are usually developed and maintained manually along with experimental evidence
these common knowledge representation resources are usually developed and maintained essentially coevolve along with experimental evidence
experimental evidence produced by scientists worldwide
visualize these common knowledge representation resources deltas is a challenging task for escience
detecting automatically the differences between versions of the same ontology in order to store resources deltas is a challenging task for escience
concise deltas which are expressive
languages allowing the formulation of intuitive deltas enough to describe unambiguously that can be efficiently detected
languages allowing the formulation of concise deltas enough to describe unambiguously that can be efficiently detected
languages allowing the formulation of concise deltas enough to describe unambiguously any possible change
intuitive deltas which are expressive
in this paper we focus on languages
languages allowing the formulation of intuitive deltas enough to describe unambiguously any possible change
languages allowing the formulation of intuitive deltas enough to describe unambiguously that can be effectively detected
languages allowing the formulation of concise deltas enough to describe unambiguously that can be effectively detected
a specific language that provide a change detection algorithm
we propose a specific language
a specific language that provably exhibits those characteristics
a change detection algorithm which is complete with respect to the proposed language
a change detection algorithm which is sound with respect to the proposed language
we framework using real ontologies from the cultural domains
finally we provide a promising experimental evaluation of we framework
we framework using real ontologies from the bioinformatics domainssemantic annotations taken from an ontology
enriching business process models with semantic annotations has become a crucial necessity both in business processes management
enriching business process models with semantic annotations has become a crucial necessity both in integration
enriching business process models with semantic annotations has become a crucial necessity both in composition
enriching business process models with semantic annotations has become a crucial necessity both in service provisioning
an owl knowledge base that formalises a set of criteria
an owl knowledge base that formalises the business domain
an owl knowledge base that formalises the business process structure
in our work our represent semantically annotated business processes as part of an owl knowledge base describing correct semantic annotations
our show how semantic web representation can be effectively applied to formalise
diagrams that involve both knowledge about the process structure
in this paper our show sets of constraints on business process diagrams
our show how reasoning techniques can be effectively applied to formalise
our show how semantic web representation can be effectively applied to verify
our show how reasoning techniques can be effectively applied to verify
diagrams that involve both knowledge about the domain
our also present a tool for the automated transformation of an annotated business process diagram into an owl ontology
the use of the semantic web techniques presented in this paper we show how reasoning techniques can be effectively applied to verify results in a novel support for the management of business processes in the phase of process modeling
the use of the semantic web techniques presented in this paper we show how semantic web representation can be effectively applied to formalise
the use of the semantic web techniques presented in this paper we show how reasoning techniques can be effectively applied to formalise
the use of tool presented in this paper we show how reasoning techniques can be effectively applied to verify results in a novel support for the management of business processes in the phase of process modeling
process modeling whose usefulness will be illustrated by means of a concrete example
the use of tool presented in this paper we show how reasoning techniques can be effectively applied to formalise
the use of tool presented in this paper we show how semantic web representation can be effectively applied to verify results in a novel support for the management of business processes in the phase of process modeling
process modeling whose feasibility will be illustrated by means of a concrete example
the use of tool presented in this paper we show how semantic web representation can be effectively applied to formalise
the use of the semantic web techniques presented in this paper we show how semantic web representation can be effectively applied to verify results in a novel support for the management of business processes in the phase of process modelingontology matching is one of the key research topics in the field of the semantic web
there are many matching systems or semiautomatically
many matching systems that generate mappings between different ontologies either automatically
however the mappings generated by many matching systems
many matching systems that generate mappings between different ontologies
however the mappings semiautomatically may be inconsistent with the ontologies
however the mappings automatically may be inconsistent with the ontologies
several approaches have been proposed to deal with the inconsistencies between ontologies
several approaches have been proposed to deal with the inconsistencies between mappings
this problem is often called a mapping revision problem as the ontologies are assumed to be correct whereas ontologies are repaired when resolving the inconsistencies between mappings
this problem is often called a mapping revision problem as the ontologies are assumed to be correct whereas mappings are repaired when resolving the inconsistencies between mappings
this problem is often called a mapping revision problem as the ontologies are assumed to be correct whereas mappings are repaired when resolving the inconsistencies between ontologies
this problem is often called a mapping revision problem as the ontologies are assumed to be correct whereas ontologies are repaired when resolving the inconsistencies between ontologies
in this paper we show that it can be characterized by two logical postulates adapted from some existing postulates for belief base revision
in this paper we first propose a conflictbased mapping revision operator
we then provide an algorithm for iterative mapping revision by using an ontology revision operator
we then show that an algorithm defines a conflictbased mapping revision operator
the iterative algorithm which result in three different mapping revision algorithms
three concrete ontology revision operators are given to instantiate the iterative algorithm
we implement three different mapping revision algorithms
we provide some preliminary but interesting evaluation resultsthe web of data is built upon two simple ideas employ the rdf data model to publish structured data on the web of data
the web of data is built upon two simple ideas employ the rdf data model to create explicit data links between entities within different data sources
the web of data presents the silk linking a toolkit for discovering data links between web data sources
the web of data presents the silk linking a toolkit for maintaining data links between web data sources
the web of data presents the silk linking framework 
silk consists of three components 
silk consists of 1
a link discovery engine 2
a link discovery engine which computes links between data sources based on a declarative specification of the conditions that entities must fulfill in order to be interlinked
a tool for evaluating the 3
the generated data links in order to finetune the linking specification
a protocol for maintaining data links between continuously changing data sources
a protocol for maintaining data links between continuously changing data sources
allows data sources to exchange both linksets
allows data sources to exchange detailed change information
enables continuous link recomputation
the interplay of all the components is demonstrated within a life science use caserecently the w3c linking open data effort has boosted the publication and interlinkage of large amounts of rdf datasets on the semantic web
knowledge bases with millions of rdf triples from wikipedia mostly in escience have been created
various ontologies with millions of rdf triples from wikipedia mostly in escience are publicly available
various ontologies with millions of rdf triples from wikipedia mostly in escience have been created
various ontologies with millions of rdf triples from other sources mostly in escience have been created
knowledge bases with millions of rdf triples from other sources mostly in escience are publicly available
various ontologies with millions of rdf triples from other sources mostly in escience are publicly available
knowledge bases with millions of rdf triples from other sources mostly in escience have been created
knowledge bases with millions of rdf triples from wikipedia mostly in escience are publicly available
recording provenance information of rdf triples is crucial in order to effectively support trust mechanisms
recording provenance information of rdf triples is crucial in order to effectively support digital rights
recording provenance information of rdf triples is crucial in order to effectively support privacy policies
rdf triples aggregated from different heterogeneous sources
managing provenance becomes even more important when we consider not only explicitly stated for updating rdf graphs
managing provenance becomes even more important when we consider implicit triples  through rdfs inference rules  in conjunction with declarative languages for updating rdf graphs
managing provenance becomes even more important when we consider implicit triples  through rdfs inference rules  in conjunction with declarative languages for querying rdf graphs
managing provenance becomes even more important when we consider not only explicitly stated for querying rdf graphs
in this paper we rely on colored rdf triples represented as quadruples to capture explicit provenance information
in this paper we rely on colored rdf triples represented as quadruples to manipulate explicit provenance informationsemantic formalisms represent content in a uniform way according to ontologies
this limits the users ability to explore the semantic data from a point of view
this enables reasoning via automated means 
this enables manipulation via semantic web services 
view that originates from knowledge representation motivations
this enables manipulation via automated means 
this enables reasoning via semantic web services 
we show how for user consumption a visualization of semantic data according to space  provides effective sensemaking of data
we show how for user consumption a visualization of semantic data according to some easily graspable dimensions  provides effective sensemaking of data
we show how for user consumption a visualization of semantic data according to time  provides effective sensemaking of data
in this paper we look holistically at the interaction between semantic data
in this paper we propose dynamic filters to support the exploration of semanticrich data
in this paper we look holistically at the interaction between users
in this paper we propose multiple visualization strategies to support the exploration of semanticrich data
we discuss a user evaluation
we discuss how interaction challenges could be overcome to create an effective usercentred framework for the visualization and manipulation of semantic data
the approach has been implemented on a real company archive
the approach has been evaluated on a real company archivethe rdfs closure that are often ignored such as literal generalization
the rdfs closure that are often ignored such as container membership properties
in this paper we consider the problem of materializing the complete finite rdfs closure in a scalable manner this includes those parts of the rdfs closure
128 cores using differentsize subsets of the lubm 10000university data
we point out characteristics of rdfs
rdfs that allow we to derive an embarrassingly parallel algorithm for producing said closure
we evaluate we cmpi implementation of the algorithm on a cluster with 128 cores
the lubm 10000university data set
we show that the time to produce inferences scales linearly with the number of processes evaluating this behavior on up to hundreds of millions of triples
inferences produced for different subsets of lubm10k
we also show the number of inferences
to the best of we knowledge we work is the first to provide rdfs inferencing on such large data sets in such low times
finally we discuss future work in terms of promising applications of this approach
massive scaling on supercomputers
finally we discuss future work in terms of promising applications of
this approach
owl2rl rules
mapreduce implementationsthe web of linked data forms a single globally distributed dataspace
all data sources that might be relevant for query answering
due to the openness of this dataspace it is not possible to know in advance all data sources
a new challenge that is not addressed by traditional research on federated query processing
the openness of this dataspace poses a new challenge
in this paper we present an approach to execute sparql queries over the web of linked data
data that might be relevant for answering a query during the query execution the query execution
the main idea of we approach is to discover data itself
the query execution itself is driven by following rdf links between data sources based on the uris
the query execution itself is driven by following rdf links between data sources based in partial results
the uris are resolved over the http protocol into rdf data
rdf data which is continuously added to the queried dataset
this paper describes concepts to implement our approach using an iteratorbased pipeline
this paper describes algorithms to implement our approach using an iteratorbased pipeline
we introduce a formalization of the pipelining approach and show that classical iterators may because blocking due to the latency of http requests
to avoid blocking we propose an extension of the iterator paradigm
the evaluation of we approach shows the query execution itself strengths as well as the still existing challengesthe success of significant efforts can be seen by the impressive number of web sites exposing data in rdfxml
significant efforts have focused in the past years on bringing large amounts of metadata online
the success of significant efforts can be seen by the impressive number of web sites exposing data in rdfa
however little is known about the extent to which this data fits the needs of ordinary web users with everyday information needs
the queries submitted to a major web search engine
in this paper we study what we perceive as the semantic gap between the supply of data on the semantic web in the needs of web users as expressed in the queries
in this paper we study what we perceive as the semantic gap between the supply of data on the semantic web in general needs of web users as expressed in the queries
we perform we analysis on both the level of instances
we perform we analysis on both ontologies
first we first look at how much data is actually relevant to web queries
first we first look what kind of data is it
second we provide a generic method to extract the attributes that web users are searching for regarding particular classes of entities
contrast class definitions found in the semantic web in general vocabularies with the attributes of objects that users are interested in
a generic method to extract the attributes that web users are searching for regarding particular classes of entities allows to contrast class definitions
we findings are crucial to measuring the potential of semantic search
we findings also speak to the state of the semantic web in generalscenarios where selected subontologies of a large ontology are offered as views to users based on criteria like the users access right
the framework can deal with scenarios
the trust level required by the application
the framework can deal with the level of detail
detail requested by the users
the framework can deal with the trust level
the framework developed in this paper
instead of materializing a large number of different subontologies we propose to keep just one ontology
instead of materializing a large number of different subontologies we propose to equip each axiom with the consequence label
the corresponding subontology is determined by comparing the consequence label with the axiom labels
the access right required trust level is then also represented by a label  called user label  from an appropriate labeling lattice
for largescale ontologies certain consequence  like the concept hierarchy  are often precomputed
instead of precomputing these consequences for every possible subontology our approach computes just one label for each consequence such that a comparison of the user label with the consequence label determines whether the consequence follows from the corresponding subontology or notmodule extraction methods have proved to be effective in improving the performance of some ontology reasoning tasks including finding justifications to explain why an entailment holds in an owl dl ontology
the existing module extraction methods that compute a syntactic localitybased module for the subconcept in a subsumption entailment though ensuring the resulting module to preserve all justifications of a subsumption entailment
however the existing module extraction methods may be insufficient in improving the performance of finding all justifications
this is because a syntactic localitybased module always contains all conceptrole assertions
this is because a syntactic localitybased module is independent of the superconcept in a subsumption entailment
in order to extract smaller modules to further optimize finding all justifications in an owl dl ontology we propose a goaldirected method for extracting a module
a module that preserves all justifications of a given entailment
a module extracted by our method
experimental results on large ontologies show that a module is smaller than the corresponding syntactic localitybased module making the subsequent computation of all justifications more scalable
experimental results on large ontologies show that a module is smaller than the corresponding syntactic localitybased module making the subsequent computation of all justifications more efficientan important application of semantic web technology is recognizing humandefined concepts in text
facilities that let users specify
facilities that let users reformulate users queries
facilities that let users complete
query transformation is a strategy often used in search engines to derive queries
queries that are able to return more useful search results than the original query provide facilities
queries that are able to return more useful search results than most popular search engines provide facilities
we study the problem of a special type of query transformation based on identifying semantic concepts
we study the problem of semantic query suggestion based on identifying semantic concepts
semantic concepts contained in user queries
we use a featurebased approach in conjunction with supervised machine learning augmenting termbased features with conceptspecific features
we use a featurebased approach in conjunction with supervised machine learning augmenting termbased features with search historybased
we apply we method to the task of linking queries from realworld query logs  to the dbpedia knowledge base
we apply we method to the task of linking queries from the transaction logs of the netherlands institute for vision  to the dbpedia knowledge base
we apply we method to the task of linking queries from the transaction logs of the netherlands institute for sound  to the dbpedia knowledge base
we show significant improvements over an already high baseline
we evaluate the utility of different machine learning feature types in identifying semantic concepts using a manually developed test bed
we evaluate the utility of different machine learning features in identifying semantic concepts using a manually developed test bed
we evaluate the utility of different machine learning algorithms in identifying semantic concepts using a manually developed test bed
queries human assessments are available for download
queries extracted features are available for download
the resources developed for this paperontocase is a framework for semiautomatic patternbased ontology construction
in this paper we focus on the reuse phases where an initial ontology is enriched based on content ontology design patterns 
in this paper we focus on the retain phases where an initial ontology is enriched based on especially the implementation and evaluation of these phases
in this paper we focus on the reuse phases where an initial ontology is enriched based on especially the implementation and evaluation of these phases
in this paper we focus on the retain phases where an initial ontology is enriched based on content ontology design patterns 
in this paper we focus on the retain phases where an initial ontology is enriched based on content odps 
in this paper we focus on the reuse phases where an initial ontology is enriched based on content odps 
applying content odps within semiautomatic ontology construction ontology learning is a novel approach
ontologies constructed automatically based on odps
the main contributions of this paper are the methods for the subsequent evaluation
the main contributions of this paper are the methods for pattern ranking
the main contributions of this paper are the methods for pattern selection
the subsequent evaluation showing the characteristics of ontologies
the main contributions of this paper are the methods for pattern integration
we show that it is possible to improve the results of existing ontology learning methods by selecting content odps
we show that it is possible to improve the results of existing ontology learning methods by reusing content odps
by exploiting background knowledge the ontology is given a richer overall structure
ontocase is able to introduce a general top structure into the ontologiesstateoftheart discovery of semantic web services is based on hybrid algorithms
hybrid algorithms that combine syntactic matchmaking
hybrid algorithms that combine semantic matchmaking
the results returned by the service
these approaches are purely based on similarity measures between parameters of a service request
these approaches are available service descriptions which however fail to completely capture the actual functionality of the service
these approaches are the quality of the results
on the other hand with the advent of web 20 collaboration has become an increasingly popular trend
on the other hand with the advent of web 20 active user participation has become an increasingly popular trend
users often rate providing valuable information improve the accuracy of search results
users often group relevant items thus providing valuable information improve the accuracy of search results
valuable information that can be taken into account to further
in this paper we tackle this issue by proposing a method to further improve the results of the matchmaker
a method that combines multiple matching criteria with user feedback
we extend a previously proposed dominancebased approach for service discovery
we describe how user feedback is incorporated in the matchmaking process
we evaluate the performance of we approach using a publicly available collection of owls servicesontologies are tools for structuring knowledge with many applications in analyzing complex knowledge bases
ontologies are tools for describing knowledge with many applications in analyzing complex knowledge bases
ontologies are tools for describing knowledge with many applications in searching complex knowledge bases
ontologies are tools for structuring knowledge with many applications in searching complex knowledge bases
since building many applications in analyzing complex knowledge bases manually is a costly process there are various approaches for bootstrapping ontologies automatically through the analysis of appropriate documents
since building many applications in searching complex knowledge bases manually is a costly process there are various approaches for bootstrapping ontologies automatically through the analysis of appropriate documents
such an analysis needs to find the relationships
the relationships that should form the ontology
such an analysis needs to find the concepts
however since relationship extraction methods can not homogeneously cover all concepts the initial set of relationships is usually inconsistent a problem which to the best of our knowledge was mostly ignored so far
however since relationship extraction methods can not homogeneously cover all concepts the initial set of relationships is rather imbalanced a problem which to the best of our knowledge was mostly ignored so far
however since relationship extraction methods are imprecise the initial set of relationships is rather imbalanced a problem which to the best of our knowledge was mostly ignored so far
however since relationship extraction methods are imprecise the initial set of relationships is usually inconsistent a problem which to the best of our knowledge was mostly ignored so far
in this paper our define the problem of extracting a consistent as well as properly structured ontology from a set of heterogeneous relationships
in this paper our define the problem of extracting a consistent as well as properly structured ontology from a set of inconsistent relationships
moreover we compare three graphbased methods for solving the ontology extraction problem
moreover we propose three graphbased methods for solving the ontology extraction problem
our extract relationships from a largescale data set of more than 325k documents
our extract relationships from evaluate our methods against a gold standard ontology
a gold standard ontology comprising more than 12k relationships
an algorithm based on a modified formulation of the dominating set problem
our study shows that an algorithm outperforms greedy methodssocial network analysis provides graph algorithms to characterize strategic positions in social networks specific subnetworks
social network analysis provides graph algorithms to characterize decompositions of people
social network analysis provides graph algorithms to characterize decompositions of activities
social network analysis provides graph algorithms to characterize the structure of social networks
huge social networks enabling people to connect
online social platforms like facebook form huge social networks share people online activities across several social applications
online social platforms like facebook form huge social networks interact people online activities across several social applications
we extended social network analysis  operators
we extended social network analysis  operators
social network analysis  operators using semantic web frameworks to include the semantics of these graphbased representations when analyzing such social networks
social network analysis  operators using semantic web frameworks to include the semantics of these graphbased representations when to deal with the diversity of interactions
social network analysis  operators using semantic web frameworks to include the semantics of these graphbased representations when analyzing such social networks
social network analysis  operators using semantic web frameworks to include the semantics of these graphbased representations when to deal with the diversity of such social networks relations
social network analysis  operators using semantic web frameworks to include the semantics of these graphbased representations when to deal with the diversity of such social networks relations
social network analysis  operators using semantic web frameworks to include the semantics of these graphbased representations when to deal with the diversity of interactions
we present here the results of this approach connecting interacting content
this approach when it was used to analyze a real social network with 60000 users
we present here the results of this approach connecting sharing contentscalable distributed reasoning proposing a technique for materialising the closure of an rdf graph based on mapreduce
we address the problem of scalable distributed reasoning
we have implemented we approach on top of hadoop
we have deployed hadoop on a compute cluster of up to 64 commodity machines
we show that we present several nontrivial optimisations
we show that a naive implementation on top of mapreduce is straightforward
we show that a naive implementation on top of mapreduce performs badly
we algorithm is scalable
we algorithm allows us to compute the rdfs closure of 865m triples from the web  producing 30b triples  in less than two hours faster than any other published approachthe ways in which services can be composed
to direct automated web service composition it is compelling to provide a template workflow or scaffolding
a template workflow or scaffolding that dictates the ways
web service composition that builds on work
in this paper we present an approach to web service composition
work using hierarchical task networks for web service composition
work using ai planning for web service composition
composition based upon the needs of the available services
a significant advantage of we approach is that our approach provides much of the howto knowledge of a choreography while enabling optimization of composition
a significant advantage of we approach is that our approach provides much of the howto knowledge of a choreography while enabling customization of integrated web service selection
composition based upon the needs of the preferences of the customer
a significant advantage of we approach is that our approach provides much of the howto knowledge of a choreography while enabling optimization of integrated web service selection
a significant advantage of we approach is that our approach provides much of the howto knowledge of a choreography while enabling customization of composition
composition based upon the needs of the specific problem
many customers must also be concerned with enforcement of regulations perhaps in the form of corporate policies andor government regulations
regulations are traditionally enforced at design time by verifying that a workflow or composition adheres to regulations
we approach supports optimization enforcement all at composition construction time
we approach supports customization enforcement all at composition construction time
we approach supports regulation enforcement all at composition construction time
to maximize efficiency we have developed novel search heuristics together with a branch
to maximize efficiency we bound search algorithm
search algorithm that enable the generation of high quality compositions with the performance of stateoftheart planning systemsthis paper explores the application of statistical nlp techniques to improve named entity annotation in challenging informal english domains
this paper explores the application of restricted relationship graphs to improve named entity annotation in challenging informal english domains
we validate we approach using online forums discussing popular music
named entity annotation is particularly difficult in this domain because named entity annotation is characterized by a large number of ambiguous entities such as the madonna album  music  or lilly allens pop hit  smile human users  which are difficult to handle for large data sets
in tabular  which are difficult to handle for large data sets
rdf data are usually accessed using one of two methods either graphs are rendered in forms perceivable by in graphical form 
in graphical form  which are difficult to handle for large data sets
rdf data are usually accessed using one of two methods either graphs are rendered in forms perceivable by human users 
rdf data are usually accessed using one of two methods either graphs are rendered in forms perceivable by in tabular 
alternatively query languages like sparql provide means to express information needs in structured form hence query languages like sparql are targeted towards experts
alternatively query languages like sparql provide means to express information needs in structured form hence query languages like sparql are targeted towards developers
spreadsheet tools where users can perform relatively complex calculations by splitting formulas and values across multiple cells
inspired by the concept of spreadsheet tools we have investigated mechanisms yet formally grounded manner
mechanisms that allow we to access rdf graphs in a more intuitive and manageable
in this paper we make three contributions towards this direction
first we present rdfunctions an algebra contained in a background graph
an algebra that consists of mappings between sets of blank nodes  under consideration of the triples
an algebra that consists of mappings between sets of rdf language elements  under consideration of the triples
an algebra that consists of mappings between sets of uris  under consideration of the triples
an algebra that consists of mappings between sets of literals  under consideration of the triples
rdfunctions which can be edited parsed
second we define a syntax for expressing rdfunctions
rdfunctions which can be edited evaluated
third we discuss an implementation of rdfunctions
rdfunctions using a spreadsheet metaphor
third we discuss tripcel
using this tool users can easily edit function expressions
using this tool users can easily perform analysis tasks on the data
using this tool users can easily execute function expressions
the data stored in an rdf graphthe field of biomedicine has embraced the semantic web probably more than any other field
as a result there is a large number of biomedical ontologies
biomedical ontologies covering overlapping areas of the field of biomedicine
we have developed an open communitybased repository of the ontologies themselves
we have developed bioportal 
we analyzed ontologies in bioportal creating more than 4 million mappings between concepts in these ontologies and terminologies based on the lexical similarity of concept names and synonyms
we analyzed ontologies in the unified medical language system creating more than 4 million mappings between concepts in these ontologies and terminologies based on the lexical similarity of concept names and synonyms
we analyzed terminologies in the unified medical language system creating more than 4 million mappings between concepts in these ontologies and terminologies based on the lexical similarity of concept names and synonyms
we analyzed terminologies in bioportal creating more than 4 million mappings between concepts in these ontologies and terminologies based on the lexical similarity of concept names and synonyms
we then analyzed what the mappings tell us about the ontologies the mappings
we then analyzed what the mappings tell us about the ways
we then analyzed what the mappings tell us about the structure of the ontology repository
the ways in which the mappings can help in the process of ontology design and evaluation
we then analyzed the mappings
the domain that are not covered sufficiently by the ontologies themselves in the ontology repository
for example we can use the mappings to guide users
users who are new to a field to the most pertinent ontologies in the field of biomedicine to identify which ontologies will serve
users who are new to a field to the most pertinent ontologies in the field of biomedicine to identify areas of the domain
for example we can use the mappings to guide background knowledge in domainspecific tools
while we used a specific  but large  ontology repository for the study we believe that the lessons we learned about the value of a largescale set of mappings to ontology users and developers apply in many other domains
while we used a specific  but large  ontology repository for the study we believe that the lessons we learned about the value of a largescale set of mappings to ontology users and developers are generalforgetting is an important tool for reducing ontologies by eliminating some concepts and roles while preserving sound
forgetting is an important tool for reducing ontologies by eliminating some concepts and roles while preserving complete reasoning
these attempts have previously been made to address the problem of forgetting in relatively simple description logics such as dllite
these attempts extended el
the ontologies were kbs 
the ontologies used in these attempts
the ontologies were mostly restricted to tboxes 
the ontologies were mostly restricted to general knowledge bases 
however the issue of forgetting for general kbs in more expressive description logics such as owl relatively simple description logic is largely unexplored
however the issue of forgetting for general kbs in more expressive description logics such as alc is largely unexplored
in particular the problem of computing forgetting for such logics is still open
in particular the problem of characterizing forgetting for such logics is still openrdf is an increasingly important paradigm for the representation of information on the web
sophisticated graph matching queries expressible in languages like sparql become increasingly important
as rdf databases increase in size to approach tens of millions of triples scalability becomes an issue
as rdf databases increase in size as sophisticated graph scalability becomes an issue
a way that makes the index diskresident
to date there is no graphbased indexing method for rdf data where the index was designed in a way
indexes that can operate efficiently when the index the index resides on disk
there is therefore a growing need for indexes
in this paper we first propose the dogma index for fast subgraph matching on disk
in this paper we develop a basic algorithm to answer queries over the index
algorithm that uses efficient  but correct  pruning strategies when combined with two different extensions of the index
an optimized algorithm
a basic algorithm is then significantly sped up via an
four existing rdf database systems developed by others
we have implemented a preliminary system
we have tested a preliminary system against four existing rdf database systems
four existing rdf database systems developed by others with orders of magnitude improvements for complex graph queries
we experiments show that a basic algorithm performs very well compared to four existing rdf database systemsprocess modeling is a core task in web service modeling in particular
process modeling is a core task in software engineering in general
process structures based on process entities process activities hierarchical relationship between activities and activities parts temporal relationships between activities conditions on process
the modeling of domain knowledge
the explicit management of process models for purposes such as process selection andor process reuse requires flexible retrieval of process structures flows
process structures based on relationships process activities hierarchical relationship between activities and activities parts temporal relationships between activities conditions on process
the explicit management of process models for purposes such as process selection andor process reuse requires intelligent retrieval of process structures flows
in this paper we analyze requirements for modeling of process models
in this paper we analyze requirements for querying of process models
in this paper we present a patternoriented approach exploiting owldl representation for expressive process modeling and retrieval
in this paper we present a patternoriented approach exploiting reasoning capabilities for expressive process modeling and retrievaldue to significant improvements in the capabilities of small devices such as smart phones small devices such as smart phones also provide web services
due to significant improvements in the capabilities of small devices such as smart phones small devices such as pdas can not only consume
due to significant improvements in the capabilities of small devices such as pdas small devices such as pdas also provide web services
due to significant improvements in the capabilities of small devices such as pdas small devices such as smart phones can not only consume
due to significant improvements in the capabilities of small devices such as pdas small devices such as pdas can not only consume
due to significant improvements in the capabilities of small devices such as smart phones small devices such as pdas also provide web services
due to significant improvements in the capabilities of small devices such as smart phones small devices such as smart phones can not only consume
due to significant improvements in the capabilities of small devices such as pdas small devices such as smart phones also provide web services
the dynamic nature of mobile environment means that users need fast approaches for service discovery
the dynamic nature of mobile environment means that users need accurate approaches for service discovery
in order achieve high accuracy semantic languages can be used in conjunction with logic reasoners
since powerful broker nodes are not always available  due to lack of long range connectivity  create a bottleneck  since mobile devices are all trying to access the same server  onboard mobile reasoning must be supported
since powerful broker nodes are not always available  due to lack of long range connectivity  create single point of failure  in the case that a central server fails  onboard mobile reasoning must be supported
however reasoners are notoriously resource do not scale to small devices
however reasoners are notoriously resource intensive scale to small devices
an efficient mobile reasoner which relaxes the complete matching approaches to support anytime reasoning
therefore in this paper we provide an efficient mobile reasoner
an efficient mobile reasoner which relaxes the current strict to support anytime reasoning
we provides a degree of match result to the user
we approach matches the most important request conditions  deemed by the user  first
we provides a degree of confidence result to the user
we provide performance evaluation of we work
we provide a prototype implementation of we workontology matching plays a key role for semantic interoperability
many methods have been proposed for automatically finding the alignment between heterogeneous ontologies
however in many realworld applications finding the alignment in a completely automatic way is highly infeasible
ideally an ontology matching system would have an interactive interface to allow users to provide feedbacks to guide the automatic algorithm
fundamentally we need answer the following questions how can a system perform an efficiently interactive process with the users
how many interactions are sufficient for finding a more accurate matching
to address the following questions we propose an active learning framework for ontology matching matches to query the users
ontology matching which tries to find the most informative candidate
the users feedbacks are used to 1  correct the mistake matching and 2  propagate the supervise information to help the entire matching process
three measures are proposed to estimate the confidence of each matching candidate
further proposed to maximize the spread of the users  guidance 
a correct propagation algorithm is further
experimental results on several public data sets show that the proposed approach can significantly improve the matching accuracy  80novel applications targeting a more efficient and satisfying exploitation of the data available on faceted browsing of linked open data
novel applications targeting a more efficient and satisfying exploitation of the data available on the web faceted browsing of linked open data
the semantic web fosters novel applications
high diversity of knowledge in the semantic web pose the challenging question of appropriate relevance ranking for producing finegrained descriptions of the available data to guide the user along most promising knowledge aspects
high diversity of knowledge in the semantic web pose the challenging question of appropriate relevance ranking for producing rich descriptions of to guide the user along most promising knowledge aspects
large amounts pose the challenging question of appropriate relevance ranking for producing finegrained descriptions of the available data to guide the user along most promising knowledge aspects
large amounts pose the challenging question of appropriate relevance ranking for producing rich descriptions of the available data to guide the user along most promising knowledge aspects
large amounts pose the challenging question of appropriate relevance ranking for producing finegrained descriptions of to guide the user along most promising knowledge aspects
high diversity of knowledge in the semantic web pose the challenging question of appropriate relevance ranking for producing finegrained descriptions of to guide the user along most promising knowledge aspects
high diversity of knowledge in the semantic web pose the challenging question of appropriate relevance ranking for producing rich descriptions of the available data to guide the user along most promising knowledge aspects
large amounts pose the challenging question of appropriate relevance ranking for producing rich descriptions of to guide the user along most promising knowledge aspects
existing methods for graphbased authority ranking lack support for finegrained latent coherence between predicates support for link semantics in the linked data model 
existing methods for graphbased authority ranking lack support for finegrained latent coherence between resources support for link semantics in the linked data model 
in this paper we present a novel approach for faceted authority ranking in the context of rdf knowledge bases
in this paper we present triplerank 
triplerank captures the additional latent semantics of the semantic web data by means of statistical methods in order to produce richer descriptions of the available data
a novel approach for faceted authority ranking in the context of rdf knowledge bases captures the additional latent semantics of the semantic web data by means of statistical methods in order to produce richer descriptions of the available data
a 3dimensional tensor that enables the seamless representation of arbitrary semantic links
we model the semantic web by a 3dimensional tensor
for the analysis of that model we apply the parafac decomposition
the parafac decomposition which can be seen as a multimodal counterpart to web authority ranking with hits
the result predicates that characterize groupings of resources authority to identified topics
the result predicates that hub  properties with respect to identified topics
the result predicates that navigational  properties with respect to identified topics
the result are groupings of resources
we gathered encouraging feedback in a user evaluation where triplerank results have been exploited in a faceted browsing scenario
we have applied triplerank to multiple data sets from the linked open data communitythe ql profile of owl 2 has been designed so that it is possible to use database technology for query answering via query rewriting
we present a comparison of we resolution
we resolution based rewriting algorithm with the standard algorithm conducting an empirical evaluation using ontologies
ontologies derived from realistic applications
we resolution based rewriting algorithm with the standard algorithm conducting an empirical evaluation using queries
queries derived from realistic applications
we resolution based rewriting algorithm with the standard algorithm implementing both
the standard algorithm proposed by calvanese et al
the results indicate that our algorithm produces significantly smaller rewritings in most cases
most cases which could be important for practicality in realistic applicationsprocessing uncertain emergent knowledge that comes from multiple resources with varying relevance
we present a lightweight framework for processing uncertain emergent knowledge
a lightweight framework for processing uncertain emergent knowledge is essentially rdfcompatible
a lightweight framework for processing uncertain emergent knowledge allows also for direct representation of provenance 
processing uncertain emergent knowledge that comes from multiple resources with varying relevance
a lightweight framework for processing uncertain emergent knowledge allows also for direct representation of contextual features 
we support robust querying of the represented content
the represented content based on wellfounded notions of similarity
the represented content based on wellfounded notions of ranking
we support soft integration of the represented content
the represented content based on wellfounded notions of aggregation
a proofofconcept implementation is presented within large scale knowledgebased search in life science articles
a proofofconcept implementation is evaluated within large scale knowledgebased search in life science articlesin this paper a novel approach is presented for generating rdf graphs of arbitrary complexity from various spreadsheet layouts
currently none of the available spreadsheettordf wrappers supports cross tables and tables where data is not aligned in rows
similar to rdf123 xlwrap is based on template graphs where fragments of triples can be mapped to specific cells of a spreadsheet
additionally similar to rdf123 xlwrap features a full expression algebra based on the syntax of openoffice calc
openoffice calc which can be used to repeat similar mappings in order to wrap
additionally similar to rdf123 xlwrap features a full expression algebra based on the syntax of various shift operations
spreadsheet files
multiple sheets
various shift operations which can be used to repeat similar mappings in order to wrap cross tables
cross tables
openoffice calc which can be used to repeat similar mappings in order to wrap cross tables
various shift operations which can be used to repeat similar mappings in order to wrap
the set of available expression functions can be easily extended by users of xlwrap
the set of available expression functions includes most of the native functions of openoffice calcimages
the web
text
the social web allows users to share the social web work very effectively leading to the rapid reuse and remixing of content on
the social web allows users to share the social web work very effectively leading to the rapid reuse and remixing of content on the web
videos
photo sharing sites known collectively as the social web have lots of increasingly complex information
scientific research data social networks blogs photo
photo sharing other such applications known collectively as the social web have lots of increasingly complex information
such information from several web pages can be mashed up
such information from several web pages can be presented in other web pages
such information from several web pages can be very easily aggregated
content generation of this nature inevitably leads to license violations motivating research into effective methods to prevent such violations
content generation of this nature inevitably leads to license violations motivating research into effective methods to detect such violations
content generation of this nature inevitably leads to many copyright motivating research into effective methods to prevent such violations
content generation of this nature inevitably leads to many copyright motivating research into effective methods to detect such violationsranking of web service compositions are some of the most interesting challenges at present
optimization of web service compositions are some of the most interesting challenges at present
formal semantic descriptions forming the  semantic web services 
since web services can be enhanced with formal semantic descriptions it becomes conceivable to exploit the quality of semantic links between services  of any composition  as one of the optimization criteria
for this we propose to use the semantic similarities between input parameters of web services
for this we propose to use the semantic similarities between output parameters of web services
coupling this with other criteria such as quality of service allow us to rank compositions achieving the same goal
coupling this with other criteria such as quality of service allow us to optimize compositions achieving the same goal
an extensible optimization model designed to balance or functional quality  with nonfunctional coupling this with other criteria such as quality of service metrics
an innovative optimization model designed to balance semantic fit  with nonfunctional coupling this with other criteria such as quality of service metrics
an extensible optimization model designed to balance semantic fit  with nonfunctional coupling this with other criteria such as quality of service metrics
an innovative optimization model designed to balance or functional quality  with nonfunctional coupling this with other criteria such as quality of service metrics
here we suggest an innovative optimization model
here we suggest an extensible optimization model
an extensible optimization model designed to balance semantic fit  with nonfunctional coupling this with other criteria such as quality of service metrics in the context of a large number of services as foreseen by the strategic ecfunded project soa4all
an innovative optimization model designed to balance semantic fit  with nonfunctional coupling this with other criteria such as quality of service metrics in the context of a large number of services as foreseen by the strategic ecfunded project soa4all
to allow the use of an innovative optimization model we propose the use of genetic algorithms
to allow the use of an innovative optimization model we test the use of genetic algorithms
an extensible optimization model designed to balance or functional quality  with nonfunctional coupling this with other criteria such as quality of service metrics in the context of a large number of services as foreseen by the strategic ecfunded project soa4all
an innovative optimization model designed to balance or functional quality  with nonfunctional coupling this with other criteria such as quality of service metrics in the context of a large number of services as foreseen by the strategic ecfunded project soa4all
to allow the use of an extensible optimization model we propose the use of genetic algorithms
to allow the use of an extensible optimization model we test the use of genetic algorithmsin this paper we investigate different technologies to attack the automatic solution of orchestration problems based on a collection of services available on a testbed
in this paper we investigate different technologies to attack the automatic solution of orchestration problems based on a semantically enriched description of the services
in this paper we investigate different technologies to attack the automatic solution of orchestration problems based on synthesis from declarative specifications
in addition to we previously presented tableauxbased synthesis technology we consider two structurally rather different approaches here using the highlevel programming language golog
golog that internally makes use of planning techniques
in addition to we previously presented tableauxbased synthesis technology we consider two structurally rather different approaches here using jmosel golog
in addition to we previously presented tableauxbased synthesis technology we consider two structurally rather different approaches here using we tool for monadic secondorder logic on strings golog
as a common case study we consider the mediation scenario of the semantic web service challenge
the semantic web service challenge which is a benchmark for process orchestration
all three synthesis solutions have been embedded in the jabcjeti modeling framework
all three synthesis solutions have been used to synthesize the abstract mediator processes concrete running  web  service counterpart
all three synthesis solutions have been used to synthesize all three synthesis solutions concrete running  web  service counterpart
using the jabc as a common frame helps highlighting the essential differences and similarities
it turns out at least at the level of complication of the all approaches behave quite similarly both considering the modeling
it turns out at least at the level of complication of the all approaches behave quite similarly both considering the performance
the considered case study
solutions answering questing like when the overhead to achieve compositionality pays of
we believe that turning the jabc framework into experimentation platform along the lines will help understanding the application profiles of the individual synthesis technologies
we believe that turning the jabc framework into experimentation platform along the lines will help understanding the application profiles of the individual synthesis solutions
technologies answering questing like where  heuristic  search is the technology of choice
technologies answering questing like when the overhead to achieve compositionality pays of
solutions answering questing like where  heuristic  search is the technology of choice
the lines presented here
