ABox dependencies which are compiled into a socalled EBox to limit the expansion of the rewriting
A central limitation of this approach however is that this approach can not capture how a users tastes have evolved beforehand thereby ignoring if a users preference for a factor is likely to change
a clientside sparql query processing algorithm that uses a dynamic iterator pipeline
a closed algebra that discuss a closed algebra
a closed algebra that discuss its properties
a closed algebra that underlies SYRql
a closed algebra that underlies SYRql
a consistency layer with which the data can be checked for logical consistency
a crossplatform benchmark framework that supplies 1  a generic standardsbased Semantic Web layer on top of existing mobile reasoning engines
a crossplatform benchmark framework that supplies 2  a benchmark engine to compare mobile reasoning performance
a crossplatform benchmark framework that supplies 2  a benchmark engine to investigate mobile reasoning performance
A data consumer can make A data consumer available through a SPARQL endpoint
A data consumer can make partial copies of different datasets
A data consumer can update A data consumer local copy with consumers
A data consumer can update A data consumer local copy with data providers
A data consumer can update A data consumer share updates with consumers
A data consumer can update A data consumer share updates with data providers
a dataflow language designed to process Semantic Web data at a large scale
a framework for automatically inducing ontology lexica in multiple languages on the basis of a multilingual corpus exploits a set of languagespecific dependency patterns
a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies generalizes the mechanism of JAPE transducers
a heterogeneous graph that includes drugtarget interaction edges
a holistic framework for the extraction of RDF also includes a consistency layer
a key when A set of properties values uniquely identify resources
All datasets we discuss here are freely available online
All datasets we discuss here are updated regularlyDriven by initiatives like Schemaorg the amount of semantically annotated data is expected to grow steadily towards massive scale requiring clusterbased solutions to query clusterbased solutions
all entailed triples explicitly
Also a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies provides a tool
Also Semano provides a tool
Altogether Facebook consist of almost 30 billion RDF quads
A mobile app for Android devices is built on top of the LOD to improve multimedia content browsing
Among others we find that license metadata is still rarely provided by the data sources
Among others we find that provenance metadata is still rarely provided by the data sources
Among others we find that the number of linked datasets has approximately doubled between 2011
Among others we find that the number of linked datasets has approximately doubled between 2014SPARQL 11 supports the use of ontologies to enrich query results with logical entailments
Among others we find that there is increased agreement on common vocabularies for describing certain types of entities
an algorithm that operates over streams of RDF annotated EPCIS events to generate linked pedigrees
an approach that discovers keys in RDF data in an efficient way
an approach which combines the outcomes of two independent outlier detection runs to also prevent problems
an approach which combines the outcomes of two independent outlier detection runs to get a more reliable result
an empirical evaluation comparing with a stateoftheart OntologyBased Data Access system
An empirical evaluation is conducted to demonstrate the effectiveness and accuracy of the system in the multimedia domainGoogles introduce a new methodology for benchmarking the performance per watt of semantic web reasoners with information critical for deploying semantic web tools on powerconstrained devices
an implementation that compare the performance to other big data processing languagesLinked Open Data faces severe issues of availability quality
an implementation that translates SYRql scripts into a series of MapReduce jobs
an indepth experimental analysis that shows existing SPARQL benchmarks are not suitable for testing systems for diverse queries
an indepth experimental analysis that varied workloads
an initial rule base can be easily extended to meet the requirements of the application in question
an item for which Matrix Factorisation has not rated the semantic categories previously
an OntologyBased Data Access system comprising bidirectional evaluation
a novel method that combines a data mining framework for link prediction
a novel method that combines an algorithmic approach to partition
a novel method that combines Our use semantic knowledge  from ontologies
a novel method that combines Our use semantic knowledge  from semantic spaces
a novel method that combines semantic knowledge  from ontologies
a novel method that combines semantic knowledge  from semantic spaces
a novel wrapper induction technique that does not require any human supervision to detect the wrappers
any RDF triples entailed by others already
a opensource framework for the extraction of RDF also includes a consistency layer
approach adopted by TableMinerOver the last decades several billion Web pages have been made available on the Web of Data
a recommendation approach that tries to understand what factors interest a user use this factor information to predict future item ratings
a recommendation approach that tries to understand what factors interest then use this factor information to predict future item ratings
a rewriter is a sequence of fixedpoint iterators for algebraic rules
As an integral part of the EU GRANATUM project a Linked Biomedical Dataspace augment the design of in silico experiments for cancer chemoprevention drug discovery
As an integral part of the EU GRANATUM project a Linked Biomedical Dataspace was developed to semantically interlink data from multiple sources
As a result the Linked Open Data cloud now contains a high level of dirty data
a semantic framework that enables the semantics of SPARQL Update to be used as the basis for a  cutandpaste  provenance model in a principled manner
A set of properties is considered as a key
a SPARQLoverSQLonHadoop approach designed with selective queries in mind
a stateoftheart OntologyBased Data Access system which demonstrates favorable execution times
a stateoftheart OntologyBased Data Access system which validates the cost model
As the dataset series covers four years it can also be used to analyze the evolution of the adoption of the markup formatsThe recent big data movement resulted in a surge of activity on layering declarative languages on top of distributed computation platforms
As The Resource Description Framework data continue to be integrated at Webscale such as in the Linked Open Data cloud The Resource Description Framework data management systems are being exposed to queries
As The Resource Description Framework data continue to be integrated at Webscale such as in the Linked Open Data cloud The Resource Description Framework data management systems are being exposed to workloads
As The Resource Description Framework data continue to be published across heterogeneous domains management systems are being exposed to queries
As The Resource Description Framework data continue to be published across heterogeneous domains management systems are being exposed to workloads
a suitable Semantic Web driven technology to address the challenge
a tool that can generate an initial rule base from an ontology
At the same time a number of challenges arise as well related to limited computing resources
At the same time a number of challenges arise as well related to mobile platform heterogeneity
At the same time Hadoop has become dominant in the area of Big Data processing with large infrastructures
a user based on Matrix Factorisation past ratings for products movies songs 
a way that is fully independent of the actual schema
Based on this combination AGDISTIS can efficiently detect the correct URIs for a given set of named entities within an input text
bootstrapping learning approach seeded by automatically
By performing a detailed analysis we pinpoint these problems to specific types of queriesIn this paper we present Semano 
By performing a detailed analysis we pinpoint these problems to specific types of workloads
By the efforts of thousands of volunteers the project has produced a large open knowledge base with many interesting applications
columns containing named entity
community detection which allows a node to participate in more than one cluster or community
Compared with category systems of BabelNet Zhishischema contains the largest number of subsumptions between categories
Compared with category systems of BabelNet Zhishischema has wide coverage of categories
Compared with category systems of DBpedia Zhishischema contains the largest number of subsumptions between categories
Compared with category systems of DBpedia Zhishischema has wide coverage of categories
Compared with category systems of Freebase Zhishischema contains the largest number of subsumptions between categories
Compared with category systems of Freebase Zhishischema has wide coverage of categories
Compared with category systems of Yago Zhishischema contains the largest number of subsumptions between categories
Compared with category systems of Yago Zhishischema has wide coverage of categories
computing entailed answers by ontologies
concept disjointness relationships hidden in the datasets
Consequently when analysing large RDF datasets users are left with an existing nonRDFspecific own limitations
Consequently when analysing large RDF datasets users are left with two main options using SPARQL or using an existing nonRDFspecific big data language
data consumers performing federated queries
data enriched with hierarchical information
datainterface on top of mobile reasoning engines should be provided as well
data publishing guidelines
datasets of complexity flows
datasets of existing reasoning process flows
datasets of varying scale flows
datasets that are clean at creation
datasets where erroneous data or duplicates exist
data that can actually be retrieved by a Linked Data crawler
Deploying Semantic Web reasoning processes directly on mobile devices has a number
Deploying Semantic Web reasoning processes directly on mobile devices has robustness to connectivity loss more timely results
Deploying Semantic Web reasoning processes directly on mobile devices reduced infrastructure requirements
dirty data that is difficult for humans for machines to processThis paper describes bootstrapping learning approach selected  partial  data from a table
dirty data that is difficult for humans to clean
DLLite which is less expressive than ELHIO
Doing so allows us to confirm the assessment of a wrong value
Doing so allows us to reject the assessment of a wrong value
domain experts that are not familiar with the technical details of the framework to set up a domainspecific annotator
edge based community detection
empirically demonstrate the superior performance that our achieve over existing SemanticSVD with no transferred semantic categories
empirically demonstrate the superior performance that our achieve over existing SVD models
empirically demonstrate the superior performance that our achieve over existing SVD modelsThe ability to integrate a wealth of humancurated knowledge from scientific datasets and ontologies can benefit drugtarget interaction prediction
Evaluation on two datasets shows that compared to two baselines TableMiner consistently obtains the best performance
evaluations using earlier benchmarks
Even the W3C SPARQL 11 Update explicitly exclude a standard behaviour for entailment regimes other than simple entailment in the context of updates
existing supervised machine learning approaches on this task
existing systems that went unnoticed in evaluations
Experimental results show the high quality of Zhishischema
Experimental results suggest the scalability of We approach to Linked Dataa user based on Matrix Factorisation past ratings for items 
Experiments on the DBpedia datasets demonstrate the feasibility of our approach
Experiments on the NELL datasets demonstrate the feasibility of our approachdata instances stored in relational databases
extraction methods based on adhoc solutions
Finally We describe We preliminary installation and evaluation of the platform in Siemensweb pages that is processed by the major search engines to improve search performance
For Hadoopbased applications a common data pool provides many synergy benefits
formal semantics for schemaorg provides a complete basis for a plausible version of what schemaorg should be
For some queries there can be as much as five orders of magnitude difference between the query execution time of the fastest while the fastest system on one query may unexpectedly time out on another query
For some queries there can be as much as five orders of magnitude difference between the query execution time of the slowest system while the fastest system on one query may unexpectedly time out on another query
For the second step we exploit owl sameAs links for the instances to get additional property values
For the second step we exploit owl sameAs links for the instances to perform a second outlier detection on additional property values
Furthermore We approach can discover keys in datasets  ie almost keys 
Furthermore we describe an implementation
Furthermore we propose instanceconcept compatability metrics based on instanceconcept assertions
Furthermore we propose instanceconcept compatability metrics based on instanceinstance relationships
further optimizing the runtime of link specifications by presenting Helios
Given a mapping from the source database to the target ontology OntologyBased Data Access concerns
Given a source relational database OntologyBased Data Access concerns
Googles rule engines on smartphones to provide developers with information critical for deploying semantic web tools on powerconstrained devices
Googles validate Googles methodology by applying our methodology to rule engines answering queries on two ontologies with expressivities in OWL DL
Googles validate Googles methodology by applying our methodology to rule engines answering queries on two ontologies with expressivities in RDFS DL
Googles validate Googles methodology by applying our methodology to three wellknown reasoners answering queries on two ontologies with expressivities in OWL DL
Googles validate Googles methodology by applying our methodology to three wellknown reasoners answering queries on two ontologies with expressivities in RDFS DL
Helios comprises both an execution planner for link specifications
Helios comprises both a rewriter for link specifications
However computing is usually treated orthogonally to updates in triple stores
However existing Linked Data approaches provide a single global view over existing Linked Data with no way of varying the notion of equivalence to be appliedThe central idea of Linked Data is that data publishers support applications in integrating data by complying to a set of best practices in vocabulary usage
However Keys are usually not available
However no comparison of the performance of existing supervised machine has been presented so far
however RDF databases are capable of answering much more expressive queries directly
However Semantic datasets performance is always decreased by the noises in the datasets among which noisy type assertions play an important role
However the further optimization of such link specifications has not been paid much attention to
However when updates occur in an uncontrolled way consistency issues arise
ie linked data
If a data consumer finds an error how can a data consumer fix a data consumer
If they have chemical structure
If they have some other criteria
If they have the same drug name
Implementing the vision of the Semantic Web thus requires transforming this unstructured data into structured data
In 2011 the State of the LOD Cloud report analyzed the adoption of these best practices by linked datasets within different topical domains
In addition to an initial rule base Java API a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies includes two GUI components an annotation viewer
In addition to an initial rule base Java API a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies includes two GUI components a rule base editor
In addition to an initial rule base Java API Semano includes two GUI components an annotation viewer
In addition to an initial rule base Java API Semano includes two GUI components a rule base editor
In a first step we apply outlier detection methods to the property values using a novel approach for splitting the data into relevant subsets
In a multilingual setting such knowledge is needed for each of the supported languages
In combination with the default japelates two GUI components an annotation viewer can be used by domain experts
In combination with the default japelates two GUI components a rule base editor can be used by domain experts
In combination with the rule generator two GUI components an annotation viewer can be used by domain experts
In combination with the rule generator two GUI components a rule base editor can be used by domain experts
Indeed existing SPARQLon Hadoop approaches have already demonstrated very good scalability however query runtimes are rather slow due to the underlying batch processing framework
information that was provided by the dataset publishers the dataset publishers themselves via the datahubio Linked Data catalog via the datahubio Linked Data catalog
initial experimental results demonstrating that by applying the the ontologybased data access platform Ontop technologies the treewitness query Tmapping optimisations  the system produces scalable SQL queries
initial experimental results demonstrating that by applying the the ontologybased data access platform Ontop technologies the treewitness query Tmappings  the system produces scalable SQL queries
In making Linked Data analysis effectively accessible to the general public We tool has been integrated in a number of live services where people use our tool to analyse discover facts with Linked DataIn this paper we study rewriting in ontologybased data access
In making Linked Data analysis effectively accessible to the general public We tool has been integrated in a number of live services where people use our tool to analyse discuss facts with Linked Data
In order to support web applications to understand the content of HTML pages an increasing number of websites have started to annotate structured data within an increasing number of websites pages
In our abstraction events are encoded as spatially and temporally oriented named graphs while linked pedigrees as RDF datasets are linked pedigrees as RDF datasets specific compositions
In particular We investigate the contribution of each single dependency patternThe increase in the volume and heterogeneity of biomedical data sources has motivated researchers to embrace Linked Data technologies to solve the ensuing integration challenges
In particular We materialised RDF stores redundancyfree RDF stores
In particular We perform an analysis of the impact of different parameters
In particular we present a novel wrapper induction technique
In particular We reduced RDF Stores that is redundancyfree RDF stores
In particular We treat both redundancyfree RDF stores
In previous works the combination of the results of timeefficient algorithms through settheoretical operators has been shown to be very timeefficient for Link Discovery
In the classification task TableMiner achieves significant improvements of 038 F1 depending on different baseline methods in the classification task TableMiner outperforms both baselines by between 019 in Precision on one dataset and between 002 F1 on the other dataset
In the classification task TableMiner achieves significant improvements of 038 F1 depending on different baseline methods in the classification task TableMiner outperforms both baselines by between 019 in Precision on one dataset and between 003 F1 on the other dataset
In the classification task TableMiner achieves significant improvements of 038 F1 depending on different baseline methods in the classification task TableMiner outperforms both baselines by between 037 in Precision on one dataset and between 002 F1 on the other dataset
In the classification task TableMiner achieves significant improvements of 038 F1 depending on different baseline methods in the classification task TableMiner outperforms both baselines by between 037 in Precision on one dataset and between 003 F1 on the other dataset
In the classification task TableMiner achieves significant improvements of between 008 F1 depending on different baseline methods in the classification task TableMiner outperforms both baselines by between 019 in Precision on one dataset and between 002 F1 on the other dataset
In the classification task TableMiner achieves significant improvements of between 008 F1 depending on different baseline methods in the classification task TableMiner outperforms both baselines by between 019 in Precision on one dataset and between 003 F1 on the other dataset
In the classification task TableMiner achieves significant improvements of between 008 F1 depending on different baseline methods in the classification task TableMiner outperforms both baselines by between 037 in Precision on one dataset and between 002 F1 on the other dataset
In the classification task TableMiner achieves significant improvements of between 008 F1 depending on different baseline methods in the classification task TableMiner outperforms both baselines by between 037 in Precision on one dataset and between 003 F1 on the other dataset
In the Semantic Web realm this surge of analytics languages was not reflected despite the significant growth in the available Semantic Web data
In this paper we define these fragments as SPARQL CONSTRUCT federated queries
In this paper we define these fragments as SPARQL CONSTRUCT propose a correction criterion to maintain these fragments incrementally without reevaluating the query
In this paper we devise an extension of the federation of Linked Data to data consumers
in this paper We discuss the tradeoff between the reduction of the size of the rewriting and the computational cost of We approachwebsites pages using markup formats such as Microdata RDFa Microformats
In this paper we explore how the provenance information can be defined by reinterpreting SPARQL updates
In this paper we explore how to adapt the dynamic copypaste provenance model of Buneman et al to RDF datasets
In this paper we explore how to represent the resulting provenance records SPARQL updates as RDF in a manner compatible with W3C PROV
In this paper we introduce a dataflow language
In this paper we introduce CAMO a new system developed jointly with Samsung to enrich multimedia metadata by integrating Linked Open Data 
In this paper we introduce CAMO a new system developed jointly with Samsung to enrich multimedia metadata by integrating the LOD 
In this paper we introduce SYRql
In this paper we introduce the first effort to publish Chinese linked open schema
In this paper we introduce the rule base model of Semano a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies provide examples of adapting an initial rule base to meet particular application requirements and report we experience with applying a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies within the domain of nano technology
In this paper we introduce the rule base model of Semano a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies provide examples of adapting an initial rule base to meet particular application requirements and report we experience with applying Semano Linking Open Data is the largest community effort for semantic data publishing
In this paper we introduce Zhishischema 
In this paper we present a crossplatform benchmark framework
In this paper we present a framework for automatically inducing ontology lexica in multiple languages on the basis of a multilingual corpus
In this paper we present a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies
In this paper we present a holistic framework for the extraction of RDF from templated websites
In this paper we present a method to overcome this limitation by transferring incorporate this into our prior SemanticSVD model
In this paper we present a method to overcome this limitation by transferring rated semantic categories in place of unrated categories through the use of vertex kernels
In this paper we present a novel method the edges of a heterogeneous graph
In this paper we present a opensource framework for the extraction of RDF from templated websites
In this paper we present a series of publicly accessible Microdata RDFa Microformats datasets that we have extracted from three large web corpora
In this paper we present a SPARQLoverSQLonHadoop approach
In this paper we present drugdrug similarity edges
In this paper we present MATOLL 
In this paper we present Sempala
In this paper we present targettarget similarity edges
In this paper we revisit the findings of the 2011 State of the LOD Cloud report based on a crawl of the Web of Linked Data
In this paper We showcase three different workflows
In this paper we show how event processing over semantically annotated streams of events can be exploited for implementing tracing of products in supply chains through the automated generation of linked pedigrees
In this paper we study query answering
In this paper we study the problem of noisy type assertion detection in semantic web datasets by making use of concept disjointness relationships
In this paper we take a first step to close this gap
In this paper we update the findings of the 2011 State of the LOD Cloud report based on a crawl of the Web of Linked Data
In this work we instead propose an approach
JAPE transducers that has been introduced within the General Architecture for Text Engineering to enable modular development of annotation rule bases
Keys can be very useful to discover these identity links
language dealing with updates both of ABox and of TBox statements
languagespecific dependency patterns which are formalized as SPARQL queries
languagespecific dependency patterns which are run over a parsed corpus
large infrastructures being already deployed in manifold application fields
large infrastructures being already used in manifold application fields
Largescale DBpedia are integrated using instance linkage techniques
Largescale DBpedia are integrated using ontology matching
Largescale heterogeneous LOD sources are integrated using instance linkage techniques
Largescale heterogeneous LOD sources are integrated using ontology matching
Largescale LinkMDB are integrated using instance linkage techniques
Largescale LinkMDB are integrated using ontology matching
Largescale MusicBrainz are integrated using instance linkage techniques
Largescale MusicBrainz are integrated using ontology matching
Linked Data conducted in April 2014
Linked Data Fragments that enable lowcost publication of queryable data by moving intelligence from the server to the client
Linked Data is especially suited for the application of such an idea since Linked Data also contains explicit links between instances
Linked Data is especially suited for the application of such an idea since Linked Data provides large amounts of data
Linked Open Data faces severe issues of data quality
Linked Open Data faces severe issues of scalability quality
Links between knowledge bases build the backbone of the Linked Data Web
mainly positive results confirming that the Query Wizard simplifies searching refining
mainly positive results confirming that the Query Wizard simplifies searching transforming  in particular that people learn to perform interactive analysis tasks on the resulting Linked Data
mainly positive results confirming that the Query Wizard simplifies searching transforming Linked Data
many synergy benefits making Hadoopbased very attractive to use these infrastructures for semantic data processing as well
Many tasks require knowledge about how the elements of ie classes  are expressed in natural language
Many tasks require knowledge about how the elements of ie individuals  are expressed in natural language
Many tasks require knowledge about how the elements of ie properties  are expressed in natural language
Many tasks require knowledge about how the elements of the vocabulary  are expressed in natural language
Matrix Factorisation is a recommendation approach
Moreover datasets can get stains over time
Moreover Helios improvements are statistically significantMetadata is a vital factor for effective management organization and retrieval of multimedia content
Moreover Our results indicate the weaknesses of we current implementationsExploiting identity links among RDF resources allows applications to efficiently integrate data
Moreover Our results indicate the weaknesses of we how our current implementations can be extended
Moreover we introduce several partial exports
Moreover we introduce simplified views on The data
natural outliers which are exceptional values in the dataset
natural outliers which are nevertheless correct
new RDF exports that connect Wikidata to the Linked Data Web
Observation also shows that the bootstrapping can potentially deliver computational savings of 60
Observation also shows that the bootstrapping can potentially deliver computational savings of between 24
One key step during this process is the recognition of named entities
One of the key steps towards extracting RDF from text is the disambiguation of named entities
One solution to this is to include users preferences for ie  categories however this approach is limited should a user be presented with an item so called coldstart categories
One solution to this is to include users preferences for semantic  categories however this approach is limited should a user be presented with an item so called coldstart categories
On the other hand existing big data languages are designed mainly for tabular data and therefore applying languages to Semantic Web data results in verbose sometimes inefficient scripts
On the other hand existing big data languages are designed mainly for tabular data and therefore applying languages to Semantic Web data results in verbose unreadable scripts
On the theoretical side we present initial experimental results
On the theoretical side we show how any input SPARQL query OWL 2 QL ontology can be rewritten to an equivalent SQL query solely over the data
On the theoretical side we show how R2RML mappings can be rewritten to an equivalent SQL query solely over the data
ontologies expressed in the description logic ELHIO
OntologyBased Data Access concerns answering queries over the target ontology
other data over 125 million postal addresses originating from thousands of websites
other data over 211 million product descriptions originating from thousands of websites
other data over 54 million reviews originating from thousands of websites
our approach has been evaluated on different synthetic
our approach has been evaluated on real datasets
our evaluated several vertex kernels
our evaluation results show that for fast moving supply chains smaller window sizes on event streams enable early counterfeit detectionAs the Web of Data is growing at an ever increasing speed the lack of reliable query solutions for live public data becomes apparent
our evaluation results show that for fast moving supply chains smaller window sizes on event streams provide significantly higher efficiency in the generation of pedigrees
Our evaluation shows performance improvements by an order of magnitude compared to existing approaches paving the way for interactivetime SPARQL query processing on HadoopWhile the Semantic Web currently can exhibit provenance information by using the W3C PROV standards there is a  missing link  in connecting PROV to querying for dynamic changes to RDF graphs
our exemplify our approach using the pharmaceuticals supply chain
our propose an algorithm
Our semantics based edge partitioning approach semEP
Our semantics has the advantages of edge
our several vertex kernels effects on recommendation error
our show how counterfeit detection is an implicit part of our pedigree generation
Our use semantic knowledge to specify edge constraints that should not participate in the same cluster
Our use semantic knowledge to specify specific drugtarget interaction edges that should not participate in the same cluster
Outlier detection is typically applied to single datasets to search data for values of unexpected behavior
OWL 2 provides a dedicated fragment OWL QL for this purpose
people using the Query Wizard quickly
Previous works suggest that ensemble learning can be used to improve the performance of named entity recognition tools
problems arising from natural outliers
queries that are far more diverse
RDF datasets that change over time in response to SPARQL updates
RDF graphs using SPARQL
RDF stores which store all
redundancyfree RDF stores that do not store any RDF triples
research data extracted from scientific publications
rule templates called japelates
rule templates called The core of the Semano rule base model instantiations
Sailors Linked Data Cloud Sailors Linked Data Cloud is a potential hub dataset for digital history research
Sailors Linked Data Cloud Sailors Linked Data Cloud is a prime example of the benefits of Linked Data for this field
scenarios involving data from the Linked Open Data Cloud
Schemaorg is a way to add machineunderstandable information to web pages
Semano generalizes the mechanism of JAPE transducers
semantic data publishing which converts the Web from a Web of document to a Web of interlinked knowledge
Semantic datasets provide support to automate many tasks such as question answering
Semantic Web technologies are used in enabling expressive standardsbased reasoning
separate datamodels designed in close collaboration with maritime historical researchers
several generic japelates that can serve as a starting point
several partial exports that provide more selective
severe issues of availability are observed by data consumers SPARQL endpoints do not respond can be outofdate
severe issues of availability are observed by data consumers SPARQL endpoints do not respond can be wrong
severe issues of availability are observed by data consumers SPARQL endpoints results can be outofdate
severe issues of availability are observed by data consumers SPARQL endpoints results can be wrong
severe issues of data are observed by data consumers SPARQL endpoints do not respond can be outofdate
severe issues of data are observed by data consumers SPARQL endpoints do not respond can be wrong
severe issues of data are observed by data consumers SPARQL endpoints results can be outofdate
severe issues of data are observed by data consumers SPARQL endpoints results can be wrong
severe issues of scalability are observed by data consumers SPARQL endpoints do not respond can be outofdate
severe issues of scalability are observed by data consumers SPARQL endpoints do not respond can be wrong
severe issues of scalability are observed by data consumers SPARQL endpoints results can be outofdate
severe issues of scalability are observed by data consumers SPARQL endpoints results can be wrong
similar targets interact with the same drugs
smartphones running Googles Android operating system
So far EBoxes have only been used in query rewriting in the case of DLLite
Solving this problem would be required for such clear usecases as the creation of version control systems for RDF
SPARQL 11 Entailment Regimes specifications explicitly exclude a standard behaviour for entailment regimes other than simple entailment in the context of updates
sparql implementations have deliver impressive performance for public sparql endpoints yet poor availability especially under high loads prevents sparql implementations use in realworld applications
sparql implementations have matured yet poor availability especially under high loads prevents sparql implementations use in realworld applications
SPARQL is the query language for The Resource Description Framework
SPARQL queries that are typically much more selective requiring only small subsets of the data
Specifically our experiments with five popular The Resource Description Framework data management systems show that management systems can not deliver good performance uniformly across workloads
Specifically we present an algorithm for computing a perfect rewriting of unions of conjunctive queries posed over ontologies 2 EL profiles
stateoftheart machine learning based prediction methods
SYRql blends concepts from both SPARQL languages
SYRql blends concepts from existing big data languages
TableMiner is able to use various types of contextual information outside tables for Table Interpretation including eg RDFamicrodata annotations  that to the best of our knowledge have never been used in Natural Language Processing tasks
TableMiner is able to use various types of contextual information outside tables for Table Interpretation including semantic markups  that to the best of our knowledge have never been used in Natural Language Processing tasks
TableMiner labels columns mentions with semantic concepts that best describe data in these columns
TableMiner labels columns mentions with semantic concepts that best disambiguates entity content cells in these columns
The annotations are used by Bing to display entity descriptions within Facebook applications
The annotations are used by Bing to enrich search results
The annotations are used by Facebook to display entity descriptions within Facebook applications
The annotations are used by Facebook to enrich search results
The annotations are used by Google to display entity descriptions within Facebook applications
The annotations are used by Google to enrich search results
The annotations are used by Yahoo to display entity descriptions within Facebook applications
The annotations are used by Yahoo to enrich search results
The annotations are used by Yandex to display entity descriptions within Facebook applications
The annotations are used by Yandex to enrich search results
The approaches can easily be overwhelmed by the size of the data
The approaches can easily require clean data
The approaches that attempt to automatically discover keys
The availability of Facebook lays the foundation for further research for exploring the data utility within different application contexts
The availability of Facebook lays the foundation for further research on cleansing the data
The availability of Facebook lays the foundation for further research on integrating the data
the bootstrapping learning approach
The central idea of Linked Data is that data publishers support applications in discovering
The central idea of Linked Data is that data publishers support applications in integrating data by complying to a set of best practices in metadata provision
The central idea of Linked Data is that data publishers support applications in integrating data by complying to a set of best practices in the areas of linking
The choice depends upon the application to which existing Linked Data will be put
the collaborative processes which would make it easier for Linked Data practitioners to create such dataspaces in other domains
the collected categories and tags which results in a large semantic network
the collected categories and tags which results in an integrated concept taxonomy
The core of the Semano rule base model are rule templates
the corresponding SPARQL update language
the data extracted by the wrappers
The data is also complex in RDF
The data is also not available in RDF
The data is also very rich in RDF
The data is highly interlinked and connected to many other datasets
the data that are dynamically detected during the process
The definition of schemaorg is incomplete in a number of places
The definition of schemaorg is provided as a set of web pages plus a partial mapping into RDF triples with unusual properties
the description logic ELHIO which covers the OWL 2 QL and OWL
The different components of the a Linked Biomedical Dataspace facilitate both the bioinformaticians to publish link query
The different components of the a Linked Biomedical Dataspace facilitate both the biomedical researchers to publish link query
The different components of the a Linked Biomedical Dataspace visually explore the heterogeneous datasets
the document characteristics used within japelates
the Dutch Ships is a potential hub dataset for digital history research
the Dutch Ships is a prime example of the benefits of Linked Data for this fieldWhen are two entries about a small molecule in different datasets the same
The first contribution of our work is an indepth experimental analysis
the first OntologyBased Data Access system supporting ontologies with transitivity by using SQL recursion
the first semantic Table Interpretation method that adopts an incremental mutually recursive
the high cost of evaluation can be limiting in some scenarios
The hypothesis is that similar drugs interact with the same targets
The increase in the volume and heterogeneity of biomedical data sources enhance information discovery
The individual datasets use separate datamodels
The individual models are mapped to a common interoperability layer allowing for analysis of the data on the general level
the LOD Cloud report was based on information
The majority of Linked Open Data does not meet even a core set of data
The most recent of Facebook contains amongst other data over 125 million postal addresses
The most recent of Facebook contains amongst other data over 211 million product descriptions
The most recent of Facebook contains amongst other data over 54 million reviews
The multiclass classification is solved by Adaboost with C4 5 as the base classifier
the novel best predicted interactions of semEP against the STITCH interaction resourceMany tasks in which a system needs to mediate between natural language expressions and elements of a vocabulary in an ontology or dataset
The novelty of our algorithm is the use of a set of ABox dependencies
The ongoing transition from the current Web of unstructured data to the Web of Data yet requires accurate approaches for the extraction of structured data in RDF  from these websites
The ongoing transition from the current Web of unstructured data to the Web of Data yet requires accurate approaches for the extraction of structured data in Resource Description Framework  from these websites
The ongoing transition from the current Web of unstructured data to the Web of Data yet requires scalable approaches for the extraction of structured data in RDF  from these websites
The ongoing transition from the current Web of unstructured data to the Web of Data yet requires scalable approaches for the extraction of structured data in Resource Description Framework  from these websites
the ontologybased data access platform Ontop that supports answering SPARQL queries under the OWL 2 QL direct semantics entailment regime for data instances
The Our use semantic knowledge between drugs reflect a chemical semantic space while Our use semantic knowledge between targets reflect a genomic semantic space
The performed usability evaluations unveil mainly positive results sets
The planner relies on timeefficient evaluation functions to generate execution plans for link specifications
The primary contribution of this paper we explore how to adapt the dynamic copypaste provenance model of Buneman is a semantic frameworkOnly a small fraction of the information on the Web is represented as Linked Data
the property values extracted from a single repository
The pure declarative nature of SPARQL can be limiting in some scenarios
The results indicate that at the cost of lower performance query techniques with triple pattern fragments lead to high availability thereby allowing for reliable applications on top of public queryable Linked DataWe present a description and analysis of the data access challenge in the Siemens Energy
The results show both the relevance of almost keysIn this paper we show how event processing over semantically annotated streams of events can be exploited for implementing tracking of products in supply chains through the automated generation of linked pedigrees
The results show both the relevance of the efficiency of discovering almost keys
The semEP problem is to create a minimal partitioning of the edges such that the cluster density of each subset of edges is maximal
the target ontology using these three components
the treewitness query rewritingSemantic datasets provide support to automate many tasks such as decisionmaking answering
the views specified by the mappings
the Waterloo SPARQL Diversity Test Suite that provides stress testing tools for The Resource Description Framework data management systems
the Webbased frontend consisting of querying tools
the Webbased frontend consisting of visualisation tools
This allows us to query RDF data under OWL QL entailment without extracting OWL axioms
This allows us to query RDF data under OWL QL entailment without preprocessing OWL axiomsIt is widely accepted that proper data publishing is difficult
This analysis of provides a complete basis for a plausible version of what schemaorg should beGiven a target OWL ontology OntologyBased Data Access concerns
This heterogeneous dataset brings together four curated datasets on Dutch Maritime history as fivestar linked data
This includes a class hierarchy that we extract from the site
This includes several other types of ontological axioms that we extract from the site
This lack of coverage is partly due to the paradigms followed so far to extract Linked Data
This paper describes TableMiner
This paper describes the first semantic Table Interpretation method
This paper formalizes the Linked Data Fragments concept
This paper introduces a clientside sparql query processing algorithm
This paper presents the development of an OntologyBased Data Access system that is a hybridization of materialization
This paper presents the development of an OntologyBased Data Access system that is a hybridization of query rewriting
This paper presents the development of UltrawrapOBDA that is a hybridization of materialization
This paper presents the development of UltrawrapOBDA that is a hybridization of query rewriting
this paper raises the issue of the writability of Linked Data
This paper verifies servers availability under load
This problem has been mainly studied in the domain of data mining
This problem has been mainly studied in the semantic web community
three different workflows depicting realworld scenarios on the use of a Linked Biomedical Dataspace by the domain users to intuitively retrieve meaningful information from the integrated sources
three large web corpora dating from 2010
three large web corpora dating from 2012
three large web corpora dating from 2013
Tmapping optimisations using database integrity constraints
Tmapping optimisations using SQL expressivity
Tmappings compiling R2RML mappings with ontology hierarchies
To address these shortcomings our second contribution is the Waterloo SPARQL Diversity Test Suite
To address this issue we introduce new RDF exports
To close the gap between the expressive ontologies we contribute to the complementary part of the Open Data that is Linking Open Schema
To close the gap between the Open Data we contribute to the complementary part of the Open Data that is Linking Open Schema
To deal with the current heterogeneity of a uniform rule
To deal with the current heterogeneity of rule formats
To prune the search space SAKey exploits characteristics of the data
To tackle these challenges it should be possible to benchmark mobile reasoning performance across different mobile platforms with rule
To the best of our knowledge this is the first OntologyBased Data Access system
To this end We combine four different stateofthe approaches by using 15 different algorithms for ensemble learning
To this end We evaluate ensemble learning performace on five different datasets
triples describing millions of entities
type assertions which type an individual to two potential disjoint concepts
Typical implementations use the OWL QL schema to rewrite a conjunctive query into an equivalent set of queries to be answered against the nonschema part of the data
Unfortunately this wealth of data remains inaccessible to those without indepth knowledge of semantic technologies
Update improves general data quality
Update improves general data replicated data creates opportunities for federated query engines to improve availability
Update sharing
Using additional SPARQL 11 features we develop a new method of schemaagnostic query rewriting where arbitrary conjunctive queries over OWL QL are rewritten into equivalent SPARQL 11 queries in a way
Using a wellknown dataset of drugtarget interactions we demonstrate the benefits of using semEP predictions to improve the performance of a range of stateoftheart machine learning
Using the Waterloo SPARQL Diversity Test Suite our have been able to reveal issues with existing systems
Validation of the novel best reflect both accurate and diverse predictions
We accumulated experience during the collaborative processes
We address the issue of further a runtime optimizer for Link Discovery
We address this drawback by presenting AGDISTIS 
We address this drawback by presenting a novel knowledgebaseagnostic approach for named entity disambiguation
We address this research gap by presenting a thorough evaluation of named entity recognition based on ensemble learning
We advocate for Ontology Based Data Access as a suitable Semantic Web
We also provide a concise set of generic recommendations to develop Linked Data platforms useful for drug discoveryA considerable portion of the information on the Semantic Web is still only available in unstructured form
We analyze how the adoption of the different best practices has changed and present an overview of the linkage relationships between datasets in the form of an updated LOD cloud diagram this time not based on information from dataset providers but on data
We analyze the theoretical complexity of the protocol in space
We analyze the theoretical complexity of the protocol in time
We analyze the theoretical complexity of the protocol in traffic
We approach combines the HypertextInduced Topic Search  HITS  algorithm with label expansion strategies
We approach combines the HypertextInduced Topic Search  HITS  algorithm with string similarity measures
We approach effectively detect noisy type assertions in DBpedia with a high precision of 95Semantic Web technologies are used in a variety of domains for Semantic Web technologies ability to facilitate data integration
we ask how this can be exploited in query rewriting
We collect dynamic tags from more than 50 various most popular social Web sites in China
We collect navigational categories from more than 50 various most popular social Web sites in China
We contributions include a cost model to determine which views to materialize to attain the fastest execution time
We contributions include an efficient algorithm to compile ontological entailments as mappingsWe present the Dutch Ships
We contributions include an empirical evaluation
We contributions include a proof that every SPARQL query can be rewritten into a SQL query in the context of mappings
We define a coordination free protocol based on the counting of provenance
We define a coordination free protocol based on the counting of triples derivations
We define a fragment of SPARQL basic graph patterns corresponding to DLLite
We demonstrate Linked Data applicability in scenarios
We derive requirements for applying Data Access in Siemens review existing Data Access systems
We describe a toolchain enabling users without semantic technology background to explore
We discuss Data Access limitations with respect to the Siemens requirements
We discuss possible semantics along with potential strategies for implementing potential strategies
we discuss the architecture of a holistic framework for the extraction of RDF
we discuss the architecture of a opensource framework for the extraction of RDF
we discuss the architecture of the initial implementation of each of a holistic framework for the extraction of RDF components
we discuss the architecture of the initial implementation of each of a opensource framework for the extraction of RDF components
We discuss the implications of We findings for balancing tradeoffs of local computation versus communication costs for semantic technologies on mobile platforms
We discuss the implications of We findings for other powerconstrained environments
We discuss the implications of We findings for sensor networks
We discuss the implications of We findings for the Internet of ThingsLinked Data has grown to become one of the largest available knowledge bases
We discuss Wikidata
We evaluate Helios on 17 specifications generated automatically
We evaluate the approach on both synthetic datasets and DBpedia
we evaluate the initial version of REX on three different datasets
We evaluate the system for two languages English in terms of Fmeasure for English by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages English in terms of Fmeasure for German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages English in terms of precision for English by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages English in terms of precision for German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages English in terms of recall for English by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages English in terms of recall for German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages German in terms of Fmeasure for English by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages German in terms of Fmeasure for German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages German in terms of precision for English by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages German in terms of precision for German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages German in terms of recall for English by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate the system for two languages German in terms of recall for German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia
We evaluate We approach on eight different datasets against stateoftheart named entity disambiguation frameworks
We evaluation shows that Helios is up to 300 times faster than a canonical planner
We explain the data model of Wikidata
We find that SPARQL 11 is powerful enough to  implement  a fullfledged OWL QL reasoner in a single query
We focus is on the Webbased frontend
We formally define a closed algebra and some unique optimisation opportunities this algebra provides properties and some unique optimisation opportunities this algebra provides
We have extensively evaluated the usability of the entire platform
We have extensively evaluated We new query rewriting technique
We have implemented all semantics prototypically on top of an offtheshelf triple store
We have instantiated the system for two languages English
We have instantiated the system for two languages German
We have present some indications on practical feasibilityWikidata is the central data management platform of Wikipedia
We observe that by compiling the ontological entailments as mappings is able to reduce the execution time of a SPARQL query by rewriting the query in terms of the views
We observe that by implementing the mappings as SQL views is able to reduce the execution time of a SPARQL query by rewriting the query in terms of the views
We observe that by materializing a subset of the views the underlying SQL optimizer is able to reduce the execution time of a SPARQL query by rewriting the query in terms of the views
We present an approach
We present an extension of the ontologybased data access platform Ontop
We present internal links to external data sources
We present links to external data sources
We present modeling decisions to external data sources
We present Sailors Linked Data Cloud
We present SAKey
We present the datasets to external data sources
We propose to tackle this availability problem by defining a specific kind of Linked Data Fragments
We propose to tackle this availability problem by defining triple pattern fragments
We report the important lessons that We learned through the challenges encountered
we results clearly show the potential of using templated Web pages to extend the Linked Data Cloud
We results indicate that We outperform the stateoftheart approach by up to 29Outlier detection used for identifying wrong values in data
We results suggest that ensemble learning can reduce the error rate of stateoftheart named entity recognition systems by 40The Resource Description Framework is a standard for conceptually describing data on the Web
We show ways of accessing the data
We show ways of present a number of examples of how This heterogeneous dataset can be used for historical research
We then introduce the Optique platform as a suitable Data Access solution for Siemens
We then propose a twostage method to capture equivalence subsumption
We then relate relationships between the collected categories and tags
We transform noisy type assertion detection into multiclass classification of pairs of type assertions
We visually analyse Linked Data
When substituting Zhishischema for the original category system of Zhishime we not only filter out incorrect category subsumptions but also add more finergrained categoriesUpdates in RDF stores have recently been standardised in the SPARQL 11 Update specification
While a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies does not make assumptions about the document characteristics a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies provides several generic japelates
While a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies does not make assumptions about the document characteristics Semano provides several generic japelates
While a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies is generic a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies provides several generic japelates
While a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies is generic Semano provides several generic japelates
While annotation techniques for querying provenance data originally developed with databases in mind transfer readily to RDF annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with databases in mind transfer readily to RDF annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with databases in mind transfer readily to RDF some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with databases in mind transfer readily to RDF some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with databases in mind transfer readily to SPARQL annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with databases in mind transfer readily to SPARQL annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with databases in mind transfer readily to SPARQL some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with databases in mind transfer readily to SPARQL some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to RDF annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to RDF annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to RDF some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to RDF some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to SPARQL annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to SPARQL annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to SPARQL some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to SPARQL some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with databases in mind transfer readily to RDF annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with databases in mind transfer readily to RDF annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with databases in mind transfer readily to RDF some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with databases in mind transfer readily to RDF some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with databases in mind transfer readily to SPARQL annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with databases in mind transfer readily to SPARQL annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with databases in mind transfer readily to SPARQL some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with databases in mind transfer readily to SPARQL some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to RDF annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to RDF annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to RDF some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to RDF some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to SPARQL annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to SPARQL annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to SPARQL some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to SPARQL some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While converting structured data to RDF is well supported by tools most approaches to extract RDF from semistructured data rely on extraction methods
While Semano does not make assumptions about the document characteristics a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies provides several generic japelates
While Semano does not make assumptions about the document characteristics Semano provides several generic japelates
While Semano is generic a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies provides several generic japelates
While Semano is generic Semano provides several generic japelates
While several approaches aim to tackle this problem several approaches still achieve poor accuracy
While some provenance models for querying provenance data originally developed with databases in mind transfer readily to RDF annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with databases in mind transfer readily to RDF annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with databases in mind transfer readily to RDF some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with databases in mind transfer readily to RDF some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with databases in mind transfer readily to SPARQL annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with databases in mind transfer readily to SPARQL annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with databases in mind transfer readily to SPARQL some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with databases in mind transfer readily to SPARQL some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with workflows in mind transfer readily to RDF annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with workflows in mind transfer readily to RDF annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with workflows in mind transfer readily to RDF some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with workflows in mind transfer readily to RDF some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with workflows in mind transfer readily to SPARQL annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with workflows in mind transfer readily to SPARQL annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with workflows in mind transfer readily to SPARQL some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for querying provenance data originally developed with workflows in mind transfer readily to SPARQL some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with databases in mind transfer readily to RDF annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with databases in mind transfer readily to RDF annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with databases in mind transfer readily to RDF some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with databases in mind transfer readily to RDF some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with databases in mind transfer readily to SPARQL annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with databases in mind transfer readily to SPARQL annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with databases in mind transfer readily to SPARQL some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with databases in mind transfer readily to SPARQL some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with workflows in mind transfer readily to RDF annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with workflows in mind transfer readily to RDF annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with workflows in mind transfer readily to RDF some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with workflows in mind transfer readily to RDF some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with workflows in mind transfer readily to SPARQL annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with workflows in mind transfer readily to SPARQL annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with workflows in mind transfer readily to SPARQL some provenance models for querying provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While some provenance models for storing provenance data originally developed with workflows in mind transfer readily to SPARQL some provenance models for storing provenance data do not readily adapt to describing changes in dynamic RDF datasets over time
While the Semantic Web currently can exhibit provenance information by using the W3C PROV standards there is a  missing link  in connecting PROV to storing for dynamic changes to RDF graphs
While the state of the Open Data contains billion of triples the state of the art Open Data has only a limited number of schema information
While the state of the Open Data contains billion of triples the state of the art Open Data is lack of schemalevel axioms
While this is acceptable for dataintensive queries it is not satisfactory for the majority of SPARQL queries
While this validation was conducted on smartphones Googles methodology is general
While this validation was conducted on smartphones Googles methodology may be applied to different hardware platforms to determine performance relevant to power consumption
While this validation was conducted on smartphones Googles methodology may be applied to entire applications to determine performance relevant to power consumption
While this validation was conducted on smartphones Googles methodology may be applied to ontologies to determine performance relevant to power consumption
While this validation was conducted on smartphones Googles methodology may be applied to reasoners to determine performance relevant to power consumption
Wikidata encoding in RDF
With the adoption of the recent SPARQL 11 standard
workloads that are far more varied
17 specifications created by 2180 specifications
17 specifications created by human experts
