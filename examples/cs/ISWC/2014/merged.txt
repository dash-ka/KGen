links between knowledge bases build the backbone of the linked data web
in previous works the combination of the results of timeefficient algorithms through settheoretical operators has been shown to be very timeefficient for link discovery
however the further optimization of such link specifications has not been paid much attention to
further optimizing the runtime of link specifications by presenting helios
we address the issue of further a runtime optimizer for link discovery
helios comprises both a rewriter for link specifications
helios comprises both an execution planner for link specifications
a rewriter is a sequence of fixedpoint iterators for algebraic rules
the planner relies on timeefficient evaluation functions to generate execution plans for link specifications
17 specifications created by human experts
17 specifications created by 2180 specifications
we evaluate helios on 17 specifications generated automatically
we evaluation shows that helios is up to 300 times faster than a canonical planner
moreover helios improvements are statistically significantmetadata is a vital factor for effective management organization and retrieval of multimedia content
in this paper we introduce camo a new system developed jointly with samsung to enrich multimedia metadata by integrating linked open data 
in this paper we introduce camo a new system developed jointly with samsung to enrich multimedia metadata by integrating the lod 
largescale heterogeneous lod sources are integrated using ontology matching
largescale musicbrainz are integrated using ontology matching
largescale linkmdb are integrated using instance linkage techniques
largescale heterogeneous lod sources are integrated using instance linkage techniques
largescale linkmdb are integrated using ontology matching
largescale dbpedia are integrated using instance linkage techniques
largescale musicbrainz are integrated using instance linkage techniques
largescale dbpedia are integrated using ontology matching
a mobile app for android devices is built on top of the lod to improve multimedia content browsing
an empirical evaluation is conducted to demonstrate the effectiveness and accuracy of the system in the multimedia domaingoogles introduce a new methodology for benchmarking the performance per watt of semantic web reasoners with information critical for deploying semantic web tools on powerconstrained devices
googles rule engines on smartphones to provide developers with information critical for deploying semantic web tools on powerconstrained devices
googles validate googles methodology by applying our methodology to rule engines answering queries on two ontologies with expressivities in rdfs dl
googles validate googles methodology by applying our methodology to three wellknown reasoners answering queries on two ontologies with expressivities in rdfs dl
googles validate googles methodology by applying our methodology to three wellknown reasoners answering queries on two ontologies with expressivities in owl dl
googles validate googles methodology by applying our methodology to rule engines answering queries on two ontologies with expressivities in owl dl
while this validation was conducted on smartphones googles methodology may be applied to reasoners to determine performance relevant to power consumption
while this validation was conducted on smartphones googles methodology may be applied to ontologies to determine performance relevant to power consumption
while this validation was conducted on smartphones googles methodology may be applied to entire applications to determine performance relevant to power consumption
while this validation was conducted on smartphones googles methodology is general
smartphones running googles android operating system
while this validation was conducted on smartphones googles methodology may be applied to different hardware platforms to determine performance relevant to power consumption
we discuss the implications of we findings for sensor networks
we discuss the implications of we findings for the internet of things
we discuss the implications of we findings for other powerconstrained environments
we discuss the implications of we findings for balancing tradeoffs of local computation versus communication costs for semantic technologies on mobile platformslinked data has grown to become one of the largest available knowledge bases
unfortunately this wealth of data remains inaccessible to those without indepth knowledge of semantic technologies
we describe a toolchain enabling users without semantic technology background to explore
we visually analyse linked data
research data extracted from scientific publications
we demonstrate linked data applicability in scenarios
scenarios involving data from the linked open data cloud
the webbased frontend consisting of visualisation tools
we focus is on the webbased frontend
the webbased frontend consisting of querying tools
mainly positive results confirming that the query wizard simplifies searching transforming  in particular that people learn to perform interactive analysis tasks on the resulting linked data
people using the query wizard quickly
mainly positive results confirming that the query wizard simplifies searching transforming linked data
the performed usability evaluations unveil mainly positive results sets
mainly positive results confirming that the query wizard simplifies searching refining
in making linked data analysis effectively accessible to the general public we tool has been integrated in a number of live services where people use our tool to analyse discuss facts with linked data
in making linked data analysis effectively accessible to the general public we tool has been integrated in a number of live services where people use our tool to analyse discover facts with linked datain this paper we study query answering
in this paper we study rewriting in ontologybased data access
ontologies expressed in the description logic elhio
specifically we present an algorithm for computing a perfect rewriting of unions of conjunctive queries posed over ontologies 2 el profiles
the description logic elhio which covers the owl 2 ql and owl
the novelty of our algorithm is the use of a set of abox dependencies
abox dependencies which are compiled into a socalled ebox to limit the expansion of the rewriting
so far eboxes have only been used in query rewriting in the case of dllite
dllite which is less expressive than elhio
in this paper we discuss the tradeoff between the reduction of the size of the rewriting and the computational cost of we approach
we have extensively evaluated we new query rewriting techniquein order to support web applications to understand the content of html pages an increasing number of websites have started to annotate structured data within an increasing number of websites pages
websites pages using markup formats such as microdata rdfa microformats
the annotations are used by google to enrich search results
the annotations are used by bing to enrich search results
the annotations are used by yandex to enrich search results
the annotations are used by google to display entity descriptions within facebook applications
the annotations are used by bing to display entity descriptions within facebook applications
the annotations are used by yandex to display entity descriptions within facebook applications
the annotations are used by facebook to enrich search results
the annotations are used by facebook to display entity descriptions within facebook applications
the annotations are used by yahoo to enrich search results
the annotations are used by yahoo to display entity descriptions within facebook applications
three large web corpora dating from 2012
three large web corpora dating from 2013
in this paper we present a series of publicly accessible microdata rdfa microformats datasets that we have extracted from three large web corpora
three large web corpora dating from 2010
altogether facebook consist of almost 30 billion rdf quads
other data over 125 million postal addresses originating from thousands of websites
other data over 54 million reviews originating from thousands of websites
the most recent of facebook contains amongst other data over 54 million reviews
the most recent of facebook contains amongst other data over 211 million product descriptions
other data over 211 million product descriptions originating from thousands of websites
the most recent of facebook contains amongst other data over 125 million postal addresses
the availability of facebook lays the foundation for further research for exploring the data utility within different application contexts
the availability of facebook lays the foundation for further research on cleansing the data
the availability of facebook lays the foundation for further research on integrating the data
as the dataset series covers four years it can also be used to analyze the evolution of the adoption of the markup formatsthe recent big data movement resulted in a surge of activity on layering declarative languages on top of distributed computation platforms
in the semantic web realm this surge of analytics languages was not reflected despite the significant growth in the available semantic web data
consequently when analysing large rdf datasets users are left with two main options using sparql or using an existing nonrdfspecific big data language
consequently when analysing large rdf datasets users are left with an existing nonrdfspecific own limitations
the high cost of evaluation can be limiting in some scenarios
the pure declarative nature of sparql can be limiting in some scenarios
on the other hand existing big data languages are designed mainly for tabular data and therefore applying languages to semantic web data results in verbose unreadable scripts
on the other hand existing big data languages are designed mainly for tabular data and therefore applying languages to semantic web data results in verbose sometimes inefficient scripts
in this paper we introduce a dataflow language
in this paper we introduce syrql
a dataflow language designed to process semantic web data at a large scale
syrql blends concepts from existing big data languages
syrql blends concepts from both sparql languages
we formally define a closed algebra and some unique optimisation opportunities this algebra provides properties and some unique optimisation opportunities this algebra provides
a closed algebra that discuss its properties
a closed algebra that underlies syrql
a closed algebra that underlies syrql
a closed algebra that discuss a closed algebra
an implementation that compare the performance to other big data processing languages
an implementation that translates syrql scripts into a series of mapreduce jobs
furthermore we describe an implementationlinked open data faces severe issues of scalability quality
linked open data faces severe issues of data quality
linked open data faces severe issues of availability quality
severe issues of scalability are observed by data consumers sparql endpoints results can be outofdate
severe issues of availability are observed by data consumers sparql endpoints do not respond can be outofdate
severe issues of scalability are observed by data consumers sparql endpoints do not respond can be outofdate
severe issues of data are observed by data consumers sparql endpoints results can be wrong
severe issues of scalability are observed by data consumers sparql endpoints results can be wrong
severe issues of availability are observed by data consumers sparql endpoints results can be outofdate
severe issues of data are observed by data consumers sparql endpoints do not respond can be wrong
severe issues of availability are observed by data consumers sparql endpoints results can be wrong
severe issues of data are observed by data consumers sparql endpoints do not respond can be outofdate
data consumers performing federated queries
severe issues of availability are observed by data consumers sparql endpoints do not respond can be wrong
severe issues of data are observed by data consumers sparql endpoints results can be outofdate
severe issues of scalability are observed by data consumers sparql endpoints do not respond can be wrong
if a data consumer finds an error how can a data consumer fix a data consumer
this paper raises the issue of the writability of linked data
in this paper we devise an extension of the federation of linked data to data consumers
a data consumer can make partial copies of different datasets
a data consumer can make a data consumer available through a sparql endpoint
a data consumer can update a data consumer share updates with data providers
a data consumer can update a data consumer local copy with consumers
a data consumer can update a data consumer share updates with consumers
a data consumer can update a data consumer local copy with data providers
update improves general data replicated data creates opportunities for federated query engines to improve availability
update sharing
update improves general data quality
however when updates occur in an uncontrolled way consistency issues arise
in this paper we define these fragments as sparql construct federated queries
in this paper we define these fragments as sparql construct propose a correction criterion to maintain these fragments incrementally without reevaluating the query
we define a coordination free protocol based on the counting of triples derivations
we define a coordination free protocol based on the counting of provenance
we analyze the theoretical complexity of the protocol in space
we analyze the theoretical complexity of the protocol in time
we analyze the theoretical complexity of the protocol in traffic
experimental results suggest the scalability of we approach to linked dataa recommendation approach that tries to understand what factors interest a user use this factor information to predict future item ratings
a user based on matrix factorisation past ratings for products movies songs 
a recommendation approach that tries to understand what factors interest then use this factor information to predict future item ratings
a user based on matrix factorisation past ratings for items 
matrix factorisation is a recommendation approach
a central limitation of this approach however is that this approach can not capture how a users tastes have evolved beforehand thereby ignoring if a users preference for a factor is likely to change
linked data
an item for which matrix factorisation has not rated the semantic categories previously
one solution to this is to include users preferences for  categories however this approach is limited should a user be presented with an item so called coldstart categories
one solution to this is to include users preferences for semantic  categories however this approach is limited should a user be presented with an item so called coldstart categories
in this paper we present a method to overcome this limitation by transferring incorporate this into our prior semanticsvd model
in this paper we present a method to overcome this limitation by transferring rated semantic categories in place of unrated categories through the use of vertex kernels
empirically demonstrate the superior performance that our achieve over existing svd models
our several vertex kernels effects on recommendation error
empirically demonstrate the superior performance that our achieve over existing semanticsvd with no transferred semantic categories
our evaluated several vertex kernels
empirically demonstrate the superior performance that our achieve over existing svd modelsthe ability to integrate a wealth of humancurated knowledge from scientific datasets and ontologies can benefit drugtarget interaction prediction
the hypothesis is that similar drugs interact with the same targets
similar targets interact with the same drugs
the our use semantic knowledge between drugs reflect a chemical semantic space while our use semantic knowledge between targets reflect a genomic semantic space
in this paper we present drugdrug similarity edges
a heterogeneous graph that includes drugtarget interaction edges
a novel method that combines semantic knowledge  from ontologies
in this paper we present a novel method the edges of a heterogeneous graph
a novel method that combines semantic knowledge  from semantic spaces
a novel method that combines our use semantic knowledge  from semantic spaces
a novel method that combines an algorithmic approach to partition
in this paper we present targettarget similarity edges
a novel method that combines a data mining framework for link prediction
a novel method that combines our use semantic knowledge  from ontologies
community detection which allows a node to participate in more than one cluster or community
edge based community detection
our semantics based edge partitioning approach semep
our semantics has the advantages of edge
the semep problem is to create a minimal partitioning of the edges such that the cluster density of each subset of edges is maximal
our use semantic knowledge to specify specific drugtarget interaction edges that should not participate in the same cluster
our use semantic knowledge to specify edge constraints that should not participate in the same cluster
stateoftheart machine learning based prediction methods
using a wellknown dataset of drugtarget interactions we demonstrate the benefits of using semep predictions to improve the performance of a range of stateoftheart machine learning
validation of the novel best reflect both accurate and diverse predictions
the novel best predicted interactions of semep against the stitch interaction resourcemany tasks require knowledge about how the elements of properties  are expressed in natural language
many tasks in which a system needs to mediate between natural language expressions and elements of a vocabulary in an ontology or dataset
many tasks require knowledge about how the elements of classes  are expressed in natural language
many tasks require knowledge about how the elements of the vocabulary  are expressed in natural language
many tasks require knowledge about how the elements of individuals  are expressed in natural language
in a multilingual setting such knowledge is needed for each of the supported languages
in this paper we present a framework for automatically inducing ontology lexica in multiple languages on the basis of a multilingual corpus
in this paper we present matoll 
languagespecific dependency patterns which are run over a parsed corpus
languagespecific dependency patterns which are formalized as sparql queries
a framework for automatically inducing ontology lexica in multiple languages on the basis of a multilingual corpus exploits a set of languagespecific dependency patterns
we have instantiated the system for two languages german
we have instantiated the system for two languages english
we evaluate the system for two languages german in terms of recall for english by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages german in terms of recall for german by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages german in terms of fmeasure for german by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages german in terms of precision for english by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages german in terms of fmeasure for english by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages german in terms of precision for german by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages english in terms of recall for german by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages english in terms of precision for english by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages english in terms of precision for german by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages english in terms of recall for english by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages english in terms of fmeasure for english by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
we evaluate the system for two languages english in terms of fmeasure for german by comparing an automatically induced lexicon to manually constructed ontology lexica for dbpedia
in particular we perform an analysis of the impact of different parameters
in particular we investigate the contribution of each single dependency patternthe increase in the volume and heterogeneity of biomedical data sources enhance information discovery
the increase in the volume and heterogeneity of biomedical data sources has motivated researchers to embrace linked data technologies to solve the ensuing integration challenges
as an integral part of the eu granatum project a linked biomedical dataspace augment the design of in silico experiments for cancer chemoprevention drug discovery
as an integral part of the eu granatum project a linked biomedical dataspace was developed to semantically interlink data from multiple sources
the different components of the a linked biomedical dataspace facilitate both the biomedical researchers to publish link query
the different components of the a linked biomedical dataspace visually explore the heterogeneous datasets
the different components of the a linked biomedical dataspace facilitate both the bioinformaticians to publish link query
we have extensively evaluated the usability of the entire platform
in this paper we showcase three different workflows
three different workflows depicting realworld scenarios on the use of a linked biomedical dataspace by the domain users to intuitively retrieve meaningful information from the integrated sources
the collaborative processes which would make it easier for linked data practitioners to create such dataspaces in other domains
we report the important lessons that we learned through the challenges encountered
we accumulated experience during the collaborative processes
we also provide a concise set of generic recommendations to develop linked data platforms useful for drug discoverya considerable portion of the information on the semantic web is still only available in unstructured form
implementing the vision of the semantic web thus requires transforming this unstructured data into structured data
one key step during this process is the recognition of named entities
previous works suggest that ensemble learning can be used to improve the performance of named entity recognition tools
however no comparison of the performance of existing supervised machine has been presented so far
existing supervised machine learning approaches on this task
we address this research gap by presenting a thorough evaluation of named entity recognition based on ensemble learning
to this end we combine four different stateofthe approaches by using 15 different algorithms for ensemble learning
to this end we evaluate ensemble learning performace on five different datasets
we results suggest that ensemble learning can reduce the error rate of stateoftheart named entity recognition systems by 40sparql is the query language for the resource description framework
the resource description framework is a standard for conceptually describing data on the web
as the resource description framework data continue to be integrated at webscale such as in the linked open data cloud the resource description framework data management systems are being exposed to queries
as the resource description framework data continue to be published across heterogeneous domains management systems are being exposed to workloads
workloads that are far more varied
as the resource description framework data continue to be integrated at webscale such as in the linked open data cloud the resource description framework data management systems are being exposed to workloads
as the resource description framework data continue to be published across heterogeneous domains management systems are being exposed to queries
queries that are far more diverse
an indepth experimental analysis that shows existing sparql benchmarks are not suitable for testing systems for diverse queries
the first contribution of our work is an indepth experimental analysis
an indepth experimental analysis that varied workloads
the waterloo sparql diversity test suite that provides stress testing tools for the resource description framework data management systems
to address these shortcomings our second contribution is the waterloo sparql diversity test suite
evaluations using earlier benchmarks
using the waterloo sparql diversity test suite our have been able to reveal issues with existing systems
existing systems that went unnoticed in evaluations
specifically our experiments with five popular the resource description framework data management systems show that management systems can not deliver good performance uniformly across workloads
for some queries there can be as much as five orders of magnitude difference between the query execution time of the fastest while the fastest system on one query may unexpectedly time out on another query
for some queries there can be as much as five orders of magnitude difference between the query execution time of the slowest system while the fastest system on one query may unexpectedly time out on another query
by performing a detailed analysis we pinpoint these problems to specific types of workloads
by performing a detailed analysis we pinpoint these problems to specific types of queriesin this paper we present a generic framework for annotating natural language texts with entities of owl 2 dl ontologies
in this paper we present semano 
a generic framework for annotating natural language texts with entities of owl 2 dl ontologies generalizes the mechanism of jape transducers
semano generalizes the mechanism of jape transducers
jape transducers that has been introduced within the general architecture for text engineering to enable modular development of annotation rule bases
rule templates called japelates
rule templates called the core of the semano rule base model instantiations
the core of the semano rule base model are rule templates
while semano is generic semano provides several generic japelates
while semano does not make assumptions about the document characteristics a generic framework for annotating natural language texts with entities of owl 2 dl ontologies provides several generic japelates
while a generic framework for annotating natural language texts with entities of owl 2 dl ontologies does not make assumptions about the document characteristics semano provides several generic japelates
while a generic framework for annotating natural language texts with entities of owl 2 dl ontologies is generic semano provides several generic japelates
while a generic framework for annotating natural language texts with entities of owl 2 dl ontologies is generic a generic framework for annotating natural language texts with entities of owl 2 dl ontologies provides several generic japelates
the document characteristics used within japelates
while a generic framework for annotating natural language texts with entities of owl 2 dl ontologies does not make assumptions about the document characteristics a generic framework for annotating natural language texts with entities of owl 2 dl ontologies provides several generic japelates
while semano does not make assumptions about the document characteristics semano provides several generic japelates
several generic japelates that can serve as a starting point
while semano is generic a generic framework for annotating natural language texts with entities of owl 2 dl ontologies provides several generic japelates
a tool that can generate an initial rule base from an ontology
also semano provides a tool
also a generic framework for annotating natural language texts with entities of owl 2 dl ontologies provides a tool
an initial rule base can be easily extended to meet the requirements of the application in question
in addition to an initial rule base java api a generic framework for annotating natural language texts with entities of owl 2 dl ontologies includes two gui components a rule base editor
in addition to an initial rule base java api semano includes two gui components an annotation viewer
in addition to an initial rule base java api semano includes two gui components a rule base editor
in addition to an initial rule base java api a generic framework for annotating natural language texts with entities of owl 2 dl ontologies includes two gui components an annotation viewer
in combination with the rule generator two gui components an annotation viewer can be used by domain experts
domain experts that are not familiar with the technical details of the framework to set up a domainspecific annotator
in combination with the rule generator two gui components a rule base editor can be used by domain experts
in combination with the default japelates two gui components a rule base editor can be used by domain experts
in combination with the default japelates two gui components an annotation viewer can be used by domain experts
in this paper we introduce the rule base model of semano a generic framework for annotating natural language texts with entities of owl 2 dl ontologies provide examples of adapting an initial rule base to meet particular application requirements and report we experience with applying semano 
in this paper we introduce the rule base model of semano a generic framework for annotating natural language texts with entities of owl 2 dl ontologies provide examples of adapting an initial rule base to meet particular application requirements and report we experience with applying a generic framework for annotating natural language texts with entities of owl 2 dl ontologies within the domain of nano technologysemantic data publishing which converts the web from a web of document to a web of interlinked knowledge
linking open data is the largest community effort for semantic data publishing
while the state of the open data contains billion of triples the state of the art open data has only a limited number of schema information
while the state of the open data contains billion of triples the state of the art open data is lack of schemalevel axioms
triples describing millions of entities
to close the gap between the open data we contribute to the complementary part of the open data that is linking open schema
to close the gap between the expressive ontologies we contribute to the complementary part of the open data that is linking open schema
in this paper we introduce the first effort to publish chinese linked open schema
in this paper we introduce zhishischema 
we collect navigational categories from more than 50 various most popular social web sites in china
we collect dynamic tags from more than 50 various most popular social web sites in china
we then propose a twostage method to capture equivalence subsumption
the collected categories and tags which results in a large semantic network
the collected categories and tags which results in an integrated concept taxonomy
we then relate relationships between the collected categories and tags
experimental results show the high quality of zhishischema
compared with category systems of babelnet zhishischema contains the largest number of subsumptions between categories
compared with category systems of freebase zhishischema contains the largest number of subsumptions between categories
compared with category systems of babelnet zhishischema has wide coverage of categories
compared with category systems of yago zhishischema has wide coverage of categories
compared with category systems of dbpedia zhishischema contains the largest number of subsumptions between categories
compared with category systems of freebase zhishischema has wide coverage of categories
compared with category systems of dbpedia zhishischema has wide coverage of categories
compared with category systems of yago zhishischema contains the largest number of subsumptions between categories
when substituting zhishischema for the original category system of zhishime we not only filter out incorrect category subsumptions but also add more finergrained categoriesupdates in rdf stores have recently been standardised in the sparql 11 update specification
computing entailed answers by ontologies
however computing is usually treated orthogonally to updates in triple stores
sparql 11 entailment regimes specifications explicitly exclude a standard behaviour for entailment regimes other than simple entailment in the context of updates
even the w3c sparql 11 update explicitly exclude a standard behaviour for entailment regimes other than simple entailment in the context of updates
in this paper we take a first step to close this gap
language dealing with updates both of abox and of tbox statements
we define a fragment of sparql basic graph patterns corresponding to dllite
the corresponding sparql update language
we discuss possible semantics along with potential strategies for implementing potential strategies
in particular we reduced rdf stores that is redundancyfree rdf stores
in particular we treat both redundancyfree rdf stores
in particular we materialised rdf stores redundancyfree rdf stores
redundancyfree rdf stores that do not store any rdf triples
all entailed triples explicitly
any rdf triples entailed by others already
rdf stores which store all
we have present some indications on practical feasibility
we have implemented all semantics prototypically on top of an offtheshelf triple storewikidata is the central data management platform of wikipedia
by the efforts of thousands of volunteers the project has produced a large open knowledge base with many interesting applications
the data is also very rich in rdf
the data is also not available in rdf
the data is also complex in rdf
the data is highly interlinked and connected to many other datasets
to address this issue we introduce new rdf exports
new rdf exports that connect wikidata to the linked data web
we discuss wikidata
wikidata encoding in rdf
we explain the data model of wikidata
several partial exports that provide more selective
moreover we introduce simplified views on the data
moreover we introduce several partial exports
this includes several other types of ontological axioms that we extract from the site
this includes a class hierarchy that we extract from the site
all datasets we discuss here are freely available online
all datasets we discuss here are updated regularlydriven by initiatives like schemaorg the amount of semantically annotated data is expected to grow steadily towards massive scale requiring clusterbased solutions to query clusterbased solutions
at the same time hadoop has become dominant in the area of big data processing with large infrastructures
large infrastructures being already used in manifold application fields
large infrastructures being already deployed in manifold application fields
for hadoopbased applications a common data pool provides many synergy benefits
many synergy benefits making hadoopbased very attractive to use these infrastructures for semantic data processing as well
indeed existing sparqlon hadoop approaches have already demonstrated very good scalability however query runtimes are rather slow due to the underlying batch processing framework
while this is acceptable for dataintensive queries it is not satisfactory for the majority of sparql queries
sparql queries that are typically much more selective requiring only small subsets of the data
in this paper we present sempala
a sparqloversqlonhadoop approach designed with selective queries in mind
in this paper we present a sparqloversqlonhadoop approach
our evaluation shows performance improvements by an order of magnitude compared to existing approaches paving the way for interactivetime sparql query processing on hadoopwhile the semantic web currently can exhibit provenance information by using the w3c prov standards there is a  missing link  in connecting prov to storing for dynamic changes to rdf graphs
rdf graphs using sparql
while the semantic web currently can exhibit provenance information by using the w3c prov standards there is a  missing link  in connecting prov to querying for dynamic changes to rdf graphs
solving this problem would be required for such clear usecases as the creation of version control systems for rdf
while annotation techniques for querying provenance data originally developed with databases in mind transfer readily to sparql annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with databases in mind transfer readily to rdf some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with databases in mind transfer readily to rdf some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with databases in mind transfer readily to rdf some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to sparql annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with databases in mind transfer readily to rdf some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with databases in mind transfer readily to sparql some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with databases in mind transfer readily to rdf annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to rdf annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with workflows in mind transfer readily to sparql annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with workflows in mind transfer readily to rdf some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to sparql some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with databases in mind transfer readily to rdf annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with workflows in mind transfer readily to rdf annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to rdf annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with databases in mind transfer readily to sparql annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to sparql some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with databases in mind transfer readily to sparql annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with databases in mind transfer readily to sparql some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with databases in mind transfer readily to rdf some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with workflows in mind transfer readily to sparql some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with databases in mind transfer readily to sparql some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with databases in mind transfer readily to sparql annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to rdf some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with workflows in mind transfer readily to rdf some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to sparql annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with databases in mind transfer readily to rdf some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with databases in mind transfer readily to rdf annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with databases in mind transfer readily to rdf annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with workflows in mind transfer readily to rdf annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with databases in mind transfer readily to sparql annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to sparql annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to rdf annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with databases in mind transfer readily to sparql some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to rdf annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to rdf some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with databases in mind transfer readily to rdf annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with databases in mind transfer readily to sparql annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with databases in mind transfer readily to sparql some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with databases in mind transfer readily to rdf annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with workflows in mind transfer readily to sparql annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with workflows in mind transfer readily to rdf annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with workflows in mind transfer readily to sparql some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with workflows in mind transfer readily to rdf some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to sparql some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to rdf some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with databases in mind transfer readily to sparql annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to sparql some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with workflows in mind transfer readily to sparql annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with databases in mind transfer readily to sparql annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with databases in mind transfer readily to rdf some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with workflows in mind transfer readily to rdf annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with databases in mind transfer readily to rdf some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with workflows in mind transfer readily to sparql some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with databases in mind transfer readily to sparql some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with workflows in mind transfer readily to rdf some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with workflows in mind transfer readily to sparql annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with workflows in mind transfer readily to rdf some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with databases in mind transfer readily to sparql some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for storing provenance data originally developed with databases in mind transfer readily to sparql some provenance models for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for querying provenance data originally developed with databases in mind transfer readily to rdf annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with workflows in mind transfer readily to sparql some provenance models for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while annotation techniques for querying provenance data originally developed with databases in mind transfer readily to rdf annotation techniques for storing provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
while some provenance models for storing provenance data originally developed with workflows in mind transfer readily to sparql annotation techniques for querying provenance data do not readily adapt to describing changes in dynamic rdf datasets over time
in this paper we explore how the provenance information can be defined by reinterpreting sparql updates
in this paper we explore how to represent the resulting provenance records sparql updates as rdf in a manner compatible with w3c prov
rdf datasets that change over time in response to sparql updates
in this paper we explore how to adapt the dynamic copypaste provenance model of buneman et al to rdf datasets
the primary contribution of this paper we explore how to adapt the dynamic copypaste provenance model of buneman is a semantic framework
a semantic framework that enables the semantics of sparql update to be used as the basis for a  cutandpaste  provenance model in a principled manneronly a small fraction of the information on the web is represented as linked data
this lack of coverage is partly due to the paradigms followed so far to extract linked data
extraction methods based on adhoc solutions
while converting structured data to rdf is well supported by tools most approaches to extract rdf from semistructured data rely on extraction methods
in this paper we present a opensource framework for the extraction of rdf from templated websites
in this paper we present a holistic framework for the extraction of rdf from templated websites
we discuss the architecture of the initial implementation of each of a holistic framework for the extraction of rdf components
we discuss the architecture of a opensource framework for the extraction of rdf
we discuss the architecture of a holistic framework for the extraction of rdf
we discuss the architecture of the initial implementation of each of a opensource framework for the extraction of rdf components
in particular we present a novel wrapper induction technique
a novel wrapper induction technique that does not require any human supervision to detect the wrappers
a consistency layer with which the data can be checked for logical consistency
a holistic framework for the extraction of rdf also includes a consistency layer
the data extracted by the wrappers
a opensource framework for the extraction of rdf also includes a consistency layer
we evaluate the initial version of rex on three different datasets
we results clearly show the potential of using templated web pages to extend the linked data cloud
moreover our results indicate the weaknesses of we how our current implementations can be extended
moreover our results indicate the weaknesses of we current implementationsexploiting identity links among rdf resources allows applications to efficiently integrate data
keys can be very useful to discover these identity links
a key when a set of properties values uniquely identify resources
a set of properties is considered as a key
however keys are usually not available
the approaches can easily require clean data
the approaches that attempt to automatically discover keys
the approaches can easily be overwhelmed by the size of the data
we present an approach
an approach that discovers keys in rdf data in an efficient way
we present sakey
to prune the search space sakey exploits characteristics of the data
the data that are dynamically detected during the process
datasets where erroneous data or duplicates exist
furthermore we approach can discover keys in datasets  almost keys 
our approach has been evaluated on different synthetic
our approach has been evaluated on real datasets
the results show both the relevance of almost keys
the results show both the relevance of the efficiency of discovering almost keysin this paper we show how event processing over semantically annotated streams of events can be exploited for implementing tracking of products in supply chains through the automated generation of linked pedigrees
in this paper we show how event processing over semantically annotated streams of events can be exploited for implementing tracing of products in supply chains through the automated generation of linked pedigrees
in our abstraction events are encoded as spatially and temporally oriented named graphs while linked pedigrees as rdf datasets are linked pedigrees as rdf datasets specific compositions
an algorithm that operates over streams of rdf annotated epcis events to generate linked pedigrees
our propose an algorithm
our exemplify our approach using the pharmaceuticals supply chain
our show how counterfeit detection is an implicit part of our pedigree generation
our evaluation results show that for fast moving supply chains smaller window sizes on event streams provide significantly higher efficiency in the generation of pedigrees
our evaluation results show that for fast moving supply chains smaller window sizes on event streams enable early counterfeit detectionas the web of data is growing at an ever increasing speed the lack of reliable query solutions for live public data becomes apparent
sparql implementations have deliver impressive performance for public sparql endpoints yet poor availability especially under high loads prevents sparql implementations use in realworld applications
sparql implementations have matured yet poor availability especially under high loads prevents sparql implementations use in realworld applications
linked data fragments that enable lowcost publication of queryable data by moving intelligence from the server to the client
we propose to tackle this availability problem by defining a specific kind of linked data fragments
we propose to tackle this availability problem by defining triple pattern fragments
this paper formalizes the linked data fragments concept
this paper verifies servers availability under load
a clientside sparql query processing algorithm that uses a dynamic iterator pipeline
this paper introduces a clientside sparql query processing algorithm
the results indicate that at the cost of lower performance query techniques with triple pattern fragments lead to high availability thereby allowing for reliable applications on top of public queryable linked datawe present a description and analysis of the data access challenge in the siemens energy
a suitable semantic web driven technology to address the challenge
we advocate for ontology based data access as a suitable semantic web
we discuss data access systems limitations with respect to the siemens requirements
we derive requirements for applying data access in siemens review existing data access systems
we then introduce the optique platform as a suitable data access solution for siemens
finally we describe we preliminary installation and evaluation of the platform in siemensweb pages that is processed by the major search engines to improve search performance
schemaorg is a way to add machineunderstandable information to web pages
the definition of schemaorg is provided as a set of web pages plus a partial mapping into rdf triples with unusual properties
the definition of schemaorg is incomplete in a number of places
this analysis of provides a complete basis for a plausible version of what schemaorg should be
formal semantics for schemaorg provides a complete basis for a plausible version of what schemaorg should beontologybased data access concerns answering queries over the target ontology
given a source relational database ontologybased data access concerns
given a target owl ontology ontologybased data access concerns
the target ontology using these three components
given a mapping from the source database to the target ontology ontologybased data access concerns
this paper presents the development of an ontologybased data access system that is a hybridization of materialization
this paper presents the development of ultrawrapobda that is a hybridization of query rewriting
an ontologybased data access system comprising bidirectional evaluation
this paper presents the development of ultrawrapobda that is a hybridization of materialization
this paper presents the development of an ontologybased data access system that is a hybridization of query rewriting
we observe that by compiling the ontological entailments as mappings is able to reduce the execution time of a sparql query by rewriting the query in terms of the views
we observe that by implementing the mappings as sql views is able to reduce the execution time of a sparql query by rewriting the query in terms of the views
we observe that by materializing a subset of the views the underlying sql optimizer is able to reduce the execution time of a sparql query by rewriting the query in terms of the views
the views specified by the mappings
the first ontologybased data access system supporting ontologies with transitivity by using sql recursion
to the best of our knowledge this is the first ontologybased data access system
a stateoftheart ontologybased data access system which validates the cost model
a stateoftheart ontologybased data access system which demonstrates favorable execution times
an empirical evaluation comparing with a stateoftheart ontologybased data access system
we contributions include an empirical evaluation
we contributions include an efficient algorithm to compile ontological entailments as mappings
we contributions include a cost model to determine which views to materialize to attain the fastest execution time
we contributions include a proof that every sparql query can be rewritten into a sql query in the context of mappingswe present the dutch ships
we present sailors linked data cloud
this heterogeneous dataset brings together four curated datasets on dutch maritime history as fivestar linked data
separate datamodels designed in close collaboration with maritime historical researchers
the individual datasets use separate datamodels
the individual models are mapped to a common interoperability layer allowing for analysis of the data on the general level
we present links to external data sources
we present the datasets to external data sources
we present internal links to external data sources
we present modeling decisions to external data sources
we show ways of accessing the data
we show ways of present a number of examples of how this heterogeneous dataset can be used for historical research
the dutch ships is a potential hub dataset for digital history research
sailors linked data cloud sailors linked data cloud is a potential hub dataset for digital history research
the dutch ships is a prime example of the benefits of linked data for this field
sailors linked data cloud sailors linked data cloud is a prime example of the benefits of linked data for this fieldwhen are two entries about a small molecule in different datasets the same
if they have the same drug name
if they have some other criteria
if they have chemical structure
the choice depends upon the application to which existing linked data will be put
however existing linked data approaches provide a single global view over existing linked data with no way of varying the notion of equivalence to be appliedthe central idea of linked data is that data publishers support applications in integrating data by complying to a set of best practices in the areas of linking
the central idea of linked data is that data publishers support applications in integrating data by complying to a set of best practices in metadata provision
the central idea of linked data is that data publishers support applications in integrating data by complying to a set of best practices in vocabulary usage
the central idea of linked data is that data publishers support applications in discovering
in 2011 the state of the lod cloud report analyzed the adoption of these best practices by linked datasets within different topical domains
the lod cloud report was based on information
information that was provided by the dataset publishers the dataset publishers themselves via the datahubio linked data catalog via the datahubio linked data catalog
in this paper we update the findings of the 2011 state of the lod cloud report based on a crawl of the web of linked data
in this paper we revisit the findings of the 2011 state of the lod cloud report based on a crawl of the web of linked data
linked data conducted in april 2014
data that can actually be retrieved by a linked data crawler
we analyze how the adoption of the different best practices has changed and present an overview of the linkage relationships between datasets in the form of an updated lod cloud diagram this time not based on information from dataset providers but on data
among others we find that the number of linked datasets has approximately doubled between 2011
among others we find that the number of linked datasets has approximately doubled between 2014
among others we find that there is increased agreement on common vocabularies for describing certain types of entities
among others we find that provenance metadata is still rarely provided by the data sources
among others we find that license metadata is still rarely provided by the data sourcessparql 11 supports the use of ontologies to enrich query results with logical entailments
owl 2 provides a dedicated fragment owl ql for this purpose
typical implementations use the owl ql schema to rewrite a conjunctive query into an equivalent set of queries to be answered against the nonschema part of the data
we ask how this can be exploited in query rewriting
with the adoption of the recent sparql 11 standard
however rdf databases are capable of answering much more expressive queries directly
we find that sparql 11 is powerful enough to  implement  a fullfledged owl ql reasoner in a single query
using additional sparql 11 features we develop a new method of schemaagnostic query rewriting where arbitrary conjunctive queries over owl ql are rewritten into equivalent sparql 11 queries in a way
a way that is fully independent of the actual schema
this allows us to query rdf data under owl ql entailment without extracting owl axioms
this allows us to query rdf data under owl ql entailment without preprocessing owl axiomsit is widely accepted that proper data publishing is difficult
data publishing guidelines
the majority of linked open data does not meet even a core set of data
moreover datasets can get stains over time
datasets that are clean at creation
dirty data that is difficult for humans for machines to process
dirty data that is difficult for humans to clean
as a result the linked open data cloud now contains a high level of dirty datathis paper describes tableminer
this paper describes bootstrapping learning approach selected  partial  data from a table
this paper describes the first semantic table interpretation method
the first semantic table interpretation method that adopts an incremental mutually recursive
bootstrapping learning approach seeded by automatically
columns containing named entity
tableminer labels columns mentions with semantic concepts that best describe data in these columns
tableminer labels columns mentions with semantic concepts that best disambiguates entity content cells in these columns
tableminer is able to use various types of contextual information outside tables for table interpretation including rdfamicrodata annotations  that to the best of our knowledge have never been used in natural language processing tasks
tableminer is able to use various types of contextual information outside tables for table interpretation including semantic markups  that to the best of our knowledge have never been used in natural language processing tasks
evaluation on two datasets shows that compared to two baselines tableminer consistently obtains the best performance
in the classification task tableminer achieves significant improvements of between 008 f1 depending on different baseline methods in the classification task tableminer outperforms both baselines by between 019 in precision on one dataset and between 003 f1 on the other dataset
in the classification task tableminer achieves significant improvements of 038 f1 depending on different baseline methods in the classification task tableminer outperforms both baselines by between 037 in precision on one dataset and between 002 f1 on the other dataset
in the classification task tableminer achieves significant improvements of 038 f1 depending on different baseline methods in the classification task tableminer outperforms both baselines by between 019 in precision on one dataset and between 003 f1 on the other dataset
in the classification task tableminer achieves significant improvements of 038 f1 depending on different baseline methods in the classification task tableminer outperforms both baselines by between 037 in precision on one dataset and between 003 f1 on the other dataset
in the classification task tableminer achieves significant improvements of between 008 f1 depending on different baseline methods in the classification task tableminer outperforms both baselines by between 037 in precision on one dataset and between 002 f1 on the other dataset
in the classification task tableminer achieves significant improvements of between 008 f1 depending on different baseline methods in the classification task tableminer outperforms both baselines by between 019 in precision on one dataset and between 002 f1 on the other dataset
in the classification task tableminer achieves significant improvements of 038 f1 depending on different baseline methods in the classification task tableminer outperforms both baselines by between 019 in precision on one dataset and between 002 f1 on the other dataset
in the classification task tableminer achieves significant improvements of between 008 f1 depending on different baseline methods in the classification task tableminer outperforms both baselines by between 037 in precision on one dataset and between 003 f1 on the other dataset
observation also shows that the bootstrapping can potentially deliver computational savings of 60
the bootstrapping learning approach
approach adopted by tableminer
observation also shows that the bootstrapping can potentially deliver computational savings of between 24over the last decades several billion web pages have been made available on the web of data
the ongoing transition from the current web of unstructured data to the web of data yet requires scalable approaches for the extraction of structured data in rdf  from these websites
the ongoing transition from the current web of unstructured data to the web of data yet requires accurate approaches for the extraction of structured data in resource description framework  from these websites
the ongoing transition from the current web of unstructured data to the web of data yet requires scalable approaches for the extraction of structured data in resource description framework  from these websites
the ongoing transition from the current web of unstructured data to the web of data yet requires accurate approaches for the extraction of structured data in rdf  from these websites
one of the key steps towards extracting rdf from text is the disambiguation of named entities
while several approaches aim to tackle this problem several approaches still achieve poor accuracy
we address this drawback by presenting agdistis 
we address this drawback by presenting a novel knowledgebaseagnostic approach for named entity disambiguation
we approach combines the hypertextinduced topic search  hits  algorithm with string similarity measures
we approach combines the hypertextinduced topic search  hits  algorithm with label expansion strategies
based on this combination agdistis can efficiently detect the correct uris for a given set of named entities within an input text
we evaluate we approach on eight different datasets against stateoftheart named entity disambiguation frameworks
we results indicate that we outperform the stateoftheart approach by up to 29outlier detection used for identifying wrong values in data
outlier detection is typically applied to single datasets to search data for values of unexpected behavior
an approach which combines the outcomes of two independent outlier detection runs to get a more reliable result
problems arising from natural outliers
natural outliers which are nevertheless correct
in this work we instead propose an approach
natural outliers which are exceptional values in the dataset
an approach which combines the outcomes of two independent outlier detection runs to also prevent problems
linked data is especially suited for the application of such an idea since linked data also contains explicit links between instances
linked data is especially suited for the application of such an idea since linked data provides large amounts of data
data enriched with hierarchical information
the property values extracted from a single repository
in a first step we apply outlier detection methods to the property values using a novel approach for splitting the data into relevant subsets
for the second step we exploit owl sameas links for the instances to perform a second outlier detection on additional property values
for the second step we exploit owl sameas links for the instances to get additional property values
doing so allows us to reject the assessment of a wrong value
doing so allows us to confirm the assessment of a wrong value
experiments on the nell datasets demonstrate the feasibility of our approach
experiments on the dbpedia datasets demonstrate the feasibility of our approachdata instances stored in relational databases
we present an extension of the ontologybased data access platform ontop
the ontologybased data access platform ontop that supports answering sparql queries under the owl 2 ql direct semantics entailment regime for data instances
on the theoretical side we show how r2rml mappings can be rewritten to an equivalent sql query solely over the data
on the theoretical side we show how any input sparql query owl 2 ql ontology can be rewritten to an equivalent sql query solely over the data
initial experimental results demonstrating that by applying the ontop technologies the treewitness query  the system produces scalable sql queries
tmappings compiling r2rml mappings with ontology hierarchies
initial experimental results demonstrating that by applying the ontop technologies tmapping optimisations  the system produces scalable sql queries
on the theoretical side we present initial experimental results
tmapping optimisations using database integrity constraints
the treewitness query rewriting
initial experimental results demonstrating that by applying the ontop technologies tmappings  the system produces scalable sql queries
tmapping optimisations using sql expressivitysemantic datasets provide support to automate many tasks such as question answering
semantic datasets provide support to automate many tasks such as decisionmaking answering
however semantic datasets performance is always decreased by the noises in the datasets among which noisy type assertions play an important role
this problem has been mainly studied in the domain of data mining
this problem has been mainly studied in the semantic web community
in this paper we study the problem of noisy type assertion detection in semantic web datasets by making use of concept disjointness relationships
concept disjointness relationships hidden in the datasets
we transform noisy type assertion detection into multiclass classification of pairs of type assertions
type assertions which type an individual to two potential disjoint concepts
the multiclass classification is solved by adaboost with c4 5 as the base classifier
furthermore we propose instanceconcept compatability metrics based on instanceinstance relationships
furthermore we propose instanceconcept compatability metrics based on instanceconcept assertions
we evaluate the approach on both synthetic datasets and dbpedia
we approach effectively detect noisy type assertions in dbpedia with a high precision of 95semantic web technologies are used in a variety of domains for semantic web technologies ability to facilitate data integration
semantic web technologies are used in enabling expressive standardsbased reasoning
deploying semantic web reasoning processes directly on mobile devices reduced infrastructure requirements
deploying semantic web reasoning processes directly on mobile devices has a number
deploying semantic web reasoning processes directly on mobile devices has robustness to connectivity loss more timely results
at the same time a number of challenges arise as well related to mobile platform heterogeneity
at the same time a number of challenges arise as well related to limited computing resources
datasets of complexity flows
datasets of existing reasoning process flows
datasets of varying scale flows
to tackle these challenges it should be possible to benchmark mobile reasoning performance across different mobile platforms with rule
datainterface on top of mobile reasoning engines should be provided as well
to deal with the current heterogeneity of rule formats
to deal with the current heterogeneity of a uniform rule
in this paper we present a crossplatform benchmark framework
a crossplatform benchmark framework that supplies 2  a benchmark engine to investigate mobile reasoning performance
a crossplatform benchmark framework that supplies 2  a benchmark engine to compare mobile reasoning performance
a crossplatform benchmark framework that supplies 1  a generic standardsbased semantic web layer on top of existing mobile reasoning engines
