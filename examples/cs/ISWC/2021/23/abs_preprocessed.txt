kg  embedding models have emerged as powerful means for kg completion
knowledge graph  embedding models have emerged as powerful means for kg completion
to learn the representation of relations are projected in a lowdimensional vector space so that not only existing triples in the kg are preserved
to learn the representation of kgs are projected in a lowdimensional vector space so that not only existing triples in the kg are preserved
to learn the representation of entities are projected in a lowdimensional vector space so that not only existing triples in the kg are preserved
new triples can be predicted
due to the nature of machine learning approaches embedding models often lose the semantics of entities
entities which might lead to nonsensical predictions
embedding models might learn a good representation of the input kg
due to the nature of machine learning approaches embedding models often lose the semantics of relations
relations which might lead to nonsensical predictions
embeddings using ontological reasoning
to address this issue we propose to improve the accuracy of embeddings
a novel iterative approach reasonkge that identifies dynamically via symbolic reasoning inconsistent predictions
a novel iterative approach reasonkge that feeds embedding models as negative samples for retraining a given embedding model
more specifically we present a novel iterative approach reasonkge
symbolic reasoning inconsistent predictions produced by a given embedding model
the scalability problem that arises when integrating ontological reasoning into the training process
in order to address the scalability problem we propose an advanced technique to generalize the inconsistent predictions to other semantically similar negative samples during retraining
experimental results demonstrate the improvements in accuracy of facts
facts produced by our method compared to the stateoftheart