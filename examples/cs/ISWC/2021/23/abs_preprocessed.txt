Knowledge graph  embedding models have emerged as powerful means for KG completion
KG  embedding models have emerged as powerful means for KG completion
To learn the representation of KGs are projected in a lowdimensional vector space so that not only existing triples in the KG are preserved
new triples can be predicted
To learn the representation of relations are projected in a lowdimensional vector space so that not only existing triples in the KG are preserved
To learn the representation of entities are projected in a lowdimensional vector space so that not only existing triples in the KG are preserved
due to the nature of machine learning approaches Embedding models often lose the semantics of relations
Embedding models might learn a good representation of the input KG
entities which might lead to nonsensical predictions
relations which might lead to nonsensical predictions
due to the nature of machine learning approaches Embedding models often lose the semantics of entities
To address this issue we propose to improve the accuracy of embeddings
embeddings using ontological reasoning
a novel iterative approach ReasonKGE that feeds Embedding models as negative samples for retraining a given embedding model
a novel iterative approach ReasonKGE that identifies dynamically via symbolic reasoning inconsistent predictions
symbolic reasoning inconsistent predictions produced by a given embedding model
More specifically we present a novel iterative approach ReasonKGE
In order to address the scalability problem we propose an advanced technique to generalize the inconsistent predictions to other semantically similar negative samples during retraining
the scalability problem that arises when integrating ontological reasoning into the training process
facts produced by our method compared to the stateoftheart
Experimental results demonstrate the improvements in accuracy of facts