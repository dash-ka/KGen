knowledge graphs such as wikidata are created by a range of sources leaving contributors prone to two types of errors
knowledge graphs such as wikidata are created by a diversity of contributors leaving contributors prone to two types of errors
the first type of error  is addressed by property graphs through the representation of validity making triples occur as firstorder objects in subject position of metadata triples
the first type of error  is addressed by property graphs through the representation of provenance making triples occur as firstorder objects in subject position of metadata triples
falsity of facts is addressed by property graphs through the representation of validity making triples occur as firstorder objects in subject position of metadata triples
falsity of facts is addressed by property graphs through the representation of provenance making triples occur as firstorder objects in subject position of metadata triples
violation of domain constraints has not been addressed with regard to property graphs so far
the second type of error  has not been addressed with regard to property graphs so far
in rdf representations this error can be addressed by shape languages such as shex
shacl which allow for checking whether graphs are valid with respect to a set of domain constraints
in rdf representations this error can be addressed by shape languages such as shacl
shex which allow for checking whether graphs are valid with respect to a set of domain constraints
borrowing ideas from the semantics definitions of shacl we design a shape language for property graphs progs such as keyvalue annotations to both nodes and edges
borrowing ideas from the semantics definitions of shacl we design a shape language for property graphs progs such as edges with identities
semantics definitions of shacl specific constructs
progs which allows for formulating shape constraints on property graphs
borrowing ideas from the syntax definitions of shacl we design a shape language for property graphs progs such as keyvalue annotations to both nodes and edges
property graphs
borrowing ideas from the syntax
borrowing ideas from the syntax definitions of shacl we design a shape language for property graphs progs such as edges with identities
progs which allows for formulating shape constraints on
we define a formal semantics of progs investigate the resulting complexity of validating property graphs against sets of progs shapes implement a prototypical validator
a prototypical validator that utilizes answer
answer set programming
we define a formal semantics of progs investigate the resulting complexity of validating property graphs against sets of progs shapes compare with corresponding results for shaclfor many years link prediction on knowledge graphs has been a purely transductive task not allowing for reasoning on unseen entities
recently increasing efforts are put into emerging entities
recently increasing efforts are put into exploring semi and fully inductive scenarios enabling inference over unseen
still all these approaches only consider triplebased knowledge graphs whereas all these approaches richer counterparts hyperrelational knowledge graphs  have not yet been properly studied
still all these approaches only consider triplebased knowledge graphs whereas all these approaches richer counterparts wikidata  have not yet been properly studied
in this work we classify different inductive settings
in this work we study the benefits of employing hyperrelational knowledge graphs on a wide range of semi
fully inductive link prediction tasks powered by recent advancements in graph neural networks
in this work we study fully inductive link prediction tasks
absolute gains compared to tripleonly baselines
our experiments on a novel set of benchmarks show that qualifiers over typed edges can lead to performance improvements of 6 percent of absolute gainsfor reusing an rdf dataset understanding an rdf dataset content is a prerequisite
to support the comprehension of an rdf dataset large structure existing methods mainly generate an abridged version of an rdf dataset by extracting representative data patterns as a summary
to support the comprehension of an rdf dataset complex structure existing methods mainly generate an abridged version of an rdf dataset by extracting representative data patterns as a summary
as a complement recent attempts extract a representative subset of concrete data as a snippet
we extend this line of research by injecting the strength of summary into snippet
we propose to generate a patterncoverage snippet that best exemplifies the patterns of entity descriptions and links in an rdf dataset
this extensible approach incorporates formulations of group steiner tree to generate compact snippets
this extensible approach incorporates formulations of set cover problems to generate compact snippets
this extensible approach is also capable of modeling query relevance to be used with dataset search
experiments on thousands of real rdf datasets demonstrate the effectiveness and practicability of we approachkg  embedding models have emerged as powerful means for kg completion
knowledge graph  embedding models have emerged as powerful means for kg completion
to learn the representation of relations are projected in a lowdimensional vector space so that not only existing triples in the kg are preserved
to learn the representation of kgs are projected in a lowdimensional vector space so that not only existing triples in the kg are preserved
to learn the representation of entities are projected in a lowdimensional vector space so that not only existing triples in the kg are preserved
new triples can be predicted
due to the nature of machine learning approaches embedding models often lose the semantics of entities
entities which might lead to nonsensical predictions
embedding models might learn a good representation of the input kg
due to the nature of machine learning approaches embedding models often lose the semantics of relations
relations which might lead to nonsensical predictions
embeddings using ontological reasoning
to address this issue we propose to improve the accuracy of embeddings
a novel iterative approach reasonkge that identifies dynamically via symbolic reasoning inconsistent predictions
a novel iterative approach reasonkge that feeds embedding models as negative samples for retraining a given embedding model
more specifically we present a novel iterative approach reasonkge
symbolic reasoning inconsistent predictions produced by a given embedding model
the scalability problem that arises when integrating ontological reasoning into the training process
in order to address the scalability problem we propose an advanced technique to generalize the inconsistent predictions to other semantically similar negative samples during retraining
experimental results demonstrate the improvements in accuracy of facts
facts produced by our method compared to the stateoftheartincorporating external knowledge to visual question answering has become a vital practical need
existing methods mostly adopt pipeline approaches with different components for knowledge matching and extraction feature learning however such pipeline approaches suffer when some component does not perform well which leads to error cascading
existing methods mostly adopt pipeline approaches with different components for knowledge matching and extraction feature learning however such pipeline approaches suffer when some component does not perform well which leads to poor overall performance
furthermore the majority of existing approaches ignore the answer bias issue many answers may have never appeared during unseen answers  in realword application
furthermore the majority of existing approaches ignore the answer bias issue many answers may have never appeared during training  in realword application
visual question answering
a zeroshot visual question answering algorithm using a maskbased learning mechanism for better
to bridge these gaps in this paper we propose a zeroshot visual question answering algorithm
a zeroshot visual question answering algorithm using knowledge graph for better
better incorporating external knowledge
to bridge these gaps in this paper we propose present new answerbased zeroshot visual question splits for the fvqa dataset
zeroshot visual question answering with unseen answers
experiments show that our method can achieve stateoftheart performance in zeroshot visual question meanwhile dramatically augment existing endtoend models on the normal fvqa taskwikidata is an open knowledge graph
an open knowledge graph built by a global community of volunteers
as wikidata advances in scale wikidata faces substantial challenges around editor engagement
substantial challenges are in terms of both attracting new editors to keep up with the sheer amount of work
substantial challenges are in terms of both retaining existing editors
experience  suggests that personalised recommendations could help especially newcomers
wikipedia suggests that personalised recommendations could help especially newcomers
newcomers who are sometimes unsure about how to contribute best to an ongoing effort
for this reason we propose a recommender system wikidatarec for wikidata items
editors relying on both item features
editors relying on itemeditor previous interaction
a recommender system uses a hybrid of contentbased filtering techniques to rank items for editors
a recommender system uses a hybrid of collaborative filtering techniques to rank items for editors
a neural network is designed to optimize a neural network with editorbased representation by itemeditor interaction
a neural network named neural mixture of representations
a neural network is designed to learn fine weights for the combination of itembased representations
a neural network named neural mixture of representations
to facilitate further research in this space we also create two benchmark datasets  000 editors responsible for a second one 000 more active editors
to facilitate further research in this space we also create a generalpurpose one with 220 000 editors responsible for 14 million interactions with 4 million items 000 more active editors
to facilitate further research in this space we also create a generalpurpose one with 220 000 editors responsible for a second one 000 more active editors
to facilitate further research in this space we also create two benchmark datasets  000 editors responsible for 14 million interactions with 4 million items 000 more active editors
a second one focusing on the contributions of more than 8
we perform an offline evaluation of the system on both datasets with promising results
we code
datasets are available at githubcomwikidatarecdeveloperwikidata  recommender
datasets are available at httpsmost of the existing structured digital information today is still stored in relational databases
that is why it is important for the semantic web effort to allow to query semantic web using sparql
that is why it is important for the semantic web effort to expose the information in relational databases as rdf
rdf that does not require formulating 8
rdf that does not require formulating explicit mapping rules 2
direct mapping is a fully automated approach for converting wellstructured relational data to rdf
along with the it is desirable to have a description of that data
the mapped rdf data
previous work 3 has attempted to describe the rdf graph in terms of owl axioms
8 has attempted to describe the rdf graph in terms of owl axioms
owl axioms which is problematic partly due to the open world semantics of owl
et al which integrates the functionalities of proposal
et al which integrates the functionalities of the w3c recommendation
et al which extends the functionalities of the w3c recommendation
we start from the direct mapping and present a sourcetotarget semantics to equivalent shacl constraints on the rdf graph
et al which extends the functionalities of proposal
a sourcetotarget semantics preserving rewriting of constraints in an sql database schema
the direct mapping suggested by sequeda et al
we thus provide a shacl description of the rdf data
the rdf data generated by the direct mapping without the need to perform a costly validation of those constraints on the generated data
following the approach of we define the rewriting from sql constraints to shacl by a set of datalog rules
semantics preserving
our sourcetotarget rewriting of constraints
we prove that our sourcetotarget is semantics
we prove that our sourcetotarget is constraint preservingtraditional computer vision approaches based on neural networks
traditional computer vision approaches are typically trained on a large amount of image data
by minimizing the crossentropy loss between a given class label the neural networks visual embedding space are learned to fulfill a given task
by minimizing the crossentropy loss between a prediction the neural networks are learned to fulfill a given task
by minimizing the crossentropy loss between a given class label the neural networks are learned to fulfill a given task
by minimizing the crossentropy loss between a prediction the neural networks visual embedding space are learned to fulfill a given task
however due to the sole dependence on the image data distribution of the training domain these models tend to fail when applied to a target domain
a target domain that differs from these models source domain
the training using imagedatainvariant auxiliary knowledge
to learn a more robust neural networks to domain shifts we propose the knowledge graph neural network a neurosymbolic approach
a neurosymbolic approach that supervises the training
respective concepts relationships which is then transformed into a dense vector representation via an embedding method
the auxiliary knowledge is first encoded in a knowledge graph with respective concepts
the auxiliary knowledge is first encoded in a knowledge graph with respective concepts relationships
respective concepts which is then transformed into a dense vector representation via an embedding method
using a contrastive loss function neural network learns to adapt its visual embedding space
using a contrastive loss function neural network learns to its weights according to the imagedata invariant knowledge graph embedding space
classification using road sign recognition datasets from china
classification using the miniimagenet dataset
we evaluate neural network on visual transfer
visual transfer learning tasks for classification
classification using road sign recognition datasets from germany
classification using its derivatives
the results show that a visual model outperforms a model
particular when the domain gap increases
a visual model trained with a knowledge graph as a trainer
a model trained with crossentropy in all experiments in particular
besides stronger robustness to these neural network adapts to multiple datasets and classes without suffering heavily from catastrophic
besides better performance
catastrophic forgetting
besides stronger robustness to domain shifts to multiple datasets and classes without suffering heavily from catastrophicquestion answering in vague open domain information needs is hard to be adequate
question answering in complex open domain information needs is hard to be satisfying
question answering in vague open domain information needs is hard to be pleasing for end users
question answering in complex open domain information needs is hard to be pleasing for end users
question answering in complex open domain information needs is hard to be adequate
question answering in vague open domain information needs is hard to be satisfying
in this paper we investigate an approach
an approach where question answering complements a general purpose interactive keyword search system over rdf
question answering that involves a general purpose entity search service over entity enrichment through sparql
we evaluate a pipeline for question answering
we describe the role of question answering in that context
question answering that involves a general purpose entity search service over answer type prediction
question answering that involves a general purpose entity search service over pretrained neural models
question answering that involves a general purpose entity search service over rdf
we describe we detail
the fact that we start from a general purpose keyword search over rdf makes the proposed pipeline widely applicable and realistic in the sense that the proposed pipeline does not presuppose the availability of knowledge graphspecific training dataset
we evaluate various aspects of the proposed pipeline including the effect of answer type prediction
we evaluate various aspects of the proposed pipeline including the performance of question answering over existing benchmarks
the results show that even by using different data sources for training the proposed pipeline achieves a satisfactory performance
moreover we show that the ranking of entities for question answering can improve the entity rankingsemantic markup such as schemaorg allows providers on the web to describe content using a shared controlled vocabulary
this markup is invaluable in enabling a broad range of applications from vertical search engines to rich snippets in search results to actions on emails to many others
in this paper googles focus on semantic markup for datasets specifically in the context of developing a vertical search engine for datasets on the web search
in this paper googles focus on semantic markup for datasets specifically in the context of developing a vertical search engine for datasets on googles dataset search
googles dataset search relies on schemaorg to identify pages
pages that describe datasets
while schemaorg was the core googles also discovered that we need to address the following problem markup do not actually describe datasets
while schemaorg was the core googles also discovered that we need to address pages from 61 percent of internet hosts markup do not actually describe datasets
internet hosts that provide schemaorgdataset
the core enabling technology for this vertical search
we analyze the veracity of dataset markup for googles dataset searchs webscale corpus
we analyze the veracity of dataset markup for googles dataset categorize pages where this markup is not reliable
a deep neuralnetwork classifier that identifies a page with schemaorgdataset markup is a dataset page
we then propose a way to drastically increase the quality of the dataset metadata corpus by developing a deep neuralnetwork classifier
a deep neuralnetwork classifier that identifies whether
a deep neuralnetwork classifier achieves 967 percent recall at the 95 percent precision point
a deep neuralnetwork classifier that identifies a page with schemaorgdataset markup is a dataset page
a deep neuralnetwork classifier that identifies whether
this level of precision enables googles dataset search to use the metadata to provide high quality results to users
this level of precision enables googles dataset search to circumvent the noise in semantic markupconjunctive query  cq  answering over knowledge bases is an important reasoning task
however with expressive ontology languages such as owl query answering is computationally very expensive
the pagoda system addresses this issue by using a tractable reasoner to compute upperbound approximations
lower approximations falling back to a fullyfledged owl reasoner only when these bounds do not coincide
upperbound approximations falling back to a fullyfledged owl reasoner only when these bounds do not coincide
the pagoda system addresses this issue by using a tractable reasoner to compute lower approximations
the effectiveness of this approach critically depends on the quality of the approximations
an ontology language that subsumes all the owl 2 profiles while still maintaining tractability
in this paper we explore a technique for computing closer approximations via an ontology language
in this paper we explore a technique for computing closer approximations via rsa
we present a novel approximation of owl 2 ontologies into rsa
an algorithm to compute a closer  than pagoda  lower bound approximation using the rsa combined approach
we present a preliminary evaluation of we system
we have implemented these algorithms in a prototypical cq answering system
we system that shows significant performance improvements wrt pagodaoptimal censors maximizing answers while hiding data
we study information disclosure in description logic ontologies in the spirit of controlled query evaluation where query answering is filtered through optimal censors
data protected by a declarative policy
previous works have considered limited forms of policy typically constituted by conjunctive queries whose answer must never be inferred by a user
also existing implementations adopt approximated notions of censors returned to the users
censors that might result too restrictive in the practice in terms of the amount of nonprotected information
a secret that can be disclosed to a user
in this paper we enrich the framework by extending conjunctive queries in the policy with comparison predicates and introducing preferences between ontology predicates thus in principle augmenting the throughput of query answers
ontology predicates which can be exploited to decide the portion of a secret
we show that answering conjunctive queries in we framework is firstorder rewritable for safe policies complexity
we show that answering conjunctive queries in we framework is firstorder thus in ac0 in data complexity
we show that answering conjunctive queries in we framework is firstorder rewritable for extitdllitea ontologies complexity
we also present some experiments on a popular benchmark showing feasibility of we approach in a realworld scenario
we also present some experiments on a popular benchmark showing effectiveness of we approach in a realworld scenarioobjectrank is an essential tool to evaluate an importance of nodes for a userspecified query in heterogeneous graphs
however existing methods are not applicable to massive graphs because existing methods iteratively compute all nodes and edges
this paper proposes schemarank
schemarank which detects the exact topk important nodes for a given query within a short running time
unpromising nodes and edges ensuring that schemarank detects the same topk important nodes as objectrank
schemarank dynamically excludes unpromising nodes and edges
our extensive evaluations demonstrate that the running time of schemarank outperforms existing methods by up to two orders of magnitudevisual question answering is concerned with answering freeform questions about an image
since an image requires a deep linguistic understanding of the ability to associate an image with various objects an image requires multimodal reasoning from both computer vision
since an image requires a deep linguistic understanding of the ability to associate an image with various objects an image requires multimodal reasoning from natural language processing
since an image requires a deep linguistic understanding of the question to associate an image with various objects an image is an ambitious task
since an image requires a deep semantic understanding of the ability to associate an image with various objects an image requires multimodal reasoning from natural language processing
since an image requires a deep semantic understanding of the ability to associate an image with various objects an image is an ambitious task
since an image requires a deep semantic understanding of the ability to associate an image with various objects an image requires multimodal reasoning from both computer vision
since an image requires a deep linguistic understanding of the question to associate an image with various objects an image requires multimodal reasoning from natural language processing
since an image requires a deep semantic understanding of the question to associate an image with various objects an image is an ambitious task
various objects that are present in the image
since an image requires a deep semantic understanding of the question to associate an image with various objects an image requires multimodal reasoning from both computer vision
since an image requires a deep linguistic understanding of the ability to associate an image with various objects an image is an ambitious task
since an image requires a deep semantic understanding of the question to associate an image with various objects an image requires multimodal reasoning from natural language processing
since an image requires a deep linguistic understanding of the question to associate an image with various objects an image requires multimodal reasoning from both computer vision
we propose graphhopper a novel method
a novel method that approaches the task by integrating computer vision
a novel method that approaches the task by integrating natural language processing techniques
a novel method that approaches the task by integrating knowledge graph reasoning
concretely our method is based on performing contextdriven sequential reasoning based on the scene entities spatial relationships
concretely our method is based on performing contextdriven sequential reasoning based on the scene entities semantic relationships
concretely our method is based on performing contextdriven sequential reasoning based on the scene entities
a scene graph that describes their mutual relationships
as a first step we derive a scene graph
a scene graph that describes the objects in the image
a scene graph that describes their attributes
subsequently a reinforcement is trained to autonomously navigate in a multihop manner over the extracted scene graph to generate reasoning paths
reasoning paths which are the basis for deriving answers
a reinforcement learning agent
we conduct an experimental study on the challenging dataset gqa based on both automatically generated scene graphs
we conduct an experimental study on the challenging dataset gqa based on both manually curated
we results show that we keep up with human performance on manually curated scene graphs
moreover we find that graphhopper outperforms another stateoftheart scene graph reasoning model on both generated scene graphs by a significant margin
moreover we find that graphhopper outperforms another stateoftheart scene graph reasoning model on both manually curatedrelation is essential to enable question answering over knowledge bases
relation linking
the overall endtoend question answering performance
although there are various efforts to improve relation linking performance the current stateoftheart methods do not achieve optimal results therefore negatively impacting the overall endtoend question
in this work we propose a novel approach for relation
relation linking framing this work as a generative problem
a generative problem facilitating the use of pretrained sequencetosequence models
we extend such sequencetosequence models with the idea of infusing structured data from the target knowledge base primarily to enable such sequencetosequence models to handle the nuances of the knowledge base
a structured output consisting of a list of argumentrelation pairs enabling a knowledge validation step
moreover we train the model with the aim to generate a structured output
the existing relation linking systems on four different datasets
we compared we method against the existing relation
four different datasets derived from wikidata
four different datasets derived from dbpedia
a much simpler model that can be easily adapted to different knowledge bases
we method reports large improvements over the stateoftheart while using a much simpler modelthe use of external background knowledge can be beneficial for the task of matching ontologies automatically
the use of external background knowledge can be beneficial for the task of matching schemas automatically
in this paper we exploit six generalpurpose knowledge graphs as sources of background knowledge for the matching task
the background sources are evaluated by applying three different exploitation strategies
the actual background dataset on which the strategy is applied
we find that explicit strategies still outperform latent ones
we find that the choice of the strategy has a greater impact on the final alignment than the actual background dataset
while we could not identify a universally superior resource babelnet achieved consistently good results
we best matcher configuration with babelnet performs very competitively when compared to other matching systems even though no datasetspecific optimizations were madeunsupervised fact commonly combine scoring to predict the likelihood of assertions being true
unsupervised fact commonly combine path search to predict the likelihood of assertions being true
unsupervised fact checking approaches for knowledge graphs
the discrete search space spanned by the input knowledge graph
current approaches search for make no use of continuous representations of knowledge graphs
current approaches search for said metapaths in the discrete search space
we hypothesize that augmenting existing approaches with information from continuous knowledge graph representations has the potential to improve continuous knowledge graph representations performance
we approach esther searches for metapaths in compositional embedding spaces instead of the graph itself
assertions being true that can be exploited by existing fact checking approaches
by being able to explore longer metapaths longer metapaths can detect supplementary evidence for assertions
an ensemble learning setting
we evaluate esther by combining longer metapaths with 10 other approaches in an ensemble
we results agree with we hypothesis
we suggest that all other approaches can benefit from being combined with esther by 2065 percent aucroc on average
we code can be found at https
we code is opensource
we code can be found at githubcomdicegroupestherwhile rdf are by far the most popular logicbased languages for semantic web ontologies some welldesigned ontologies are only available in languages with a much richer expressivity such as the iso standard common logic
while rdf are by far the most popular logicbased languages for semantic web ontologies some welldesigned ontologies are only available in languages with a much richer expressivity such as firstorder logic standard common logic
while owl are by far the most popular logicbased languages for semantic web ontologies some welldesigned ontologies are only available in languages with a much richer expressivity such as the iso standard common logic
while owl are by far the most popular logicbased languages for semantic web ontologies some welldesigned ontologies are only available in languages with a much richer expressivity such as firstorder logic standard common logic
this inhibits reuse of some welldesigned ontologies by the wider semantic web community
while converting owl ontologies to firstorder logic is straightforward the reverse problem of finding the closest owl approximation of an firstorder logic ontology is undecidable
however for most a  good enough  owl approximation need not be perfect to enable wider reuse by the semantic web community
however for most practical purposes  good enough  owl approximation need not be perfect to enable wider reuse by the semantic web community
first normalizing firstorder logic sentences into a functionfree prenex conjunctive normal that strips away then applying a patternbased approach to identify common owl axioms
this paper outlines such a conversion approach by first
first normalizing firstorder logic sentences into a functionfree prenex conjunctive normal that strips away minor syntactic differences applying a patternbased approach to identify common owl axioms
this paper is tested on the over 2000 firstorder logic ontologies from the common logic ontology repositorywikidata as one of the largest open collaborative knowledge bases has drawn much attention from researchers since wikidata as one of the largest open collaborative knowledge bases launch in 2012
wikidata as one of the largest open collaborative knowledge bases has drawn much attention from practitioners since wikidata as one of the largest open collaborative knowledge bases launch in 2012
as wikidata as one of the largest open collaborative knowledge bases is collaboratively maintained by a community of a great number of volunteer editors understanding have not been studied extensively in previous works
as wikidata as one of the largest open collaborative knowledge bases is collaboratively predicting the departure dynamics of those editors are crucial
as wikidata as one of the largest open collaborative knowledge bases is collaboratively predicting the departure dynamics of those editors have not been studied extensively in previous works
as wikidata as one of the largest open collaborative knowledge bases is collaboratively developed by a community of a great number of volunteer editors understanding are crucial
as wikidata as one of the largest open collaborative knowledge bases is collaboratively developed by a community of a great number of volunteer editors understanding have not been studied extensively in previous works
as wikidata as one of the largest open collaborative knowledge bases is collaboratively maintained by a community of a great number of volunteer editors understanding are crucial
our classification model which has not been explored in a similar context and problem for predicting whether a wikidata as one of the largest open collaborative knowledge bases editor will leave the platform
in this paper we investigate the synergistic effect of two different types of features patternbased ones with deepfm as our classification model
our classification model which has not been explored in a similar context and problem for predicting whether a wikidata as one of the largest open collaborative knowledge bases editor will stay the platform
in this paper we investigate the synergistic effect of two different types of features statistical ones with deepfm as our classification model
our experimental results show that using the two sets of features with deepfm achieves substantial improvement compared to using either of the sets of features and over a wide range of baselines
our experimental results show that using the two sets of features with deepfm provides the best performance regarding f1 score
our experimental results show that using the two sets of features with deepfm provides the best performance regarding aurocsupervised entity resolution methods rely on labeled record pairs for learning matching patterns between more data sources
supervised entity resolution methods rely on labeled record pairs for learning matching patterns between two data sources
active learning minimizes the labeling effort by selecting informative pairs for labeling
signals that only exist
the existing active learning methods for entity resolution all target twosource and ignore signals in multisource settings such as the web of data
all target twosource matching scenarios
in this paper we propose a graphboosted active learning method for multisource entity resolution
in this paper we propose almser 
the first active learningbased entity resolution method that is especially tailored to the multisource setting
to the best of our knowledge almser is the first active learningbased entity resolution method
the rich correspondence graph that exists in multisource settings for selecting informative record pairs
almser exploits the rich correspondence graph
in addition the correspondence graph is used to derive complementary training data
multisource matching tasks having different profiling characteristics
method using five multisource matching tasks
we evaluate we method
the experimental evaluation shows that leveraging graph signals leads to improved results over active learning methods
active learning methods using marginbased query strategies in terms of f1 score on all tasks
active learning methods using committeebased query strategies in terms of f1 score on all tasksthe concepts in knowledge graphs thus play an indispensable role in many applications
the concepts in knowledge graphs enable machines to understand natural language
however existing the concepts in knowledge graphs have the poor coverage of concepts especially finegrained concepts
in order to supply existing the concepts in knowledge graphs with more finegrained concepts we propose a novel concept extraction framework namely mrcce to extract largescale multigranular concepts from the descriptive texts of entities
in order to supply existing the concepts in knowledge graphs with more new concepts we propose a novel concept extraction framework namely mrcce to extract largescale multigranular concepts from the descriptive texts of entities
bert which can extract more finegrained concepts with a pointer network
specifically mrcce is built with a machine reading comprehension model based on bert
furthermore a random forest are also adopted to enhance mrcces precision and recall simultaneously
furthermore rulebased pruning are also adopted to enhance mrcces precision and recall simultaneously
our experiments evaluated upon multilingual the concepts in knowledge graphs
english probase justify mrcces superiority over the stateoftheart extraction models in the concepts in knowledge graph completion
chinese cndbpedia justify mrcces superiority over the stateoftheart extraction models in the concepts in knowledge graph completion
our experiments justify mrcces superiority over the stateoftheart extraction models in the concepts in knowledge graph completion
particularly after running mrcce for each entity in cndbpedia more than 7053900 new concepts  are supplied into the the concepts in knowledge graph
particularly after running mrcce for each entity in cndbpedia instanceof relations  are supplied into the the concepts in knowledge graph
the code have been released at https 
the code have been released at githubcomfcihraeipnusnacwhmrcce
datasets have been released at https 
datasets have been released at githubcomfcihraeipnusnacwhmrcceopenstreetmap is one of the richest openly available sources of volunteered geographic information
although openstreetmap includes various geographical entities various geographical entities descriptions do not follow any welldefined ontology
although openstreetmap includes various geographical entities various geographical entities descriptions are highly heterogeneous incomplete
knowledge graphs can potentially provide valuable semantic information to enrich openstreetmap entities
however interlinking openstreetmap entities with knowledge graphs is inherently difficult due to the large heterogeneous ambiguous and flat openstreetmap schema and the annotation sparsity
this paper tackles the alignment of openstreetmap tags with the corresponding knowledge graph classes holistically by jointly considering the instance layers
this paper tackles the alignment of openstreetmap tags with the corresponding knowledge graph classes holistically by jointly considering the schema layers
we propose a novel neural architecture
tagtoclass alignment created using linked entities in openstreetmap
a novel neural architecture that capitalizes upon a shared latent space for tagtoclass alignment
tagtoclass alignment created using linked entities in knowledge graphs
we experiments aligning openstreetmap datasets for several countries with two of the most prominent openly available knowledge graphs namely dbpedia demonstrate that the proposed approach outperforms the stateoftheart schema alignment baselines by up to 37 percent points f1score
we experiments aligning openstreetmap datasets for several countries with two of the most prominent openly available knowledge graphs namely wikidata demonstrate that the proposed approach outperforms the stateoftheart schema alignment baselines by up to 37 percent points f1score
we experiments
the resulting alignment facilitates new semantic annotations for over 10 million openstreetmap entities worldwide
10 million openstreetmap entities worldwide which is over a 400 percent increase compared to the existing annotationsthere are these tables
these tables cover many domains
these tables contain useful information
to make use of these tables for data integration we need precise descriptions of the concepts and relationships in the data
the data known as semantic descriptions
to make use of these tables for data discovery we need precise descriptions of the concepts and relationships in the data
however creating semantic descriptions can be error prone
however creating semantic descriptions is a complex process requiring considerable manual effort
in this paper we present a novel probabilistic approach for automatically building semantic descriptions of wikipedia tables
then the table uses collective inference to distinguish spurious relationships to form the final semantic description
we approach leverages hyperlinks in existing knowledge in wikidata to construct a graph of possible relationships in the table
we approach leverages hyperlinks in a wikipedia table in wikidata to construct a graph of possible relationships in the table context
we approach leverages hyperlinks in a wikipedia table in wikidata to construct a graph of possible relationships in the table
we approach leverages hyperlinks in existing knowledge in wikidata to construct a graph of possible relationships in the table context
then the table uses collective inference to distinguish genuine relationships to form the final semantic description
tables that require complex semantic descriptions of the population of a country in a particular year  to describe the data accurately
tables that require complex semantic descriptions of nary relations  to describe the data accurately
in contrast to existing methods our solution can handle tables
tables that require complex semantic descriptions of implicit contextual values to describe the data accurately
tables that require complex semantic descriptions of to describe the data accurately
in we empirical evaluation we approach outperforms stateoftheart systems on the semtab2020 dataset
in we empirical evaluation we approach outperforms stateoftheart systems by as much as 28 percent in f1 score on a large set of wikipedia tablesknowledge base question answering aims at automatically answering factoid questions over knowledge bases
type linking
for complex questions that require multiple knowledge base relations knowledge base question and query composition
for complex questions that require constraints knowledge base question many challenges and query composition
knowledge base question answering faces
relation linking
question understanding entity 
for complex questions that require constraints knowledge base question and query composition
many challenges
entity linking
question understanding type 
component linking
for complex questions that require multiple knowledge base relations knowledge base question many challenges and query composition
question understanding component 
question understanding relation 
a novel graph structure called entity description graph to represent the structure of complex questions
complex questions which can help alleviate the above issues
in this paper we propose a novel graph structure
dbpedia called edgqa
by leveraging the entity description graph structure of given questions we implement a qa system over dbpedia
extensive experiments demonstrate that edgbased decomposition is a feasible way for complex question answering over knowledge bases
extensive experiments demonstrate that edgqa outperforms stateoftheart results on both lcquad and qald9
