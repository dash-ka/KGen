in this paper we propose a general purpose recursion operator to develop algorithms for evaluating sparql in practical scenarios
in this paper we propose a general purpose recursion operator to be added to sparql
in this paper we propose a general purpose recursion operator to formalize sparql syntax
we also show how to implement recursion as a plugin on top of existing systems
we also show how to test a plugin performance on several real world datasetsthe original sparql proposal was often criticized for the original sparql proposal inability to navigate through rdf
for this reason property paths were introduced in sparql 11
no theoretical studies examining how property paths addition to the language affects main computational tasks such as query subsumption
no theoretical studies examining how property paths addition to the language affects main computational tasks such as query evaluation
no theoretical studies examining how property paths addition to the language affects main computational tasks such as query containment
up to date there are no theoretical studies
in this paper we show that although the addition of property paths has no impact on query evaluation property paths do make the containment problems substantially more difficult
in this paper we show that although the addition of property paths has no impact on query evaluation property paths do make the subsumption problems substantially more difficult
in this paper we tackle all of these problemssemantic relatedness are fundamental problems for linking text documents to the web of data
disambiguation are fundamental problems for linking text documents to the web of data
many approaches dealing with both problems rely on word distribution over wikipedia
many approaches dealing with both problems rely on concept distribution over wikipedia
there are many approaches
many approaches dealing with both most of both problems rely on concept distribution over wikipedia
many approaches dealing with both most of both problems rely on word distribution over wikipedia
concepts that do not have a rich textual description
both problems are therefore not applicable to concepts
in this paper we show that semantic relatedness can also be accurately computed by analysing only the graph structure of the knowledge base
wordsense disambiguation that makes use of graphbased relatedness
entity disambiguation that makes use of graphbased relatedness
in addition we propose a joint approach to wordsense disambiguation
in addition we propose a joint approach to entity disambiguation
as opposed to the majority of stateoftheart systems entities we use we approach to disambiguate both common nouns
stateoftheart systems that target mainly named
as opposed to the majority of stateoftheart systems entities we use we approach to disambiguate both entities
in we experiments we first validate we relatedness measure on multiple knowledge bases
in we experiments we first show that our relatedness measure performs better than related stateoftheart graph
related stateoftheart graph based measures
in we experiments we first validate we relatedness measure on ground truth datasets
afterwards we show that the disambiguation algorithm also achieves superior disambiguation accuracy with respect to alternative stateoftheart graphbased algorithms
afterwards we evaluate the disambiguation algorithminstance retrieval computes all instances of a given concept in dl  ontology
instance retrieval computes all instances of a given concept in a consistent description logic  ontology
although instance retrieval is a popular task for ontology reasoning there is no scalable method for instance retrieval for negated concepts by now
this paper studies a new approach to instance retrieval for negated concepts
this paper studies a new approach to instance retrieval for negated concepts based on query rewriting
the iforewritable class called iforewritable  class is identified
the iforewritable class called the inconsistencybased firstorder rewritable  class is identified
the iforewritable class guarantees that instance retrieval for an atomic negation can be reduced to answering a disjunction of conjunctive queries  cqs  over the abox
the iforewritable class is more expressive than the firstorder rewritable class
the firstorder rewritable class which guarantees that answering a cq is reducible to answering a disjunction of cqs over the abox regardless of the tbox
two sufficient conditions are proposed to detect iforewritable ontologies
iforewritable ontologies that are not firstorder rewritable
a rewritingbased method for retrieving instances of a negated concept is proposed for iforewritable ontologies
existing methods implemented in stateoftheart dl systems
preliminary experimental results on retrieving instances of all atomic negations show that a rewritingbased method for retrieving instances of a negated concept is significantly more efficient than existing methodsknowledge graphs are knowledge discovery activities
knowledge graphs are browsing
knowledge graphs are a key ingredient for searching
motivated by the need to harness knowledge available in a variety of knowledge graphs we face the following two problems
entities defined in some knowledge graph relatedness
first given a pair of entities defined in some knowledge graph find an explanation of a pair of entities
we formalize the notion of relatedness explanation
entities defined in some knowledge graph combinations
we introduce different criteria to build explanations based on diversity
we introduce different criteria to build explanations based on informationtheory
we introduce different criteria to build explanations based on a pair of entities
second given a pair of entities find pairs of  entities
second given a pair of entities find other  entities
pairs of  entities sharing a similar relatedness perspective
other  entities sharing a similar relatedness perspective
a tool called recap
recap which is based on rdf
recap which is based on sparql
we describe an implementation of we ideas in a tool
we provide an evaluation of recap
we provide a comparison with related systems on realworld datalevesques proper knowledge bases  correspond to infinite sets of ground negative facts with the notable property that for fol formulas in a certain normal form entailment reduces to formula evaluation
a certain normal form which includes positive queries possibly extended with a controlled form of negation
a certain normal form which includes conjunctive queries
levesques proper knowledge bases  correspond to infinite sets of ground positive facts with the notable property that for fol formulas in a certain normal form entailment reduces to formula evaluation
proper kbs  correspond to infinite sets of ground positive facts with the notable property that for fol formulas in a certain normal form entailment reduces to formula evaluation
proper kbs  correspond to infinite sets of ground negative facts with the notable property that for fol formulas in a certain normal form entailment reduces to formula evaluation
however proper kbs represent extensional knowledge only
in description logic terms proper kbs correspond to aboxes
in this paper we augment proper kbs with dllite tboxes expressing the ontology of the domain 
in this paper we augment proper kbs with dllite tboxes expressing of the domain 
in this paper we augment proper kbs with dllite tboxes expressing intensional knowledge 
dllite has the notable property that conjunctive query answering over tboxes is reducible to formula evaluation over the abox only
dllite has the notable property that conjunctive query answering over standard description logic aboxes is reducible to formula evaluation over the abox only
aboxes consisting of proper kbs
here we investigate whether such a property extends to aboxes
specifically we consider two dllite variants
specifically we consider extitdllite  core roughly corresponding to owl 2 ql
specifically we consider extitdllite  rdfs
specifically we consider roughly corresponding to rdfs
we show that when a extitdllite  rdfs tbox is coupled with a proper kb a extitdllite  rdfs tbox can be compiled away reducing query answering to evaluation on the proper kb alone
but this reduction is no longer possible when we associate proper kbs with extitdllite  core tboxes
indeed we show that in the latter case query answering even for conjunctive queries becomes conphard in data complexityentity navigation over linked data often follows semantic links by using linked data browsers
with the increasing volume of linked data the diverse links make it difficult for users to find target entities
with the increasing volume of linked data the rich links make it difficult for users to traverse the link graph
with the increasing volume of linked data the diverse links make it difficult for users to traverse the link graph
with the increasing volume of linked data the rich links make it difficult for users to find target entities
besides there is a necessity for navigation paradigm to take into account
besides there is entitysetoriented transition
besides there is singleentityoriented transition
to facilitate entity navigation we propose a novel concept and introduce link pattern lattice to organize semantic links when browsing an entity
a novel concept called link pattern
to facilitate entity navigation we propose a novel concept and introduce link pattern lattice to organize semantic links when browsing a set of entities
furthermore to help users quickly find target entities topk link patterns are selected for entity navigation
the proposed approach is implemented in a prototype system
the proposed approach is then compared with two linked data browsers via a user study
experimental results show that the proposed approach is effectivewe introduce optimization techniques for reasoning in dln a recently introduced family of nonmonotonic description logics whose characterizing features appear wellsuited to model the examples naturally arising in biomedical domains
we introduce optimization techniques for reasoning in dln a recently introduced family of nonmonotonic description logics whose characterizing features appear wellsuited to model the examples naturally arising in semantic web access control policies
such optimizations are validated experimentally on large kbs with more than 30k axioms
speedups exceed 1 order of magnitude
for the first time response times compatible with realtime reasoning are obtained with nonmonotonic kbs of this sizeone of the main challenges in the data web is the identification of instances
instances that refer to the same realworld entity
current instance matching benchmarks
the necessary insights pertaining to how current frameworks behave when dealing with real data
choosing the right framework for this purpose remains tedious as current instance fail to provide end users and developers with the necessary insights
benchmark generator which focuses on benchmarking instance matching systems for linked data
a domainindependent instance matching benchmark generator
in this paper we present a domainindependent instance
in this paper we present lance
the standard test cases related to structure transformations
lance is the first linked data benchmark generator to support the standard test cases
the standard test cases related to value transformations
complex semanticsaware test cases that take into account expressive owl constructs
lance is the first linked data benchmark generator to support complex semanticsaware test cases
lance supports the definition of matching tasks with varying degrees of difficulty
lance produces a weighted gold standard
a weighted gold standard which allows a more finegrained analysis of the performance of instance matching tools
the data web accompanying schema as input to produce a target dataset
the data web can accept the data web
a target dataset implementing test cases of varying levels of difficulty
the data web can accept any linked dataset
we provide a comparative analysis with lance benchmarks to identify the capabilities of state of the art instance matching an evaluation to demonstrate the scalability of lances test case generator
we provide a comparative analysis with lance benchmarks to assess the capabilities of state of the art instance matching systems to demonstrate the scalability of lances test case generator
we provide a comparative analysis with lance benchmarks to identify the capabilities of state of the art instance matching systems to demonstrate the scalability of lances test case generator
we provide a comparative analysis with lance benchmarks to assess the capabilities of state of the art instance matching an evaluation to demonstrate the scalability of lances test case generatoras the web of data is growing steadily the demand for userfriendly means for exploring visualizing linked data is also increasing
as the web of data is growing steadily the demand for userfriendly means for exploring analyzing linked data is also increasing
the key challenge for visualizing linked data consists in providing a clear overview of linked data
the key challenge for visualizing linked data consists in supporting nontechnical users in finding suitable visualizations while hiding technical details of visualization configuration
the key challenge for visualizing linked data consists in supporting nontechnical users in finding suitable visualizations while hiding technical details of linked data
in order to accomplish this we propose a largely automatic workflow which guides users through the process of creating visualizations by automatically categorizing
in order to accomplish this we propose a largely automatic workflow which guides users through binding data to visualization parameters
a comprehensive visualization model facilitating the automatic binding between data
the approach is based on a comprehensive visualization model
a comprehensive visualization model facilitating the automatic binding between visualization parameters
the approach is based on a heuristic analysis of the structure of the input data
the resulting assignments are presented to the user
the resulting assignments are ranked to the user
with linkdaviz we provide a webbased implementation of the approach
with linkdaviz we demonstrate the feasibility by an extended user
with linkdaviz we demonstrate the feasibility by performance evaluationmany lod datasets such as dbpedia are process large amounts of requests from diverse applications
many lod datasets such as linkedgeodata are voluminous
many lod datasets such as dbpedia are voluminous
many lod datasets such as linkedgeodata are process large amounts of requests from diverse applications
many data products rely on partial local lod replications to ensure faster querying
many data products rely on full local lod replications to ensure faster processing
services rely on full local lod replications to ensure faster processing
services rely on partial local lod replications to ensure faster querying
many data products rely on full local lod replications to ensure faster querying
services rely on partial local lod replications to ensure faster processing
many data products rely on partial local lod replications to ensure faster processing
services rely on full local lod replications to ensure faster querying
given the evolving nature of the authoritative datasets to ensure consistent replicas frequent replacements are required at a great cost
given the evolving nature of the authoritative datasets to ensure uptodate replicas frequent replacements are required at a great cost
given the evolving nature of the original datasets to ensure consistent replicas frequent replacements are required at a great cost
given the evolving nature of the original datasets to ensure uptodate replicas frequent replacements are required at a great cost
propagation which propagates only interesting parts of updates from the source to the target dataset
in this paper we introduce an approach for interestbased rdf update propagation
effectively this enables remote applications to  subscribe  to relevant datasets and consistently reflect the necessary changes locally without the need to frequently replace the entire dataset  or a relevant subset 
graphpatternbased interest expressions that is used to filter interesting parts of updates from the source
we approach is based on a formal definition for graphpatternbased interest expressions
we implement the approach in the irap framework
we perform a comprehensive evaluation based on dbpedia live updates to confirm the validity
value of we approacha lowcost serverside interface when high numbers of clients need to evaluate sparql queries
recently triple pattern fragments were introduced as a lowcost serverside interface
scalability is achieved by moving part of the query execution to the client at the cost of elevated query times
since the triple pattern fragments interface purposely does not support complex constructs such as sparql filters queries need to be executed mostly on the client resulting in long execution times
queries that use sparql filters
we therefore investigated the impact of adding a literal substring matching feature to the triple pattern fragments interface with the goal of improving query performance while maintaining low server cost
in this paper we discuss the clientserver setup
in this paper we compare the performance of sparql queries on multiple implementations including caseinsensitive fmindex
in this paper we compare the performance of sparql queries on multiple implementations including elastic search
we evaluations indicate that these improvements allow for faster query execution without significantly increasing the load on the server
offering the substring feature on triple pattern fragment servers allows users to obtain faster responses for filterbased sparql queries
furthermore substring matching can be used to support other filters such as complete regular expressions
furthermore substring matching can be used to support other filters such as range queriesnohr allows the user to express exceptions
nohr allows the user to query the combined knowledge base
nohr allows the user to combine an owl 2 el ontology with a set of nonmonotonic  rules suitable
nohr allows the user to express defaults
nohr allows the user to combine an owl 2 el ontology with a set of logic programming  rules suitable
the formal approach realized in nohr has been shown that even very large health care ontologies such as snomed ct can be handled
the formal approach realized in nohr is wrt data complexity 
the formal approach realized in nohr is polynomial 
as each of the tractable owl profiles is motivated by different application cases extending the tool to the other profiles is of particular interest also because the other profiles preserve the polynomial data complexity of the combined formalism
yet a straightforward adaptation of the existing approach to owl 2 ql turns out to not be viable
in this paper we provide the nontrivial solution for the extension of nohr to owl 2 ql by directly translating the ontology into rules without any prior classification
we have implemented we approach
we evaluation shows encouraging resultsdeclarative mappings that connect an ontology
ontologybased data access is a recent paradigm for accessing data sources through an ontology
an ontology that acts as a conceptual integrated view of the data
an ontology that acts as a conceptual integrated view of the data to the data sources
ontologybased data access is a recent paradigm for accessing data sources through declarative mappings
we study the formal analysis of mappings in ontologybased data access
specifically we focus on the problem of identifying two of the most important anomalies for mappings in ontologybased data access
specifically we focus on the problem of identifying mapping inconsistency and redundancy 
we consider a wide range of ontology languages
ontology languages that comprises owl 2
ontology languages that comprises all owl 2 profiles
ontology languages that examine mapping languages of different expressiveness over relational databases
the decision problems associated with mapping inconsistency and redundancy
we establish tight complexity bounds for the decision problems
we provide algorithms
we results prove that in we general framework such forms of mapping analysis enjoy nice computational properties in the sense that such forms of mapping analysis are not harder than standard reasoning tasks over an ontology
an ontology that acts as a conceptual integrated view of the data
an ontology that acts over the relational database schemacreating ontologies requires significant expertise and effort
ontologies are complex intellectual artifacts
while methodologies propose ways of building ontologies in a normative way empirical investigations of how experts actually construct ontologies  in the wild  are rare
while existing ontologyediting tools propose ways of building ontologies in a normative way empirical investigations of how experts actually construct ontologies  in the wild  are rare
yet understanding actual user behavior can play an important role in the design of effective tool support
although previous empirical investigations have produced a series of interesting insights previous empirical investigations were aimed at gauging the problem space only
although previous empirical investigations have produced a series of interesting insights previous empirical investigations were exploratory in nature
in this work we aim to advance the state of knowledge in this domain by systematically defining a set of hypotheses about how users edit ontologies
in this work we aim to advance the state of knowledge in this domain by systematically comparing a set of hypotheses about how users edit ontologies
towards that end we study the user editing trails of four realworld ontologyengineering projects
a coherent research framework called hyptrails
using a coherent research framework we derive formal definitions of hypotheses from the literature and systematically compare previous empirical investigations with each other
our findings suggest that the hierarchical structure of an ontology exercises the strongest influence on user editing behavior
user editing behavior followed by the entity similarity
our findings suggest that the hierarchical structure of an ontology exercises the strongest influence on the semantic distance of classes in the ontology
moreover our findings are strikingly consistent across all ontologyengineering projects in our study with only minor exceptions for one of the smaller datasets
our believe that our results are important for project managers
our believe that our results are important for ontology tools builders
user interfaces and processes that better support the observed editing patterns of users
project managers who can potentially leverage this information to create user interfaces and processesit has been shown both empirically that performing core reasoning tasks on large ontologies in owl 2 is timeconsuming
it has been shown both theoretically that performing core reasoning tasks on large ontologies in owl 1 is timeconsuming
it has been shown both empirically that performing core reasoning tasks on large ontologies in owl 1 is resourceintensive
it has been shown both empirically that performing core reasoning tasks on expressive ontologies in owl 1 is timeconsuming
it has been shown both theoretically that performing core reasoning tasks on expressive ontologies in owl 1 is resourceintensive
it has been shown both empirically that performing core reasoning tasks on large ontologies in owl 2 is resourceintensive
it has been shown both theoretically that performing core reasoning tasks on large ontologies in owl 1 is resourceintensive
it has been shown both theoretically that performing core reasoning tasks on expressive ontologies in owl 2 is timeconsuming
it has been shown both theoretically that performing core reasoning tasks on large ontologies in owl 2 is timeconsuming
it has been shown both theoretically that performing core reasoning tasks on expressive ontologies in owl 2 is resourceintensive
it has been shown both theoretically that performing core reasoning tasks on expressive ontologies in owl 1 is timeconsuming
it has been shown both empirically that performing core reasoning tasks on large ontologies in owl 1 is timeconsuming
it has been shown both empirically that performing core reasoning tasks on expressive ontologies in owl 1 is resourceintensive
it has been shown both empirically that performing core reasoning tasks on expressive ontologies in owl 2 is timeconsuming
it has been shown both theoretically that performing core reasoning tasks on large ontologies in owl 2 is resourceintensive
it has been shown both empirically that performing core reasoning tasks on expressive ontologies in owl 2 is resourceintensive
the different reasoning algorithms employed
moreover due to optimisation techniques each reasoner may be efficient for ontologies with different characteristics
moreover due to the different reasoning algorithms each reasoner may be efficient for ontologies with different characteristics
optimisation techniques employed
in this paper we present r  2o  2 a metareasoner and selects from a number of stateoftheart owl 2 dl reasoners to achieve high efficiency making use of ranking models
in this paper we present r  2o  2 ranks and selects from a number of stateoftheart owl 2 dl reasoners to achieve high efficiency making use of ranking models
a metareasoner that automatically combines
in this paper we present r  2o  2 ranks and selects from a number of stateoftheart owl 2 dl reasoners to achieve high efficiency making use of performance prediction models
in this paper we present r  2o  2 a metareasoner and selects from a number of stateoftheart owl 2 dl reasoners to achieve high efficiency making use of performance prediction models
we comprehensive evaluation on a large ontology corpus shows that r  2o  2 consistently outperforms 6 stateoftheart owl 2 dl reasoners on average performance with an average speedup of up to 14x
we comprehensive evaluation on a large ontology corpus shows that r  2o  2 significantly outperforms 6 stateoftheart owl 2 dl reasoners on average performance with an average speedup of up to 14x
r  2o  2 also shows a 14
r  2o  2 also speedup over konclude
r  2o  2 also speedup over the current dominant owl 2 dl reasonerin particular of linked data has led to a diverse landscape of datasets
datasets which make entity retrieval a challenging task
the increasing amount of data on the web particular of linked data has led to a diverse landscape of datasets
explicit crossdataset links for instance to indicate related entities can significantly improve entity retrieval
explicit crossdataset links for instance to indicate coreferences can significantly improve entity retrieval
however only a small fraction of entities are interlinked through explicit statements
in this paper we propose a twofold entity retrieval approach
in offline preprocessing step we cluster entities based on the xmeans
in offline preprocessing step we cluster entities based on spectral clustering algorithms
in a first  we cluster entities based on spectral clustering algorithms
in a first  we cluster entities based on the xmeans
retrieval model which takes advantage of we
an optimized retrieval model
in the second step we propose an precomputed clusters
the result set with relevant entities by considering features of entities
the result set with relevant entities by considering features of the queries
for a given set of entities we further expand the result
entities retrieved by a given user query
the result set with relevant entities by considering features of the precomputed clusters
entities retrieved by the bm25f retrieval approach
finally we rerank the expanded result
the expanded result set with respect to the relevance to the query
we perform a thorough experimental evaluation on the billions triple challenge  dataset
we perform a thorough experimental evaluation on btc12  dataset
the bm25f retrieval approach shows significant improvements compared to the baseline and state of the art approachesbenchmarking is indispensable when aiming to assess technologies with respect to benchmarking suitability for given tasks
while benchmark generation frameworks have been developed to evaluate triple stores benchmarking mostly provide a onefitsall solution to the benchmarking problem
while several benchmarks have been developed to evaluate triple stores benchmarking mostly provide a onefitsall solution to the benchmarking problem
this approach to benchmarking is however unsuitable to evaluate the performance of a triple store for a given application with particular requirements
we address this drawback by presenting an automatic approach for the generation of benchmarks out of the query history of applications
we address this drawback by presenting feasible
we address this drawback by presenting query logs
the generation of benchmarks out of the query history of applications is achieved by selecting prototypical queries of a userdefined size from the input set of queries
we evaluate we approach on two query logs
we evaluate we show that the benchmarks our approach generates are accurate approximations of the input query logs
we show that four different triple stores behave differently based on the data four different triple stores contain
we compare four different triple stores with benchmarks
moreover
the types of queries posed
benchmarks generated using our approach
our results suggest that feasible generates better sample queries than the state of the art
in addition the better query selection used lead to triple store rankings
triple store rankings which partly differ from the rankings
in addition the larger set of query types used lead to triple store rankings
the rankings generated by previous workslarge knowledge bases such as dbpedia are most often created heuristically due to scalability issues
in the building process both random errors may occur
in the building process both systematic errors may occur
in this paper we focus on finding systematic errors in dbpedia
in this paper we focus on antipatterns in dbpedia
we show that by combining clustering of the reasoning results errors can be identified at a minimal workload for the knowledge base designer
we show that by aligning the dbpedia ontology to the foundational ontology dolcezero errors can be identified at a minimal workload for the knowledge base designer
we show that by combining reasoning of the reasoning results errors can be identified at a minimal workload for the knowledge base designer
errors affecting millions of statementswe tackle the problem of resolving coreferences in textual content by leveraging semantic web techniques
potential semantic annotations that can be added to the identified mentions
specifically we focus on noun phrases the challenge in this context is to improve the coreference resolution by leveraging potential semantic annotations
identifiable entities that appear in the text
noun phrases that coreference identifiable entities
we system sanaphor first applies stateoftheart techniques to extract candidate coreferences
we system sanaphor first applies stateoftheart techniques to extract entities
we system sanaphor first applies stateoftheart techniques to extract noun phrases
an inverted index built on top of a knowledge graph 
an inverted index built on top of dbpedia 
then we propose an approach to type noun phrases
type noun phrases using an inverted index
merging coreference clusters
finally we use the semantic relatedness of the introduced types to improve the stateoftheart techniques by splitting
we evaluate sanaphor on conll datasets
we show how we techniques consistently improve the state of the art in coreference resolutionthe web of linked data is composed of tons of rdf documents
rdf documents interlinked to each other forming a huge repository of distributed semantic data
effectively querying this distributed data source is an important open problem in the semantic web area
in this paper we propose a declarative language to query linked data on the web
in this paper we propose ldql 
separately patterns that describe web navigation paths
one of the novelties of ldql is that one of the novelties of ldql expresses separately patterns
separately patterns that describe the expected query result
web navigation paths that select the data sources to be used for computing the result
we study the expressiveness of the language
we present a formal syntax and semantics prove equivalence rules
the query formalisms that have been proposed previously for linked data on the web
in particular we show that ldql is strictly more expressive than the query formalisms
queries for which a complete execution is not computationally feasible over the web
the high expressiveness allows ldql to define queries
queries satisfying a syntactic sufficient condition
we formally study this issue queries are ensured to have a procedure to be effectively evaluated over the web of linked data
we formally provide a syntactic sufficient condition to avoid this problem queries are ensured to have a procedure to be effectively evaluated over the web of linked datafederated query engines provide a unified query interface to federations of sparql endpoints
replicating data fragments from different linked data sources facilitates data reorganization to better fit federated query processing needs of data consumers
however existing federated query engines are not replicated data can negatively impact existing federated query engines are not replicated data performance
however existing federated query engines are not designed to support replication are not replicated data performance
however existing federated query engines are not designed to support replication are not designed to support replication
however existing federated query engines are not replicated data can negatively impact existing federated query engines are not designed to support replication
in this paper we formulate the source selection problem with fragment replication
for a given set of endpoints with a sparql query the source selection problem with the source selection problem with fragment replication is to select the endpoints
the endpoints that minimize the number of tuples to be transferred
for a given set of endpoints with replicated fragments the source selection problem with fragment replication is to select the endpoints
for a given set of endpoints with replicated fragments the source selection problem with the source selection problem with fragment replication is to select the endpoints
for a given set of endpoints with a sparql query the source selection problem with fragment replication is to select the endpoints
we devise the fedra source selection algorithm
the fedra source selection algorithm that approximates the source selection problem with fragment replication
the stateoftheart federated query engines anapsid and empirically evaluate we performance
the stateoftheart federated query engines fedx and empirically evaluate we performance
we implement fedra in the stateoftheart federated query engines
experimental results suggest that fedra in the stateoftheart federated query engines fedx efficiently solves the source selection problem with fragment replication reducing the number of selected sparql endpoints
experimental results suggest that fedra in the stateoftheart federated query engines anapsid efficiently solves the source selection problem with fragment replication reducing the number of selected sparql endpoints
experimental results suggest that fedra in the stateoftheart federated query engines anapsid efficiently solves the source selection problem with fragment replication reducing the size of query intermediate results
experimental results suggest that fedra in the stateoftheart federated query engines fedx efficiently solves the source selection problem with fragment replication reducing the size of query intermediate resultsbetween the sparql protocol lies a largely unexplored axis of possible interfaces to each with the sparql protocol own combination of tradeoffs
between uri dereferencing lies a largely unexplored axis of possible interfaces to each with the sparql protocol own combination of tradeoffs
between uri dereferencing lies a largely unexplored axis of possible interfaces to linked data own combination of tradeoffs
between the sparql protocol lies a largely unexplored axis of possible interfaces to linked data own combination of tradeoffs
triple pattern fragments which allows clients to execute sparql queries against lowcost servers at the cost of higher bandwidth
one of these interfaces is triple pattern fragments
requests which can among others be achieved through additional metadata in responses
increasing a clients efficiency means lowering the number of requests
we noted that typical sparql query evaluations against triple pattern fragments require a significant portion of membership subqueries
we noted that typical sparql query evaluations against triple pattern fragments require a variable pattern
membership subqueries which check the presence of a specific triple
one of these interfaces studies bloom filters as extra metadata
one of these interfaces studies the impact of providing approximate membership functions as extra metadata
one of these interfaces studies golombcoded sets as extra metadata
in addition to reducing http requests such functions allow to achieve full result recall earlier when temporarily allowing lower precision
half of the tested queries from a watdiv benchmark test set could be executed with up to a third fewer http requests with only marginally higher server cost
query times however did not improve likely due to slower metadata generation and transfer
this indicates that approximate membership functions can partly improve the clientside query process with minimal impact on the server
this indicates that approximate membership functions can partly improve the clientside query process with minimal impact on the server interfacein this paper we tackle the problem of answering sparql queries over virtually integrated databases
explicit information is available about which records in the different databases refer to the same real world entity
we assume that the entity resolution problem has already been solved
surprisingly to the best of our knowledge there has been no attempt to extend the standard ontologybased data access
the standard ontologybased data access setting to take into account these db links for consistency checking
the standard ontologybased data access setting to take into account these db links for sparql queryanswering
this is partly because the owl builtin owl the most natural representation of links between data sets is not included in the de facto ontology language for the standard ontologybased data access
this is partly because the owl builtin owl the most natural representation of links between data sets is not included in owl 2 ql 
this is partly because the owl builtin owl sameas property  is not included in the de facto ontology language for the standard ontologybased data access
this is partly because the owl builtin owl sameas property  is not included in owl 2 ql 
our formally treat several fundamental questions in this context how links over database identifiers can be represented in terms of owl sameas statements how to check consistency
our formally treat several fundamental questions in this context how links over database identifiers can be represented in terms of owl sameas statements how to recover rewritability of sparql into sql  lost because of owl 
our formally treat several fundamental questions in this context how links over database identifiers can be represented in terms of owl sameas statements how to recover rewritability of sparql into sql  lost because of sameas statements 
moreover our investigate how our solution can be made to scale up to large enterprise datasets
our have implemented the approach
our carried out an extensive set of experiments
experiments showing our scalabilitymaking available is for the most part still considered the task of classical publishing companies despite the fact that classical forms of publishing centered around printed narrative articles no longer seem wellsuited in the digital age
archiving scientific results is for the most part still considered the task of classical publishing companies despite the fact that classical forms of publishing centered around printed narrative articles no longer seem wellsuited in the digital age
in particular there exist currently agreedupon methods for publishing scientific datasets
in particular there exist currently no efficient reliable for publishing scientific datasets
publishing scientific datasets which have become increasingly important for science
here we propose to design scientific data publishing as a webbased bottomup process without topdown control of central authorities such as publishing companies
based on a novel combination of technologies we present a server network to decentrally store and archive data in the form of an rdfbased format to represent scientific data
based on a novel combination of technologies we present a server network to decentrally store and archive data in the form of nanopublications to represent scientific data
based on a novel combination of existing concepts we present a server network to decentrally store and archive data in the form of nanopublications to represent scientific data
based on a novel combination of existing concepts we present a server network to decentrally store and archive data in the form of an rdfbased format to represent scientific data
we show how this approach allows researchers to publish retrieve verify and recombine datasets of nanopublications in a trustworthy manner
we argue that this architecture could be used for the semantic web in general
we show how this approach allows researchers to publish retrieve verify and recombine datasets of nanopublications in a reliable manner
evaluation of the current small network shows that this system is efficient
evaluation of the current small network shows that this system is reliablerecently web search engines have empowered web search engines search with knowledge graphs to satisfy increasing demands of complex information needs about entities
a structured summary called knowledge card
each engine offers an online knowledge graph service to display highly relevant information about the query entity in form of a structured summary
the cards from different engines might be complementary
therefore it is necessary to fuse knowledge cards from different engines to get a comprehensive view
such a problem can be considered as a new branch of ontology alignment
ontology alignment which is actually an onthefly online data fusion based on the users needs
in this paper we present the first effort to work on knowledge cards fusion
we propose a novel probabilistic scoring algorithm for card disambiguation to select the most likely entity a card should refer to
we then design a learningbased method to align properties from cards
cards representing the same entity
finally we perform value deduplication to group equivalent values of the aligned properties as value clusters
the experimental results show that we approach outperforms the state of the art ontology alignment algorithms in terms of recall
the experimental results show that we approach outperforms the state of the art ontology alignment algorithms in terms of precisionwe present ackermann for forgetting concept symbols in ontologies
ontologies specified in the description logic alcoi
ackermann is an adaptation and improvement of a secondorder quantifier elimination method developed for modal logics
ackermann is an adaptation and improvement of a secondorder quantifier elimination method used for computing correspondence properties for modal axioms
it follows an approach adapted to description logics
an approach exploiting a result of ackermann
an important feature inherited from the modal approach
an important feature is that the inference rules are guided by an ordering compatible with the elimination order of the concept symbols
this provides more control over the inference process and reduces nondeterminism resulting in a smaller search space
ackermann is extended with several simplification rules
ackermann is extended with a new case splitting inference rule
compared to related forgetting interpolation methods for description logics ackermann can handle inverse roles nominals and aboxes
compared to related uniform interpolation methods for description logics ackermann can handle inverse roles nominals and aboxes
compared to the modal approach ackermann improves the success rates
compared to the modal approach ackermann is more efficient in time
the modal approach on which ackermann is based
ackermann has been implemented in java using the owl api
experimental results show that the order affects the success rate
experimental results show that the order affects efficiency
the order in which the concept symbols are eliminated significantlyalgebras of relations were shown useful in managing ontology alignments
algebras of relations make it possible to aggregate alignments conjunctively
algebras of relations make it possible to aggregate alignments disjunctively
to propagate alignments within a network of ontologies
the previously considered algebra of relations
the previously contains taxonomical relations between classes
however compositional inference is sound only if we assume that classes have nonempty extensions
compositional inference using this algebra
classes which occur in alignments
moreover this algebra covers relations only between classes
here we introduce a new algebra of relations which first solves the limitation of the previous one and second incorporates all qualitative taxonomical relations including the relations  is a  and  is not 
all qualitative taxonomical relations that occur between concepts
all qualitative taxonomical relations that occur between individuals
we prove that this algebra is coherent with respect to the simple semantics of alignmentsclientside query processing techniques provide a promising solution for web query processing
clientside query processing techniques that rely on the materialization of fragments of the original rdf dataset
because of unexpected data transfers the traditional optimizethenexecute paradigm is not always applicable in this context
the traditional optimizethenexecute paradigm used by existing approaches
performance of clientside execution plans can be negatively affected by live conditions where rate at which data arrive from sources changes
however
linked data eddies that is able to adjust query execution schedulers to data availability
we present a network of linked data eddies
we tackle adaptivity for clientside query processing
linked data eddies that is runtime conditions
experimental studies suggest that the network of linked data eddies outperforms static web query schedulers in scenarios with data distributions
experimental studies suggest that the network of linked data eddies outperforms static web query schedulers in scenarios with unpredictable transfer delaysintegration tools that generate potential schema mappings and users
the iterative user interaction approach for data integration can be generalized to consider interactions between integration tools
data integration proposed by noy
data integration proposed by falconer
integration tools that generate analysis tools
analysis tools that select the best mapping
each such selection then provides highconfidence guidance for the next iteration of the integration tool
this generalized approach in cogmap
we have implemented this
this generalized a matching system for both property
this generalized instance alignments between heterogeneous data
the interactions between integration tool in cogmap presents the interactions between integration tool in cogmap consequences to analysis tool
the interactions between integration tool in cogmap presents highquality property alignments to the potential schema mappings and users
the interactions between integration tool in cogmap uses the instance alignment from the previous iteration to create highquality property alignments
the interactions between integration tool in cogmap presents the interactions between integration tool in cogmap consequences to the potential schema mappings and users
the interactions between integration tool in cogmap presents highquality property alignments to analysis tool
we experiments show that the interplay between property alignment serve to improve the final alignments
we experiments show that multiple iterations serve to improve the final alignments
we experiments show that the interplay between instance alignment serve to improve the final alignmentsvarious applications that require machines to understand large knowledge graphs semantics
large knowledge graphs increasingly add value to various applications as in question answering systems
various applications that require machines to understand queries
large knowledge graphs increasingly add value to various applications as in search answering systems
various applications that require machines to recognize queries
various applications that require machines to recognize large knowledge graphs semantics
latent variable models have increasingly gained attention for the statistical modeling of knowledge graphs
tasks related to knowledge graph completion
attention for the statistical modeling of knowledge graphs showing promising results in tasks
tasks related to cleaning
relationtypes that allow machines to understand the notion of things semantic relationships
besides storing facts about the world schemabased knowledge graphs are backed by rich semantic descriptions of relationtypes
entities that allow machines to understand the notion of things semantic relationships
entities that allow machines to understand the notion of machines semantic relationships
besides storing facts about the world schemabased knowledge graphs are backed by rich semantic descriptions of entities
relationtypes that allow machines to understand the notion of machines semantic relationships
in this work we study how typeconstraints can generally support the statistical modeling with latent variable models
more precisely we integrated prior knowledge in form of typeconstraints in various state of the art latent variable approaches
we experimental results show that prior knowledge on relationtypes significantly improves latent variable models up to 77elbased ontologies which are defeasible in the sense that such a mapping only applies to individuals if this does not because an inconsistency
we present a novel approach to denote mappings between elbased ontologies
this provides the advantage of handling exceptions automatically
logical inconsistencies that may be caused due to the traditional type of mappings
this provides the advantage of avoiding logical inconsistencies
the case where mappings from many possibly heterogeneous ontologies are oneway links towards an overarching ontology
we consider the case
questions can then be asked in terms of the concepts in the overarching ontology
we provide the formal semantics for the defeasible mappings
we show that reasoning under such a setting is decidable even when the defeasible axioms apply to unknowns
furthermore we show that the formal semantics for the defeasible mappings actually is strongly related to the idea of answer sets for logic programsautomated acquisition
automated acquisition of ontologies has attracted research attention because automated acquisition of ontologies can help ontology engineers build ontologies
automated acquisition of ontologies has attracted research attention because automated acquisition of ontologies can give domain experts new insights into domain experts data
automated acquisition of ontologies has attracted research attention because automated acquisition of ontologies can give domain experts new insights into domain experts data
automated acquisition of ontologies has attracted research attention because automated acquisition of ontologies can help ontology engineers build ontologies
human involvement make assumptions about data do not fully respect background knowledge
existing approaches to ontology automated acquisition are considerably limited
focus on automated acquisition descriptions for given classes require intense supervision
however
we investigate the problem of general terminology induction automated acquisition sets of general class inclusions gcis from data
we investigate the problem of general terminology induction automated acquisition sets of general class inclusions gcis from background knowledge
measures that evaluate statistical quality of a set of gcis
we introduce measures
measures that evaluate logical quality of a set of gcis
we present methods to compute an anytime algorithm
we present methods to compute these measures
an anytime algorithm that induces sets of gcis
we experiments show that we can acquire interesting sets of gcis
we experiments show that we can provide insights into the structure of the search spacerdf datasets which are important to a large number of users such as
problems related to the description and analysis of the evolution of rdf datasets
rdf datasets which are important to a large number of domains such as
the dynamic nature of web data gives rise to a multitude of problems the curators of biological information where changes are interrelated
the dynamic nature of web data gives rise to a multitude of problems the curators of biological information where changes are constant
a framework that enables identifying understanding these dynamics
in this paper we propose a framework
a framework that enables identifying analysing these dynamics
we approach is flexible enough to capture the peculiarities and needs of different applications on dynamic data while being formally robust due to the satisfaction of the completeness properties
we approach is flexible enough to capture the peculiarities and needs of different applications on dynamic data while being formally robust due to the satisfaction of the unambiguity properties
queries involving both changes
a manner that enables easy navigation among versions queries
in addition we framework allows the persistent representation of the detected changes between versions in a manner
a manner that enables efficient navigation among versions automated processing and analysis of changes crosssnapshot queries  spanning across different versions 
a manner that enables efficient navigation among versions queries
a manner that enables easy navigation among versions automated processing and analysis of changes crosssnapshot queries  spanning across different versions 
queries involving both data
we work is evaluated using real linked open data
we work exhibits good scalability propertiesinstance matching has emerged as an important problem in the semantic web with machine learning methods
machine learning methods proving especially effective
to enhance performance taskspecific knowledge is typically used to introduce bias in the model selection problem
such biases tend to be exploited by practitioners in a piecemeal fashion
a framework where the model selection design process is represented as a factor graph
this paper introduces a framework
nodes in this bipartite graphical model represent opportunities for explicitly introducing bias
the graph is first used to visualize common biases in the design of existing instance matchers
the graph is first used to unify common biases in the design of existing instance matchers
as a direct application we then use the graph to hypothesize about potential unexploited biases
the hypotheses are evaluated by training 1032 neural networks on three instance matching tasks on microsoft azures cloudbased platform
an analysis over 25 gb of experimental data indicates that the proposed biases can improve efficiency by over 65the web of data has been introduced as a novel scheme for imposing structured data on the web of data
this renders data seamlessly processable by machines at the same time
this renders data easily understandable by human beings by machines at the same time
the recent boom in linked data facilitates a new stream of dataintensive applications that leverage the knowledge available in semantic datasets such as freebase
the recent boom in linked data facilitates a new stream of dataintensive applications that leverage the knowledge available in semantic datasets such as dbpedia
data that can be used to feed a contentbased recommender system
these latter are well known encyclopedic collections of data
in this paper we investigate how the choice of one of the two datasets may influence the performance of a recommendation engine not only also in terms of these latter diversity and novelty
in this paper we investigate how the choice of one of the two datasets may influence the performance of a recommendation engine not only in terms of precision of the results
four different recommendation approaches exploiting both dbpedia in the music domain
four different recommendation approaches exploiting both freebase in the music domain
we tested four different recommendation approachesanalytics which can provide important insights into the research activity
the amount of scholarly data available on the web is steadily increasing enabling different types of analytics
in order to explore this largescale body of knowledge we need an accurate ontology of research topics
in order to make sense of we need an comprehensive ontology of research topics
in order to explore this largescale body of knowledge we need an uptodate ontology of research topics
in order to make sense of we need an accurate ontology of research topics
in order to explore this largescale body of knowledge we need an comprehensive ontology of research topics
in order to make sense of we need an uptodate ontology of research topics
unfortunately human crafted classifications do not satisfy these criteria as human crafted classifications evolve too slowly
unfortunately human crafted classifications do not tend to be too coarsegrained
indirect semantic relationships which can help to understand the relation between two topics
indirect statistical relationships which can help to understand the relation between two topics
current automated methods for generating ontologies of research areas also present a number of limitations such as i  current automated methods for generating ontologies of research areas do not consider the rich amount of indirect semantic relationships
current automated methods for generating ontologies of research areas also present a number of limitations such as i  current automated methods for generating ontologies of research areas do not consider the rich amount of indirect statistical relationships
a novel approach which improves on we earlier work on automatic generation of semantic topic networks
in this paper we present klink2 by taking advantage of a variety of knowledge sources available on the web
a novel approach which improves on we earlier work on addresses the aforementioned limitations
in this paper we present a novel approach by taking advantage of a variety of knowledge sources available on the web
in particular klink2 analyses networks of research entities  including technologies  to infer three kinds of semantic relationships between topics
in particular klink2 analyses networks of research entities  including authors  to infer three kinds of semantic relationships between topics
in particular klink2 analyses networks of research entities  including papers  to infer three kinds of semantic relationships between topics
in particular klink2 analyses networks of research entities  including venues  to infer three kinds of semantic relationships between topics
this paper also identifies ambiguous keywords
this paper also separates ambiguous keywords into the appropriate distinct topics
we experimental evaluation shows that the ability of klink2 to generate topics with accurate contextual meaning yields significant improvements over other algorithms in terms of both precision and recall
we experimental evaluation shows that the ability of klink2 to integrate a high number of data sourcesweb tables form a valuable source of relational data
web tables contains an estimated 154 million html tables of relational data with wikipedia alone containing 16 million highquality tables
extracting the semantics of web tables to produce machineunderstandable knowledge has become an active area of research
