knowledge bases contain rich information about realworld objects
knowledge bases contain complementary information about realworld objects
textual documents contain rich information about realworld objects
textual documents contain relations among knowledge bases rrb
knowledge bases contain relations among knowledge bases rrb
knowledge bases contain relations among knowledge bases rrb
textual documents contain relations among textual documents
textual documents contain complementary information about realworld objects
knowledge bases contain relations among textual documents
textual documents contain relations among knowledge bases rrb
while text documents describe entities in freeform knowledge bases organizes such information in a structured way
this makes these two information representation forms hard to integrate limiting the possibility to use these two information representation forms jointly to improve analytical tasks
this makes these two information representation forms hard to compare limiting the possibility to use these two information representation forms jointly to improve analytical tasks
this makes these two information representation forms hard to compare limiting the possibility to use these two information representation forms jointly to improve predictive tasks
this makes these two information representation forms hard to integrate limiting the possibility to use these two information representation forms jointly to improve predictive tasks
we study this problem
a regularized multitask learning of document embeddings
we propose kade
we propose a solution
a regularized multitask learning of knowledge base
a solution based on a
in this article
a regularized multitask learning of knowledge base
a regularized multitask learning of document embeddings
a solution based on a
kade can potentially incorporate document embedding learning method
a solution can potentially incorporate document embedding learning method
a solution can potentially incorporate any knowledge base
kade can potentially incorporate any knowledge base
a regularized multitask learning of document embeddings
we experiments on multiple datasets and methods show that a solution effectively aligns entities embeddings while maintaining the characteristics of the embedding models
a solution based on a
we experiments on multiple datasets and methods show that kade effectively aligns document embeddings while maintaining the characteristics of the embedding models
we experiments on multiple datasets and methods show that a solution effectively aligns document embeddings while maintaining the characteristics of the embedding models
we experiments on multiple datasets and methods show that kade effectively aligns entities embeddings while maintaining the characteristics of the embedding models
a regularized multitask learning of knowledge baseover the recent years embedding methods have attracted increasing focus as a means for knowledge graph completion
similarly rulebased systems have been studied for this task in the past
a common evaluation that includes more than one type of method
what is missing so far is a common evaluation
we close this gap by comparing representatives of both types of systems in a frequently used evaluation protocol
a finegrained evaluation that gives insight into characteristics of the most popular points out the different strengths and shortcomings of the examined approaches
a finegrained evaluation that gives insight into characteristics of the most popular datasets out the different strengths and shortcomings of the examined approaches
leveraging the explanatory qualities of rulebased systems we present a finegrained evaluation
our results show that models such as hole have problems in solving certain types of completion tasks
our results show that models such as rescal have problems in solving certain types of completion tasks
our results show that models such as transe have problems in solving certain types of completion tasks
completion tasks that can be solved by a rulebased approach with high precision
at the same time there are other completion tasks
other completion tasks that are difficult for rulebased systems
motivated by these insights we combine both families of approaches via ensemble learning
our results support our assumption that the two methods complement each other in a beneficial wayconverting data from diverse data sources to custom rdf datasets often faces several practical challenges transform the source data
converting data from diverse data sources to custom rdf datasets often faces several practical challenges related with the need to restructure
existing rdf mapping languages assume that the resulting datasets mostly preserve the structure of the source data
real cases that highlight the limitations of existing languages and describe d2rml addresses such practical needs
real cases that highlight the limitations of existing languages and describe a transformationoriented rdf mapping language which addresses such practical needs
in this paper we present real cases by incorporating a programming flavor in the mapping processblank nodes in rdf graphs can be used to represent values whose identity remains unknown
values known to exist but
a prominent example of such usage can be found in the wikidata dataset where the author of beowulf is given as a blank node
however while sparql considers blank nodes in a query as existentials the author of beowulf treats blank nodes in rdf data more like constants
counterintuitive results which may make the standard sparql semantics unsuitable for datasets with existential blank nodes
running sparql queries over datasets with unknown values may thus lead to counterintuitive results
we thus explore the feasibility of an alternative sparql semantics based on certain answers
approximation techniques proposed in a relational database
in order to estimate the performance costs that would be associated with such a change in semantics for current implementations we adapt approximation techniques setting for a core fragment of sparql
in order to estimate the performance costs that would be associated with such a change in semantics for current implementations we evaluate approximation techniques setting for a core fragment of sparql
to further understand the impact that such a change in semantics may have on query solutions we analyse how this new semantics would affect the results of user queries over wikidatamany citizens nowadays flock to social media during crises to acquire the latest information about the event
many citizens nowadays flock to social media during crises to share the latest information about the event
due to the sheer volume of data typically circulated during such events it is necessary to be able to efficiently filter out irrelevant posts thus focusing attention on the posts
the posts that are truly relevant to the crisis
current methods for classifying the relevance of posts to a crisis or set of crises is not viable during rapidly evolving crisis situations to train new models for each language
current methods for classifying the relevance of posts to a crisis or set of crises typically struggle to deal with posts in different languages
in this paper we test statistical classification approaches on crosslingual datasets from 30 crisis events consisting of posts
in this paper we test semantic classification approaches on crosslingual datasets from 30 crisis events italian
in this paper we test semantic classification approaches on crosslingual datasets from 30 crisis events consisting of posts
in this paper we test statistical classification approaches on crosslingual datasets from 30 crisis events italian
posts written mainly in english spanish
we experiment with scenarios where the model is trained on one language
we experiment with scenarios where the data is translated to a single language
we experiment with scenarios where the model is tested on another
semantic features extracted from external knowledge bases
we show that the addition of semantic features improve accuracy over a purely statistical modelfaceted browsing has also been investigated in rdf
faceted browsing has become a popular paradigm for user interfaces on the web
however current faceted browsers for rdf graphs encounter performance issues
performance issues when faced with two challenges
heterogeneity where large numbers of classes generate many facets
however current faceted browsers for rdf graphs encounter scale
however current faceted browsers for rdf graphs encounter heterogeneity
scale where large datasets generate many results
heterogeneity where large numbers of properties generate many facets
heterogeneous largescale rdf graphs based on a materialisation strategy
to address two challenges we propose a faceted browsing system for heterogeneous largescale rdf graphs
possible facet combinations that are candidates for indexing
to address two challenges we propose grafa
a materialisation strategy that performs an offline analysis of the input graph in order to identify a subset of the exponential number of possible facet combinations
in experiments over wikidata we demonstrate that materialisation allows for displaying faceted views over millions of diverse results in under a second while keeping index sizes relatively small
we also present initial usability studies over grafawe address the concept of research activity
we address the automatic extraction from publications of two key concepts for representing research processes
we address the sequence relation between successive activities
these representations are driven by the scholarly ontology specifically conceived for documenting research processes
unlike usual named entity recognition we are facing textual descriptions of activities of widely variable length while pairs of successive activities often span multiple sentences
unlike relation extraction tasks we are facing textual descriptions of activities of widely variable length while pairs of successive activities often span multiple sentences
window classifiers using svms
we developed with several
several sliding a twostage pipeline classifier
several sliding window classifiers
window classifiers using logistic regression
window classifiers using random forests
we experimented with several
word embeddings engineered to exploit distinctive traits of research publications
we classifiers employ taskspecific features
dependency embeddings engineered to exploit distinctive traits of research publications
research publications written in english
we classifiers employ dependency embeddings
we classifiers employ partofspeech embeddings
partofspeech embeddings engineered to exploit distinctive traits of research publications
we classifiers employ word embeddings
sequences stored as rdf triples in a knowledge base
the extracted activities are associated with other relevant information from publication metadata
sequences are associated with other relevant information from publication metadata
the extracted activities stored as rdf triples in a knowledge base
evaluation on datasets from three disciplines medicine shows very promising performance
evaluation on datasets from three disciplines bioinformatics shows very promising performance
evaluation on datasets from three disciplines digital humanities shows very promising performanceoptional is a key feature in sparql for dealing with missing information
this operator complexity which can make efficient evaluation of queries with optional challenging
while this operator is used extensively this operator is also known for this operator complexity
setting where the data is exposed as a virtual rdf graph by means of an r2rml mapping
setting where the data is stored in a sql relational database
we tackle this problem in the ontologybased data access  obda  setting
we start with a succinct translation of a sparql fragment into sql
a succinct translation of a sparql fragment fully respects bag semantics and relies on the extensive use of the left join coalesce function
a succinct translation of a sparql fragment fully respects threevalued logic and relies on the extensive use of the left join coalesce function
a succinct translation of a sparql fragment fully respects threevalued logic and relies on the extensive use of the left join operator function
a succinct translation of a sparql fragment fully respects bag semantics and relies on the extensive use of the left join operator function
we then propose optimisation techniques for improving the structure of generated sql queries
we then propose optimisation techniques for reducing the size
we optimisations capture interactions between join the left join coalesce
we optimisations capture interactions between join the left join integrity constraints such as attribute nullability
we optimisations capture interactions between join the left join integrity constraints such as uniqueness
we optimisations capture interactions between join the left join integrity constraints such as foreign key constraints
finally we empirically verify effectiveness of we techniques on the bsbm obda benchmarkknowledge bases such as dbpedia contain a huge number of facts
knowledge bases such as wikidata contain a huge number of entities
knowledge bases such as yago contain a huge number of entities
knowledge bases such as wikidata contain a huge number of facts
knowledge bases such as yago contain a huge number of facts
knowledge bases such as dbpedia contain a huge number of entities
several recent works calculate statistics on knowledge bases such as yago
several recent works calculate statistics on knowledge bases such as wikidata
several recent works induce rules
several recent works calculate statistics on knowledge bases such as yago
several recent works calculate statistics on knowledge bases such as dbpedia
several recent works calculate statistics on knowledge bases such as dbpedia
several recent works calculate statistics on knowledge bases such as wikidata
most of these methods are based on the assumption that the data is a representative sample of the studied universe
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as yago are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as wikidata are built from crowdsourcing
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as yago are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as dbpedia are biased because knowledge bases such as dbpedia are built from crowdsourcing
unfortunately knowledge bases such as yago are biased because knowledge bases such as dbpedia are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as yago are biased because knowledge bases such as wikidata are built from opportunistic agglomeration of available databases
unfortunately knowledge bases such as wikidata are biased because knowledge bases such as yago are built from crowdsourcing
this paper aims at approximating the representativeness of a relation within a knowledge base
benfords law which indicates the distribution
the distribution expected by the facts of a relation
for this we use the
the generalized benfords law
we then compute the minimum number of facts
facts that have to be added in order to make the knowledge base representative of the real world
experiments show that our unsupervised method applies to a large number of relations
numerical relations where ground truths exist
for numerical relations the estimated representativeness proves to be a reliable indicatormany approaches for knowledge extraction rely on wellknown natural language processing  tasks such as named entity linking to identify
the entities mentioned in natural language text
many approaches for ontology population semantically characterize the entities
many approaches for ontology population rely on nlp  tasks such as named entity recognition and classification to identify
many approaches for knowledge extraction rely on nlp  tasks such as named entity recognition and classification to identify
many approaches for ontology population rely on wellknown natural language processing  tasks such as named entity recognition and classification to identify
many approaches for knowledge extraction rely on wellknown natural language processing  tasks such as named entity recognition and classification to identify
many approaches for ontology population rely on wellknown natural language processing  tasks such as named entity linking to identify
many approaches for knowledge extraction rely on nlp  tasks such as named entity linking to identify
many approaches for ontology population rely on nlp  tasks such as named entity linking to identify
many approaches for knowledge extraction semantically characterize the entities
the analyses differ
combining wellknown natural language processing tasks such as named classification to identify output may result in nlp annotations
the analyses performed by wellknown natural language processing tasks such as named entity linking to identify
the analyses performed by nlp tasks such as named classification to identify
even conflicting considering common world knowledge about entities
combining wellknown natural language processing tasks such as named entity linking to identify output may result in even conflicting
combining nlp tasks such as named entity recognition and classification to identify output may result in nlp annotations
combining wellknown natural language processing tasks such as named entity linking to identify output may result in nlp annotations
the analyses performed by nlp tasks such as named entity recognition to identify
the analyses performed by nlp tasks such as named entity recognition and classification to identify
combining wellknown natural language processing tasks such as named entity recognition to identify output may result in even conflicting
combining nlp tasks such as named entity recognition to identify output may result in even conflicting
the analyses performed by wellknown natural language processing tasks such as named entity linking to identify
combining wellknown natural language processing tasks such as named entity linking to identify output may result in nlp annotations
the analyses performed by wellknown natural language processing tasks such as named entity recognition to identify
the analyses performed by wellknown natural language processing tasks such as named classification to identify
combining nlp tasks such as named entity linking to identify output may result in even conflicting
the analyses performed by nlp tasks such as named entity linking to identify
combining nlp tasks such as named entity recognition to identify output may result in nlp annotations
combining wellknown natural language processing tasks such as named entity linking to identify output may result in even conflicting
the analyses performed by wellknown natural language processing tasks such as named entity recognition and classification to identify
despite being intrinsically related
combining wellknown natural language processing tasks such as named entity recognition to identify output may result in nlp annotations
combining nlp tasks such as named entity recognition and classification to identify output may result in even conflicting
combining wellknown natural language processing tasks such as named classification to identify output may result in even conflicting
combining nlp tasks such as named classification to identify output may result in even conflicting
combining wellknown natural language processing tasks such as named entity recognition and classification to identify output may result in even conflicting
combining nlp tasks such as named classification to identify output may result in nlp annotations
the analyses performed by nlp tasks such as named entity linking to identify
combining nlp tasks such as named entity linking to identify output may result in even conflicting
nlp annotations that are implausible
combining nlp tasks such as named entity linking to identify output may result in nlp annotations
combining wellknown natural language processing tasks such as named entity recognition and classification to identify output may result in nlp annotations
combining nlp tasks such as named entity linking to identify output may result in nlp annotations
different tasks insisting on the same entity
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
in this paper we present a probabilistic soft logic model mentions
the intuition behind a probabilistic soft logic model is that an annotation likely implies some ontological classes on the entity
annotations from different tasks on the same mention have to share more the same implied entity classes
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
annotations from different tasks on the same mention have to share less the same implied entity classes
different tasks insisting on the same entity
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
the entity identified by the same mention
the intuition behind a probabilistic soft logic model is that an annotation likely implies some ontological classes on the entity
different tasks insisting on the same entity
in a setting with various nlp tools a probabilistic soft logic model can be operationally applied to possibly revise the tools best annotation choice
in a setting with various nlp tools a probabilistic soft logic model can be operationally applied to compare the different annotation combinations
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
a setting with various nlp tools returning multiple confidenceweighted candidate annotations on a single mention
in a setting with various nlp tools a probabilistic soft logic model can be operationally applied to compare the different annotation combinations
in a setting with various nlp tools a probabilistic soft logic model can be operationally applied to possibly revise the tools best annotation choice
we experimented applying a probabilistic soft logic model
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
we experimented applying a probabilistic soft logic model
the candidate annotations produced by two stateoftheart tools for entity recognition and classification on three different datasets
the candidate annotations produced by two stateoftheart tools for entity linking on three different datasets
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
different tasks insisting on the same entity with the candidate annotations
the joint  a posteriori  annotation revision suggested by a probabilistic soft logic model
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
a probabilistic soft logic model that leverages ontological entity classes to relate nlp annotations from different tasks
the joint  a posteriori  annotation revision suggested by a probabilistic soft logic model
different tasks insisting on the same entity consistently
the results show that the joint  a posteriori  annotation revision improves the original scores of the two toolsthe lod cloud offers a plethora of rdf data sources where users discover items of interest by issuing sparql queries
a sparql query that returns nothing how to refine the query to obtain a nonempty set
a common query problem for users is to face with empty answers given a sparql query
in this paper we propose an rdf graph embedding
an rdf graph embedding based framework to solve the sparql emptyanswer problem in terms of a continuous vector space
a continuous vector space by an entity context preserving translational embedding model
translational embedding model which is specially designed for sparql queries
we first project the rdf graph into a continuous vector space by an entity context
a sparql query that returns an empty set we partition an empty set into several parts
a sparql query that compute approximate answers by leveraging rdf embeddings
a sparql query that compute approximate answers by leveraging the translation mechanism
then given a sparql query
returned answers which helps users recognize users expectations
returned answers which helps users refine the original query finally
we also generate alternative queries for returned answers
to validate the effectiveness and efficiency of our framework our conduct extensive experiments on the realworld rdf dataset
the results show that our framework can significantly improve the quality of approximate answers
the results show that our framework can significantly speed up the generation of alternative queriesnamed entity recognition and typing is a challenging task especially with longtail entities such as the ones
the ones found in scientific publications
the ones found in scientific publications
longtail entities such as the ones are rare often relevant only in specific knowledge domains yet important for exploration purposes
longtail entities such as the ones are rare often relevant only in specific knowledge domains yet important for retrieval purposes
stateoftheart ner approaches employ supervised machine
models trained on expensive typelabeled data laboriously produced by human annotators
supervised machine learning models
a common workaround is the generation of labeled training data from knowledge bases this approach is not suitable for longtail entity types
longtail entity types that are by definition scarcely represented in kbs
scientific publications that relies on minimal human input namely a small seed set of instances for the targeted entity type
this paper presents an iterative approach for training ner in scientific publications
this paper presents an iterative approach for net classifiers in scientific publications
we introduce different strategies for training data extraction semantic expansion
we result entity filtering
we evaluate we approach on scientific publications focusing on methods in computer science publications
we evaluate we approach on scientific publications focusing on the longtail entities types datasets
we evaluate we approach on scientific publications focusing on proteins in biomedical publicationsdealing with large tabular datasets often requires extensive preprocessing
this preprocessing happens only once so that loading the data in a database may be an overkill
this preprocessing happens only once so that indexing the data in a database may be an overkill
this preprocessing happens only once so that loading the data in triple store may be an overkill
this preprocessing happens only once so that indexing the data in triple store may be an overkill
in this paper we present an approach
an approach that allows preprocessing large tabular data in datalog without indexing the data
the datalog query is translated to unix bash
the datalog query can be executed in a she will
we experiments show that for the use case of data preprocessing we approach is competitive with stateoftheart systems in terms of speed while at the same time she will on a unix system
the same time requiring only a bash
we experiments show that for the use case of data preprocessing we approach is competitive with stateoftheart systems in terms of scalability while at the same time she will on a unix systemhistorical newspapers provide a lens on habits of the past
historical newspapers provide a lens on customs of the past
for example recipes highlight how we ate about food
recipes published in newspapers
for example recipes highlight what
for example recipes highlight how we thought about food
the challenge here is that newspaper data varied
the challenge here is that newspaper data is often unstructured
digitised historical newspapers add an additional challenge namely that of fluctuations in ocr quality
therefore it is difficult to extract recipes from recipes
therefore it is difficult to locate recipes from recipes
we present we approach automatically extracted lexicons to identify recipes in digitised historical newspapers to extract ingredient information
we present we approach based on distant supervision
we present we approach automatically extracted lexicons to identify recipes in digitised historical newspapers to generate recipe tags
we provide ocr quality indicators
we provide ocr quality indicators impact on the extraction process
we enrich the recipes with links to information on the ingredients
we research shows how machine learning can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food culture
we research shows how natural language processing can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food culture
we research shows how semantic web can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food culturethe steadilygrowing popularity of semantic data on the web have propelled the interest in online analytical processing 
the steadilygrowing popularity of the support for aggregation queries in sparql 11 have propelled the interest in online analytical processing 
the steadilygrowing popularity of the support for aggregation queries in sparql 11 have propelled data cubes in rdf
the steadilygrowing popularity of semantic data on the web have propelled the interest in sparql online analytical processing 
the steadilygrowing popularity of semantic data on the web have propelled data cubes in rdf
the steadilygrowing popularity of the support for aggregation queries in sparql 11 have propelled the interest in sparql online analytical processing 
query processing in such settings is challenging because sparql online analytical processing queries usually contain many triple patterns with grouping
query processing in such settings is challenging because sparql online analytical processing queries usually contain many triple patterns with aggregation
query processing in such settings is challenging because sparql online analytical processing queries usually contain many triple patterns with grouping
query processing in such settings is challenging because sparql online analytical processing queries usually contain many triple patterns with aggregation
moreover one important factor of query answering on web data is web data provenance
some applications in data analytics require to augment the data with provenance metadata
some applications in access control require to augment the data with provenance metadata
queries that impose constraints on its provenance
some applications in data analytics require to run queries
some applications in access control require to run queries
this task is called provenanceaware query answering
provenance information when answering provenanceaware sparql queries
in this paper we investigate the benefit of caching some parts of an rdf cube augmented with provenance information
a caching approach based on a provenanceaware partitioning of rdf graphs
we propose provenanceaware caching
we propose sparql queries with aggregation
we propose a caching approach
we propose a benefit model for rdf cubes
we results on synthetic data show that provenanceaware caching outperforms significantly the jena tdb native caching in terms of hitrate
we results on real data show that provenanceaware caching outperforms significantly the lru strategy
we results on real data show that provenanceaware caching outperforms significantly the jena tdb native caching in terms of hitrate
we results on real data show that provenanceaware caching outperforms significantly the jena tdb native caching in terms of response time
we results on synthetic data show that provenanceaware caching outperforms significantly the lru strategy
we results on synthetic data show that provenanceaware caching outperforms significantly the jena tdb native caching in terms of response timewith the popularity of rdf as an independent data model came the need for mechanisms to detect violations of such constraints
with the popularity of rdf as an independent data model came the need for specifying constraints on rdf as an independent data graphs
one of the most promising schema languages for rdf as an independent data is shacl 
one of the most promising schema languages for rdf as an independent data is a recent w3c recommendation
unfortunately the specification of shacl leaves open the problem of validation against recursive constraints
this omission is important because shacl by design favors constraints
other ones which in practice may easily yield reference cycles
constraints that reference other onesreasonable ontology templates is a language for representing ontology modelling patterns in the form of parameterised ontologies
ontology templates are simple abstractions useful for constructing
ontology templates are powerful abstractions useful for constructing
ontology templates are powerful abstractions useful for interacting with
ontology templates are simple abstractions useful for interacting with
ontology templates are simple abstractions useful for maintaining ontologies
ontology templates are powerful abstractions useful for maintaining ontologies
with ontology templates modelling patterns can be broken down into convenient pieces
with ontology templates modelling patterns can be used as queries
with ontology templates modelling patterns can be instantiated
with ontology templates modelling patterns can be broken down into manageable pieces
with ontology templates modelling patterns can be encapsulated
with ontology templates modelling patterns can be uniquely identified
formal relations defined over templates
formal relations support sophisticated maintenance tasks for sets of templates such as revealing redundancies
formal relations support sophisticated maintenance tasks for sets of templates such as suggesting new templates for representing implicit patterns
ontology templates are designed for practical use an open image for terse specification of bulk instances are available including an open source implementation for using templates
ontology templates are designed for practical use an open image in new window vocabulary convenient serialisation formats for the semantic web are available including an open source implementation for using templates
ontology templates are designed for practical use an open image for terse specification of template definitions are available including an open source implementation for using templates
our approach is successfully tested on a realworld largescale ontology in the engineering domainknowledge graphs are widely used abstractions to represent entitycentric knowledge
these approaches have become increasingly popular for these approaches ability to capture the similarity between entities
these approaches have become increasingly popular for these approaches ability to capture support other reasoning tasks
however representation of time has received little attention in these approaches
in this work we make a first step to encode time into vectorbased entity representations
vectorbased entity representations using a textbased knowledge graphs embedding model
a textbased knowledge graphs embedding model named typed entity embeddings
the entity type which is learned from entity
in typed entity embeddings each entity is represented by a vector
a vector that represents the entity type mentions found in a text corpus
a vector that represents the entity mentions found in a text corpus
inspired by evidence from applicationoriented concerns we propose an approach to encode representations of years into typed entity embeddings by aggregating the representations of the entities
the entities that occur in eventbased descriptions of the years
inspired by evidence from cognitive sciences we propose an approach to encode representations of years into typed entity embeddings by aggregating the representations of the entities
the representations of the entities are used to define two timeaware similarity measures to control the implicit effect of time on entity similarity
the entities that occur in eventbased descriptions of the years
experimental results show that the linear order of years is highly correlated with the effectiveness of the timeaware similarity measure
the timeaware similarity measure proposed to flatten the time effect on entity similarity
experimental results show that the linear order of years is highly correlated with natural time flow
years obtained using our modelinformation extraction traditionally focuses on extracting relations between identifiable entities
yet texts often also contain counting information stating that a subject is in a specific relation with a number of objects without mentioning the objects a number of objects
such counting quantifiers can help in a variety of tasks such as query answering
such counting quantifiers are neglected by prior work
such counting quantifiers can help in a variety of tasks such as knowledge base curationtoday a wealth of data are distributed using semantic web standards
today a wealth of knowledge are distributed using semantic web standards
especially in the biomedical domain several sources like more are distributed in the form of owl ontologies
especially in the biomedical domain several sources like snomed nci fma are distributed in the form of owl ontologies
these can be integrated in order to create one large medical knowledge base
these can be matched in order to create one large medical knowledge base
original structure which may affect applications
however an important issue is that the structure of owl ontologies may be profoundly different hence using the mappings as initially computed can lead to incoherences in owl ontologies original structure
however an important issue is that the structure of owl ontologies may be profoundly different hence using the mappings as initially computed can lead to changes in owl ontologies original structure
in this paper we present a framework approach for integrating independently developed ontologies
in this paper we present a novel approach for integrating independently developed ontologies
an initial seed ontology which may already be in use by an application
starting from an initial seed ontology new sources are used to iteratively extend the seed one
starting from an initial seed ontology new sources are used to iteratively enrich the seed one
a novel finegrained approach which provide an exact
a novel finegrained approach which is based on mapping repair conservativity
a novel finegrained approach which provide approximate algorithms
a novel finegrained approach which formalise structural incompatibilities
a novel finegrained approach which provide practical algorithms
to deal with structural incompatibilities we present a novel finegrained approach
a novel finegrained approach which is based on mapping alignment conservativity
our framework has already been used to integrate a number of medical ontologies provided by babylon health
our framework has already been used to integrate a number of support realworld healthcare services provided by babylon health
stateoftheart ontology integration systems that take into account the structure and coherency of the integrated ontologies obtaining encouraging results
finally our also perform an experimental evaluation
finally our also compare with stateoftheart ontology integration systemsrules over a knowledge graph capture interpretable patterns in data have been proposed
rules over various methods for rule learning have been proposed
since kgs are inherently incomplete rules can be used to deduce missing facts
statistical measures for learned rules such as confidence reflect rule quality well when the a knowledge graph is reasonably complete however statistical measures for learned rules such as confidence might be misleading otherwise
scalability dictates that only a small set of candidate rules could be generated
so it is difficult to learn highquality rules from the a knowledge graph alone
therefore pruning of candidate rules are major problems
therefore the ranking are major problems
a rule learning method that utilizes probabilistic representations of missing facts
to address this issue we propose a rule learning method
rules induced from a a knowledge graph by relying on feedback from a precomputed embedding model over external information sources including text corpora
in particular we iteratively extend rules
rules induced from a a knowledge graph by relying on feedback from a precomputed embedding model over the a knowledge graph including text corpora
experiments on realworld kgs demonstrate the effectiveness of our novel approach both with respect to the quality of fact predictions that experiments on realworld kgs produce
experiments on realworld kgs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rulesknowledge base population is an important problem in semantic web research
knowledge base population is a key requirement for successful adoption of semantic technologies in many applications
in this paper we present socrates
in this paper we present a deep learning
a deep learning based solution for automated knowledge base population from text
manual annotations which would make the solution hard to adapt to a new domain
socrates does not require manual annotations
instead the solution exploits a large corpus of text documents to train a set of deep neural network models
instead the solution exploits a partially populated knowledge base to train a set of deep neural network models
as a result of the training process the system learns how to identify implicit relations between entities across a highly heterogeneous set of documents from various sources making the system suitable for largescale knowledge extraction from web documents
main contributions of this paper include a novel approach
unifying relation extraction using binary
unifying relation extraction using unary
main contributions of this paper include an architecture for unifying relation extraction
a novel approach based on composite contexts to acquire implicit relations from title oriented documents
unifying relation extraction using composite contexts
an extensive evaluation of the system across three different benchmarks with different characteristics showing that we unified framework can consistently outperform state of the art solutions
we provide an extensive evaluation of the system across three different benchmarks with different characteristics
remarkably socrates ranked first in both attribute validation track at the semantic web challenge at iswc 2017
remarkably socrates ranked first in both the knowledge base population at the semantic web challenge at iswc 2017named entities mentioned in text
named entity recognition and disambiguation are subtasks of information extraction
information extraction that aim to recognize named entities
information extraction that aim to assign them predefined types
information extraction that aim to link them with them
them matching entities in a knowledge base
many approaches often exposed as web apis have been proposed to solve these tasks during the last years
entities using different taxonomies with different knowledge bases
web apis classify entities
entities using disambiguate web apis with different knowledge bases
in this paper we describe ensemble nerd normalizes numerous extractors responses
a framework that collects numerous extractors responses
in this paper we describe a framework normalizes numerous extractors responses
in this paper we describe ensemble nerd combines numerous extractors responses in order to produce a final entity list
in this paper we describe a framework combines numerous extractors responses in order to produce a final entity list
the presented approach is based on using the extractors responses as input samples for two deep learning networks ensemble neural network for disambiguation 
the presented approach is based on representing the extractors responses as realvalue vectors
the presented approach is based on using the extractors responses as input samples for two deep learning networks enntr 
the presented approach is based on using the extractors responses as input samples for two deep learning networks ensemble neural network for type recognition 
the presented approach is based on using the extractors responses as input samples for two deep learning networks ennd 
we train two deep learning networks ensemble neural network for disambiguation rrb
ennd using specific gold standards
ensemble neural network for type recognition using specific gold standards
we train two deep learning networks ensemble neural network for type recognition rrb
enntr using specific gold standards
ensemble neural network for disambiguation using specific gold standards
we train two deep learning networks ennd rrb
we train two deep learning networks enntr rrb
we show that the models produced outperform each single extractor responses in terms of micro f1 measures
we show that the models produced outperform each single extractor responses in terms of macro f1 measures
micro f1 measures computed by the gerbil framework
macro f1 measures computed by the gerbil frameworkinformation extraction refers to automatically extracting structured relation tuples from unstructured texts
relation extraction are severely restricted by limited relation types
common information extraction solutions are severely restricted by informal relation specifications
open information extraction systems can hardly handle crosssentence tuples
relation extraction can hardly handle crosssentence tuples
open information extraction systems are severely restricted by limited relation types
relation extraction are severely restricted by informal relation specifications
common information extraction solutions can hardly handle crosssentence tuples
open information extraction systems are severely restricted by informal relation specifications
common information extraction solutions are severely restricted by limited relation types
qa4 which leverages the flexible question
a novel information extraction framework named qa4ie
in order to overcome these weaknesses we propose a novel information extraction framework
the flexible question answering approaches to produce high quality relation triples across sentences
based on a novel information extraction framework we develop a large information extraction benchmark with high quality human evaluation
qa4 which leverages the flexible question answering approaches to produce high quality relation triples across sentences
qa4 which leverages the flexible question answering approaches to produce high quality relation triples across sentences
a novel information extraction framework named qa4ie
this benchmark contains 293k documents
this benchmark contains 636 relation types
this benchmark contains 2m golden relation triples
the results show that our system achieves great improvements
we compare we system with some information extraction baselines on we benchmarkknowledge expressed in rdf knowledge bases to enhance truth discovery performances
this study exploits knowledge
truth discovery aims to identify facts 
facts  when conflicting claims are made by several sources
true claims  when conflicting claims are made by several sources
truth discovery aims to identify true claims 
based on the assumption that true claims are provided by reliable sources truth discovery models iteratively compute source trustworthiness in order to determine which claims are true
based on the assumption that reliable sources provide true claims truth discovery models iteratively compute value confidence in order to determine which claims are true
based on the assumption that reliable sources provide true claims truth discovery models iteratively compute source trustworthiness in order to determine which claims are true
based on the assumption that true claims are provided by reliable sources truth discovery models iteratively compute value confidence in order to determine which claims are true
a model that exploits the knowledge
the knowledge extracted from an existing rdf
we propose a model rdf knowledge base in the form of these rules
the evidence given by the rdf rdf knowledge base to support a claim
these rules are used to quantify the evidence
the evidence is then integrated into the computation of the confidence value to improve the evidence estimation
enhancing truth discovery models efficiently obtains a larger set of reliable facts that vice versa can populate rdf rdf knowledge bases
the proposed approach which led to an improvement of up to 18percent compared to the model we modified
empirical experiments on realworld datasets showed the potential of the proposed approachwe present an unsupervised approach to process natural language questions
process natural language questions that can not be answered by advanced data querying requiring instead adhoc code generation and execution
process natural language questions that can not be answered by factual question answering querying requiring instead adhoc code generation and execution
to address this challenging task our system open image in new window performs languagetocode translation by interpreting the natural language question
a sparql query that is run against codeontology
to address this challenging task our system open image in new window performs languagetocode translation by generating a sparql query
a large rdf repository containing millions of triples
a sparql query that is run against a large rdf repository
triples representing java code constructs
a sparql query that is run against a large rdf repository
triples representing java code constructs
a sparql query that is run against codeontology
the best candidate that is then executed to get the correct answer
a large rdf repository containing millions of triples
a sparql query retrieves a number of java source code snippets and methods ranked by open image in new window on semantic features to find the best candidate
a sparql query retrieves a number of java source code snippets and methods ranked by open image in new window on both syntactic to find the best candidate
experimental results show that our approach is comparable with other stateoftheart proprietary systems such as the closedsource wolframalpha computational knowledge engine
a dataset extracted from stackoverflow
the evaluation of our system is based on a datasetcloudbased systems provide a rich platform for managing largescale rdf data
however the distributed nature of cloudbased systems introduces several performance challenges especially for rdf queries
rdf queries that involve multiple join operations
several optimization techniques that enhance the performance of rdf queries
to alleviate these challenges this paper studies the effect of several optimization techniques
intermediate results that are common for certain
based on the query workload reduced sets of intermediate results join patterns are computed
furthermore these reductions are not computed beforehand
furthermore these reductions are rather computed only for the frequent join patterns in an online fashion
an online fashion using bloom filters
rather than caching the final results of each query we show that caching these reductions allows reusing intermediate results across multiple queries
multiple queries that share the same join patterns
in addition we introduce an efficient solution for rdf queries with unbound properties
based on a realization of the proposed optimizations on top of spark extensive experimentation demonstrates how the proposed optimizations on top of spark lead to an order of magnitude enhancement in terms of storage compared to the stateoftheart solutions
based on a realization of the proposed optimizations on top of spark extensive experimentation demonstrates how the proposed optimizations on top of spark lead to an order of magnitude enhancement in terms of preprocessing compared to the stateoftheart solutions
extensive experimentation using two synthetic benchmarks
based on a realization of the proposed optimizations on top of spark extensive experimentation demonstrates how the proposed optimizations on top of spark lead to an order of magnitude enhancement in terms of query performance compared to the stateoftheart solutions
extensive experimentation using a real datasetfacts stored in a knowledge graph
we address the problem of finding descriptive explanations of facts
this is important in highrisk domains such as healthcare intelligence where machinelearned systems extract facts from an input corpus is opaque to the enduser
intelligence where users need additional information for decision making
this is important in highrisk domains such as healthcare intelligence where working of the extractors is opaque to the enduser
applications that rely on automatically constructed knowledge graphs
intelligence where users is especially crucial for applications
a simple yet effective and efficient solution that takes into account passage level to produce a ranked list of passages
we follow an approach propose a simple yet effective and efficient solution
passages describing a given input relation
a simple yet effective and efficient solution that takes into document level properties to produce a ranked list of passages
we follow an approach inspired from information retrieval
we test we approach using wikidata as the knowledge base
user studies conducted to study the effectiveness of we
we test we approach using wikipedia as the source corpus
report results of user studies proposed modelmany question answering systems over knowledge graphs rely on entity linking components in order to connect the natural language input to the underlying knowledge graph
many question answering systems over knowledge graphs rely on relation linking components in order to connect the natural language input to the underlying knowledge graph
traditionally entity have been performed either as independent parallel tasks
relation linking
traditionally relation have been performed either as independent parallel tasks
traditionally entity have been performed either as dependent sequential tasks
traditionally relation have been performed either as dependent sequential tasks
entity linking
a framework called earl
in this paper we propose relation
relation linking as a joint task
in this paper we propose a framework
entity linking
earl which performs entity
the joint entity and relation linking tasks as an instance of the generalised travelling salesman problem
earl implements two different solution strategies the first strategy is a formalisation of the joint entity and relation  gtsp 
two different solution strategies for which we provide a comparative analysis in this paper
in order to be computationally feasible we employ approximate gtsp solvers
the first strategy uses machine learning in order to exploit the connection density between nodes in the knowledge graph
the first strategy relies on reranking steps in order to predict relations
the first strategy relies on three base features in order to predict relations
the first strategy relies on reranking steps in order to predict entities
the first strategy relies on three base features in order to predict entities
we evaluate the strategies on a dataset with 5000 questions
we compare the strategies
the strategies significantly outperform the current stateoftheart approaches for relation linking
the strategies significantly outperform the current stateoftheart approaches for entity linkingontology summarization aspires to produce an abridged version of the original data source highlighting ontology summarization most important concepts
however in an ideal scenario the user should not be limited only to static summaries
the data source requesting more detailed information for a particular part of more detailed information
starting from the summary she should be able to further explore the data source
a new approach enabling the dynamic exploration of summaries through two novel operations extend
in this paper we present a new approach
a new approach enabling the dynamic exploration of summaries through two novel operations zoom
extend focuses on a specific subgraph of the initial summary whereas zoom on the whole graph
extend providing granular information access to the enduser
we show that calculating these operators is npcomplete
we provide approximations for these operators calculation
more queries focusing on specific nodes whereas using global zoom we can answer overall more queries
then we show that using extend we can answer more queries
finally we show that the algorithms can efficiently approximate both operators
the algorithms employedthis paper addresses the problem of fake news detection
there are many works already in this space however most of many works are using news content for the decision making
there are many works already in this space however most of many works are for social media
in this paper we propose some novel approaches including the btranse model to detecting fake news based on news content
news content using knowledge graphs
in our solutions our need to address a few technical challenges
all the relations needed for fake news detection
firstly computationaloriented fact checking is not comprehensive enough to cover all the relations
secondly it is challenging to validate the correctness of the extracted triples from news articles
our approaches are evaluated with the kaggles  getting some true articles from main stream media
our approaches are evaluated with the kaggles  getting real about fake news dataset
the evaluations show that some of our approaches have over 080 f1scoresconversational systems have become increasingly popular as a way for humans to interact with computers
to be able to provide intelligent responses conversational systems must correctly model the structure and semantics of a conversation
concepts introduced during a conversation
background knowledge which relies on the identification of semantic relations between concepts
we introduce the task of measuring semantic  in  coherence in a conversation with respect to background knowledge
we propose graphbased learningbased approaches for measuring semantic coherence using word embedding models as sources of background knowledge
we evaluate graphbased learningbased approaches for measuring semantic coherence using word embedding models as sources of background knowledge
we evaluate machine learningbased approaches for measuring semantic coherence using word embedding models as sources of background knowledge
we evaluate machine learningbased approaches for measuring semantic coherence using knowledge graphs as sources of background knowledge
we evaluate graphbased learningbased approaches for measuring semantic coherence using knowledge graphs vector space embeddings as sources of background knowledge
we propose graphbased learningbased approaches for measuring semantic coherence using knowledge graphs as sources of background knowledge
we propose machine learningbased approaches for measuring semantic coherence using knowledge graphs as sources of background knowledge
we propose graphbased learningbased approaches for measuring semantic coherence using knowledge graphs vector space embeddings as sources of background knowledge
we propose machine learningbased approaches for measuring semantic coherence using knowledge graphs vector space embeddings as sources of background knowledge
we evaluate machine learningbased approaches for measuring semantic coherence using knowledge graphs vector space embeddings as sources of background knowledge
we evaluate graphbased learningbased approaches for measuring semantic coherence using knowledge graphs as sources of background knowledge
we propose machine learningbased approaches for measuring semantic coherence using word embedding models as sources of background knowledge
we demonstrate how these approaches are able to uncover different coherence patterns in conversations on the ubuntu dialogue corpusin domains such as humanitarian assistance events rather than named entities are the primary focus of analysts
in domains such as disaster relief events rather than named entities are the primary focus of analysts
in domains such as humanitarian assistance events rather than named entities are the primary focus of aid officials
in domains such as disaster relief events rather than named entities are the primary focus of aid officials
subevents that refer to the same underlying event
an important problem that must be solved to provide situational awareness to aid providers
an important problem is automatic clustering of subevents
an effective solution to an important problem requires judicious use of statistical methods like deep neural embeddings
an effective solution to an important problem requires judicious use of both domainspecific information
an important problem that must be solved to provide situational awareness to aid providers
an effective solution to an important problem requires judicious use of both semantic information
in this paper we present an approach augmented feature sets for structured event entity resolution
augmented feature sets for structured event entity resolution that combines advances in deep neural embeddings both on text data with minimally supervised inputs from domain experts
augmented feature sets for structured event entity resolution that combines advances in deep neural embeddings both on graph data with minimally supervised inputs from domain experts
structured batch scenarios
structured event entity resolution can operate in both online
on five realworld disaster relief datasets event entity resolution is found on average to outperform the next best baseline result by almost 15percent on the cluster purity metric on the f1measure metric
five realworld disaster relief datasets structured
on five realworld disaster relief datasets event entity resolution is found on average to outperform the next best baseline result by almost 15percent on the cluster purity by 3percent on the f1measure metric
in contrast textbased approaches are found to perform poorly demonstrating the importance of semantic information in devising a good solution
we also use subevent clustering visualizations to illustrate the qualitative potential of structured event entity resolutionwe develop a declarative framework for privacypreserving linked data publishing in which utility policies are specified as sparql queries
we introduce a declarative framework for privacypreserving linked data publishing in which utility policies are specified as sparql queries
we develop a declarative framework for privacypreserving linked data publishing in which privacy policies are specified as sparql queries
we introduce a declarative framework for privacypreserving linked data publishing in which privacy policies are specified as sparql queries
we approach leads to inspect utility policies in order to determine the sequence of anonymization operations applicable to any graph instance for satisfying the policies
we approach is dataindependent
we approach leads to inspect only the privacy in order to determine the sequence of anonymization operations applicable to any graph instance for satisfying the policies
we prove the soundness of we algorithms
we gauge our algorithms performance through experimentsincreasingly organizations are adopting these ontologies to describe organizations large catalogues of items
these ontologies need to evolve regularly in response to changes in the domain
these ontologies need to evolve regularly in response to the emergence of new requirements
an important step of this process is the selection of candidate concepts to include in the new version of the ontology
this operation needs to take into account a variety of factors
in particular reconcile application performance
in particular reconcile user requirements
current ontology evolution methods focus either on ranking concepts according to current ontology evolution methods relevance
current ontology evolution methods focus either on preserving compatibility with existing applications
however current ontology evolution methods do not take in consideration the impact of the ontology evolution process on the performance of computational tasks similarity computation
however current ontology evolution methods do not take in consideration the impact of the ontology evolution process on the performance of computational tasks generation of recommendations
however current ontology evolution methods do not take in consideration the impact of the ontology evolution process on the performance of computational tasks data clustering
however current ontology evolution methods do not take in consideration the impact of the ontology evolution process on the performance of computational tasks in this work we focus on instance tagging
in this paper we propose the pragmatic ontology evolution framework a novel approach for selecting from a group of candidates a set of concepts able to produce a new version of a given ontology effectively supports relevant computational tasks
concepts able to produce a new version of a given ontology that is consistent with the a set of user requirements
in this paper we propose the pragmatic ontology evolution framework a novel approach for selecting from a group of candidates a set of concepts able to produce a new version of a given ontology is parametrised with respect to a number of dimensions
we approach also supports users in navigating the space of possible solutions by showing how certain choices such as privileging trendy concepts would reflect on the application performance
we approach also supports users in navigating the space of possible solutions by showing how certain choices such as privileging historical ones would reflect on the application performance
we approach also supports users in navigating the space of possible solutions by showing how certain choices such as limiting the number of concepts would reflect on the application performance
excellent results demonstrating a significant improvement over alternative approaches
an evaluation of the pragmatic ontology evolution on the realworld scenario of the evolving springer nature taxonomy for editorial classification yielded excellent resultsin the absence of a central naming authority on the semantic web it is common for different datasets to refer to the same thing by different iris
whenever multiple names are used to denote owl sameas statements are needed in order to link foster reuse
whenever multiple names are used to denote owl sameas statements are needed in order to link the data
whenever multiple names are used to denote the same thing  sameas statements are needed in order to link foster reuse
whenever multiple names are used to denote the same thing  sameas statements are needed in order to link the data
studies that date back as far as 2009
studies have observed that sameas property is sometimes used incorrectly
studies have observed that the owl is sometimes used incorrectly
in this paper we show how network metrics such as the community structure of the owl sameas property sameas graph can be used in order to detect such possibly erroneous statements
one benefit of the here presented approach is that the here presented approach can be applied to the network of owl sameas links the here presented approach
one benefit of the here presented approach is that the here presented approach can be applied to the network of owl sameas does not rely on any additional knowledge
in order to illustrate the here presented approach ability to scale the approach is evaluated on the largest collection of identity links to date containing over 558m owl sameas links scraped from the lod cloudcaching in the context of expressive query languages such as sparql is complicated by the difficulty of detecting equivalent queries deciding if two conjunctive queries are equivalent is npcomplete where adding further query features makes the problem undecidable
despite this complexity in this paper we propose an algorithm
an algorithm that performs syntactic canonicalisation of sparql queries such that the answers for the canonicalised query will not change versus the original
the canonicalised query
we can guarantee that the canonicalisation of two queries within a core fragment of sparql is equal if the two queries are equivalent we also support other sparql features but with a weaker soundness guarantee that the is equivalent to the input query
we can guarantee that the canonicalisation of two queries within a core fragment of sparql is equal if
despite the fact that canonicalisation must be harder than the equivalence problem we show an algorithm such that the answers for the canonicalised query will not change versus the original to be practical for realworld queries
an algorithm that performs syntactic canonicalisation of sparql queries
an algorithm that performs syntactic canonicalisation of sparql queries such that the answers for the canonicalised query will not change versus the original
realworld queries taken from sparql endpoint logs
despite the fact that canonicalisation must be harder than the equivalence problem we show that an algorithm detects more equivalent queries than when compared with purely syntactic methods
we also present the results of experiments over synthetic queries designed to stresstest the canonicalisation method highlighting difficult casesa number of real benchmarks have been proposed for evaluating the performance of link discovery systems
a number of synthetic benchmarks have been proposed for evaluating the performance of link discovery systems
so far only a limited number of link discovery benchmarks target the problem of linking geospatial entities
however some of the largest knowledge bases of the linked open data web such as linkedgeodata contain vast amounts of spatial information
several systems that consider the topology of the spatial resources
several systems that consider the topological relations between the spatial resources
several systems that manage spatial data
several systems have been developed
in order to consider the topological relations between them to perform the much needed data integration in the it is imperative to develop benchmarks for geospatial link discovery
in order to consider the topology of the spatial resources to handle the vast amount of spatial data it is imperative to develop benchmarks for geospatial link discovery
the linked geo data cloud
in order to assess the ability of several systems it is imperative to develop benchmarks for geospatial link discovery
in order to consider the topological relations between them to handle the vast amount of spatial data it is imperative to develop benchmarks for geospatial link discovery
several systems that manage spatial data
in order to consider the topology of the spatial resources to perform the much needed data integration in the it is imperative to develop benchmarks for geospatial link discovery
the spatial benchmark generator that can be used to test the performance of link discovery systems
in this paper we propose the spatial benchmark generator
link discovery systems which deal with topological relations as proposed in the state of the art de9im  dimensionally extended nineintersection model 
the spatial benchmark generator implements all topological relations of de9im between linestrings in the twodimensional space
the spatial benchmark generator implements all topological relations of de9im between polygons in the twodimensional space
a comparative analysis with benchmarks is provided
benchmarks produced using the spatial benchmark generator to identify the capabilities of aml spatial link discovery systems
benchmarks produced using the spatial benchmark generator to assess the capabilities of silk spatial link discovery systems
benchmarks produced using the spatial benchmark generator to assess the capabilities of aml spatial link discovery systems
benchmarks produced using the spatial benchmark generator to assess the capabilities of radon spatial link discovery systems
benchmarks produced using the spatial benchmark generator to assess the capabilities of ontoidea spatial link discovery systems
benchmarks produced using the spatial benchmark generator to identify the capabilities of radon spatial link discovery systems
benchmarks produced using the spatial benchmark generator to identify the capabilities of ontoidea spatial link discovery systems
benchmarks produced using the spatial benchmark generator to identify the capabilities of silk spatial link discovery systemswe give an operational semantics to the ontology via a rule language
we present an ontology for representing workflows over components with readwrite linked data interfaces
workflow languages have been successfully applied for modelling behaviour in enterprise information systems
enterprise information systems in which the data is often managed in a relational database
scenarios involving the internet of things
linked data interfaces have been widely deployed on the web to support data integration in very diverse domains increasingly also in scenarios
things in which application behaviour is often specified using imperative programming languages
linked data which integrated data access
with we work we aim to combine workflow languages which allow for the highlevel specification of application behaviour by nonexpert users with linked data
linked data which allows for decentralised data publication
we show that we ontology is expressive enough to cover the basic workflow patterns
we demonstrate the applicability of we approach with a prototype system
pilots carrying out tasks in a virtual reality aircraft cockpit
a prototype system that observes pilots
on a synthetic benchmark from the building automation domain the runtime scales linearly with the size of the number of internet of things devices
