a caching approach based on a provenanceaware partitioning of RDF graphs
a common evaluation that includes more than one type of method
A common query problem for users is to face with empty answers given a SPARQL query
A common workaround is the generation of labeled training data from knowledge bases this approach is not suitable for longtail entity types
A comparative analysis with benchmarks is provided
a continuous vector space by an entity context preserving translational embedding model
a dataset extracted from StackOverflowCloudbased systems provide a rich platform for managing largescale RDF data
a deep learning based solution for Automated Knowledge Base Population from Text
a finegrained evaluation that gives insight into characteristics of the most popular datasets out the different strengths and shortcomings of the examined approaches
a finegrained evaluation that gives insight into characteristics of the most popular points out the different strengths and shortcomings of the examined approaches
a framework called EARL
a framework that collects numerous extractors responses
a large RDF repository containing millions of triples
a large RDF repository containing millions of triples
all the relations needed for fake news detection
a materialisation strategy that performs an offline analysis of the input graph in order to identify a subset of the exponential number of possible facet combinations
a model that exploits the knowledge
an algorithm that performs syntactic canonicalisation of SPARQL queries
an algorithm that performs syntactic canonicalisation of SPARQL queries such that the answers for the canonicalised query will not change versus the original
an algorithm that performs syntactic canonicalisation of SPARQL queries such that the answers for the canonicalised query will not change versus the original
an approach that allows preprocessing large tabular data in Datalog without indexing the data
An effective solution to An important problem requires judicious use of both domainspecific information
An effective solution to An important problem requires judicious use of both semantic information
An effective solution to An important problem requires judicious use of statistical methods like deep neural embeddings
An evaluation of the Pragmatic Ontology Evolution on the realworld scenario of the evolving Springer Nature taxonomy for editorial classification yielded excellent results
a new approach enabling the dynamic exploration of summaries through two novel operations extend
a new approach enabling the dynamic exploration of summaries through two novel operations zoom
an extensive evaluation of the system across three different benchmarks with different characteristics showing that We unified framework can consistently outperform state of the art solutions
An important problem is automatic clustering of subevents
An important problem that must be solved to provide situational awareness to aid providers
An important problem that must be solved to provide situational awareness to aid providers
An important step of this process is the selection of candidate concepts to include in the new version of the ontology
an initial seed ontology which may already be in use by an application
annotations from different tasks on the same mention have to share less the same implied entity classes
annotations from different tasks on the same mention have to share more the same implied entity classes
an online fashion using Bloom filters
a novel approach based on composite contexts to acquire implicit relations from Title Oriented Documents
a novel finegrained approach which formalise structural incompatibilities
a novel finegrained approach which is based on mapping alignment conservativity
a novel finegrained approach which is based on mapping repair conservativity
a novel finegrained approach which provide an exact
a novel finegrained approach which provide approximate algorithms
a novel finegrained approach which provide practical algorithms
a novel Information Extraction framework named QA4IE
a novel Information Extraction framework named QA4IE
an RDF graph embedding based framework to solve the SPARQL emptyanswer problem in terms of a continuous vector space
A number of real benchmarks have been proposed for evaluating the performance of link discovery systems
applications that rely on automatically constructed knowledge graphs
approximation techniques proposed in a relational database
a Probabilistic Soft Logic model that leverages ontological entity classes to relate NLP annotations from different tasks
a Probabilistic Soft Logic  model that leverages ontological entity classes to relate NLP annotations from different tasks
a Probabilistic Soft Logic  model that leverages ontological entity classes to relate NLP annotations from different tasks
a Probabilistic Soft Logic  model that leverages ontological entity classes to relate NLP annotations from different tasks
a Probabilistic Soft Logic  model that leverages ontological entity classes to relate NLP annotations from different tasks
a Probabilistic Soft Logic  model that leverages ontological entity classes to relate NLP annotations from different tasks
a Probabilistic Soft Logic  model that leverages ontological entity classes to relate NLP annotations from different tasks
a Probabilistic Soft Logic  model that leverages ontological entity classes to relate NLP annotations from different tasks
a Probabilistic Soft Logic  model that leverages ontological entity classes to relate NLP annotations from different tasks
A prominent example of such usage can be found in the Wikidata dataset where the author of Beowulf is given as a blank node
a prototype system that observes pilots
a regularized multitask learning of document embeddings
a regularized multitask learning of document embeddings
a regularized multitask learning of document embeddings
a regularized multitask learning of Knowledge Base
a regularized multitask learning of Knowledge Base
a regularized multitask learning of Knowledge Base
a rule learning method that utilizes probabilistic representations of missing facts
As a result of the training process the system learns how to identify implicit relations between entities across a highly heterogeneous set of documents from various sources making the system suitable for largescale knowledge extraction from Web documents
a setting with various NLP tools returning multiple confidenceweighted candidate annotations on a single mention
a simple yet effective and efficient solution that takes into account passage level to produce a ranked list of passages
a simple yet effective and efficient solution that takes into document level properties to produce a ranked list of passages
a solution based on a
a solution based on a
a solution based on a
a solution can potentially incorporate any Knowledge Base
a solution can potentially incorporate document embedding learning method
a SPARQL query retrieves a number of Java source code snippets and methods ranked by Open image in new window on both syntactic to find the best candidate
a SPARQL query retrieves a number of Java source code snippets and methods ranked by Open image in new window on semantic features to find the best candidate
a SPARQL query that compute approximate answers by leveraging RDF embeddings
a SPARQL query that compute approximate answers by leveraging the translation mechanism
a SPARQL query that is run against a large RDF repository
a SPARQL query that is run against a large RDF repository
a SPARQL query that is run against CodeOntology
a SPARQL query that is run against CodeOntology
a SPARQL query that returns an empty set we partition an empty set into several parts
a SPARQL query that returns nothing how to refine the query to obtain a nonempty set
a succinct translation of a SPARQL fragment fully respects bag semantics and relies on the extensive use of the LEFT JOIN COALESCE function
a succinct translation of a SPARQL fragment fully respects bag semantics and relies on the extensive use of the LEFT JOIN operator function
a succinct translation of a SPARQL fragment fully respects threevalued logic and relies on the extensive use of the LEFT JOIN COALESCE function
a succinct translation of a SPARQL fragment fully respects threevalued logic and relies on the extensive use of the LEFT JOIN operator function
a textbased Knowledge Graphs embedding model named Typed Entity Embeddings
At the same time there are other completion tasks
Augmented feature sets for Structured Event Entity Resolution that combines advances in deep neural embeddings both on graph data with minimally supervised inputs from domain experts
Augmented feature sets for Structured Event Entity Resolution that combines advances in deep neural embeddings both on text data with minimally supervised inputs from domain experts
a vector that represents the entity mentions found in a text corpus
a vector that represents the entity type mentions found in a text corpus
background knowledge which relies on the identification of semantic relations between concepts
Based on a novel Information Extraction framework we develop a large Information Extraction benchmark with high quality human evaluation
Based on a realization of the proposed optimizations on top of Spark extensive experimentation demonstrates how the proposed optimizations on top of Spark lead to an order of magnitude enhancement in terms of preprocessing compared to the stateoftheart solutions
Based on a realization of the proposed optimizations on top of Spark extensive experimentation demonstrates how the proposed optimizations on top of Spark lead to an order of magnitude enhancement in terms of query performance compared to the stateoftheart solutions
Based on a realization of the proposed optimizations on top of Spark extensive experimentation demonstrates how the proposed optimizations on top of Spark lead to an order of magnitude enhancement in terms of storage compared to the stateoftheart solutionsfacts stored in a knowledge graph
Based on the assumption that reliable sources provide true claims Truth Discovery models iteratively compute source trustworthiness in order to determine which claims are true
Based on the assumption that reliable sources provide true claims Truth Discovery models iteratively compute value confidence in order to determine which claims are true
Based on the assumption that true claims are provided by reliable sources Truth Discovery models iteratively compute source trustworthiness in order to determine which claims are true
Based on the assumption that true claims are provided by reliable sources Truth Discovery models iteratively compute value confidence in order to determine which claims are true
Based on the query workload reduced sets of intermediate results join patterns are computed
benchmarks produced using the Spatial Benchmark Generator to assess the capabilities of AML spatial link discovery systems
benchmarks produced using the Spatial Benchmark Generator to assess the capabilities of OntoIdea spatial link discovery systemsWe give an operational semantics to the ontology via a rule language
benchmarks produced using the Spatial Benchmark Generator to assess the capabilities of RADON spatial link discovery systems
benchmarks produced using the Spatial Benchmark Generator to assess the capabilities of Silk spatial link discovery systems
benchmarks produced using the Spatial Benchmark Generator to identify the capabilities of AML spatial link discovery systems
benchmarks produced using the Spatial Benchmark Generator to identify the capabilities of OntoIdea spatial link discovery systems
benchmarks produced using the Spatial Benchmark Generator to identify the capabilities of RADON spatial link discovery systems
benchmarks produced using the Spatial Benchmark Generator to identify the capabilities of Silk spatial link discovery systems
Benfords law which indicates the distribution
Blank nodes in RDF graphs can be used to represent values whose identity remains unknown
combining NLP  tasks such as Named Classification  to identify output may result in even conflicting
combining NLP  tasks such as Named Classification  to identify output may result in NLP annotations
combining NLP  tasks such as Named Entity Linking  to identify output may result in even conflicting
combining NLP  tasks such as Named Entity Linking  to identify output may result in even conflicting
combining NLP  tasks such as Named Entity Linking  to identify output may result in NLP annotations
combining NLP  tasks such as Named Entity Linking  to identify output may result in NLP annotations
combining NLP  tasks such as Named Entity Recognition and Classification  to identify output may result in even conflicting
combining NLP  tasks such as Named Entity Recognition and Classification  to identify output may result in NLP annotations
combining NLP  tasks such as Named Entity Recognition  to identify output may result in even conflicting
combining NLP  tasks such as Named Entity Recognition  to identify output may result in NLP annotations
combining wellknown Natural Language Processing  tasks such as Named Classification  to identify output may result in even conflicting
combining wellknown Natural Language Processing  tasks such as Named Classification  to identify output may result in NLP annotations
combining wellknown Natural Language Processing  tasks such as Named Entity Linking  to identify output may result in even conflicting
combining wellknown Natural Language Processing  tasks such as Named Entity Linking  to identify output may result in even conflicting
combining wellknown Natural Language Processing  tasks such as Named Entity Linking  to identify output may result in NLP annotations
combining wellknown Natural Language Processing  tasks such as Named Entity Linking  to identify output may result in NLP annotations
combining wellknown Natural Language Processing  tasks such as Named Entity Recognition and Classification  to identify output may result in even conflicting
combining wellknown Natural Language Processing  tasks such as Named Entity Recognition and Classification  to identify output may result in NLP annotations
combining wellknown Natural Language Processing  tasks such as Named Entity Recognition  to identify output may result in even conflicting
combining wellknown Natural Language Processing  tasks such as Named Entity Recognition  to identify output may result in NLP annotations
Common Information Extraction solutions are severely restricted by informal relation specifications
Common Information Extraction solutions are severely restricted by limited relation types
Common Information Extraction solutions can hardly handle crosssentence tuples
completion tasks that can be solved by a rulebased approach with high precision
concepts able to produce a new version of a given ontology that is consistent with the a set of user requirements
concepts introduced during a conversation
constraints that reference other ones
Converting data from diverse data sources to custom RDF datasets often faces several practical challenges related with the need to restructure
counterintuitive results which may make the standard SPARQL semantics unsuitable for datasets with existential blank nodes
Current methods for classifying the relevance of posts to a crisis or set of crises is not viable during rapidly evolving crisis situations to train new models for each language
Current methods for classifying the relevance of posts to a crisis or set of crises typically struggle to deal with posts in different languages
Current ontology evolution methods focus either on preserving compatibility with existing applications
Current ontology evolution methods focus either on ranking concepts according to Current ontology evolution methods relevance
dependency embeddings engineered to exploit distinctive traits of research publications
Despite being intrinsically related
Despite the fact that canonicalisation must be harder than the equivalence problem we show an algorithm such that the answers for the canonicalised query will not change versus the original to be practical for realworld queries
Despite the fact that canonicalisation must be harder than the equivalence problem we show that an algorithm detects more equivalent queries than when compared with purely syntactic methods
Despite this complexity in this paper we propose an algorithm
different tasks insisting on the same entity
different tasks insisting on the same entity
different tasks insisting on the same entity
different tasks insisting on the same entity consistently
different tasks insisting on the same entity with the candidate annotations
Digitised historical newspapers add an additional challenge namely that of fluctuations in OCR quality
Due to the sheer volume of data typically circulated during such events it is necessary to be able to efficiently filter out irrelevant posts thus focusing attention on the posts
EARL implements two different solution strategies The first strategy is a formalisation of the joint entity and relation  GTSP 
EARL which performs entity
Empirical experiments on realworld datasets showed the potential of the proposed approachprocess natural language questions that can not be answered by factual question answering querying requiring instead adhoc code generation and execution
Enhancing Truth Discovery models efficiently obtains a larger set of reliable facts that vice versa can populate RDF RDF Knowledge Bases
ENND  using specific gold standards
ENNTR  using specific gold standards
Ensemble Neural Network for Disambiguation  using specific gold standards
Ensemble Neural Network for Type Recognition  using specific gold standards
enterprise information systems in which the data is often managed in a relational database
entities using different taxonomies with different knowledge bases
entities using disambiguate web APIs with different knowledge bases
entity linking
entity linking
Especially in the biomedical domain several sources like more are distributed in the form of OWL ontologies
Especially in the biomedical domain several sources like SNOMED NCI FMA are distributed in the form of OWL ontologies
Evaluation on datasets from three disciplines Bioinformatics shows very promising performance
Evaluation on datasets from three disciplines Digital Humanities shows very promising performanceOPTIONAL is a key feature in SPARQL for dealing with missing information
Evaluation on datasets from three disciplines Medicine shows very promising performance
even conflicting considering common world knowledge about entities
excellent results demonstrating a significant improvement over alternative approachesIn the absence of a central naming authority on the Semantic Web it is common for different datasets to refer to the same thing by different IRIs
Existing RDF mapping languages assume that the resulting datasets mostly preserve the structure of the source data
experimental results show that our approach is comparable with other stateoftheart proprietary systems such as the closedsource WolframAlpha computational knowledge engine
Experimental results show that the linear order of years is highly correlated with natural time flow
Experimental results show that the linear order of years is highly correlated with the effectiveness of the timeaware similarity measureInformation extraction traditionally focuses on extracting relations between identifiable entities
Experiments on realworld KGs demonstrate the effectiveness of our novel approach both with respect to the quality of fact predictions that Experiments on realworld KGs produce
Experiments on realworld KGs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rulesKnowledge Base Population is an important problem in Semantic Web research
Experiments show that our unsupervised method applies to a large number of relations
Extend focuses on a specific subgraph of the initial summary whereas zoom on the whole graph
Extend providing granular information access to the enduser
extensive experimentation using a real dataset
extensive experimentation using two synthetic benchmarks
Faceted browsing has also been investigated in RDF
facts that have to be added in order to make the Knowledge base representative of the real world
facts  when conflicting claims are made by several sources
Finally Our also compare with stateoftheart ontology integration systems
Finally Our also perform an experimental evaluation
Finally We empirically verify effectiveness of We techniques on the BSBM OBDA benchmarkKnowledge bases such as YAGO contain a huge number of entities
Finally We show that the algorithms can efficiently approximate both operators
Firstly computationaloriented fact checking is not comprehensive enough to cover all the relations
five realworld Disaster Relief datasets Structured
For example recipes highlight how we ate about food
For example recipes highlight how we thought about food
For example recipes highlight what
Formal relations defined over templates
Formal relations support sophisticated maintenance tasks for sets of templates such as revealing redundancies
Formal relations support sophisticated maintenance tasks for sets of templates such as suggesting new templates for representing implicit patterns
For numerical relations the estimated representativeness proves to be a reliable indicator
For this we use the
Furthermore these reductions are not computed beforehand
Furthermore these reductions are rather computed only for the frequent join patterns in an online fashion
heterogeneity where large numbers of classes generate many facets
heterogeneity where large numbers of properties generate many facets
heterogeneous largescale RDF graphs based on a materialisation strategy
Historical newspapers provide a lens on habits of the past
However an important issue is that the structure of OWL ontologies may be profoundly different hence using the mappings as initially computed can lead to changes in OWL ontologies original structure
However an important issue is that the structure of OWL ontologies may be profoundly different hence using the mappings as initially computed can lead to incoherences in OWL ontologies original structure
However current faceted browsers for RDF graphs encounter heterogeneity
However current faceted browsers for RDF graphs encounter performance issues
However current faceted browsers for RDF graphs encounter scale
However Current ontology evolution methods do not take in consideration the impact of the ontology evolution process on the performance of computational tasks data clustering
However Current ontology evolution methods do not take in consideration the impact of the ontology evolution process on the performance of computational tasks eg in this work we focus on instance tagging
However Current ontology evolution methods do not take in consideration the impact of the ontology evolution process on the performance of computational tasks generation of recommendations
However Current ontology evolution methods do not take in consideration the impact of the ontology evolution process on the performance of computational tasks similarity computation
However in an ideal scenario the user should not be limited only to static summaries
However representation of time has received little attention in these approaches
However some of the largest knowledge bases of the Linked Open Data Web such as LinkedGeoData contain vast amounts of spatial information
However the distributed nature of Cloudbased systems introduces several performance challenges especially for RDF queries
However while SPARQL considers blank nodes in a query as existentials the author of Beowulf treats blank nodes in RDF data more like constants
In addition we introduce an efficient solution for RDF queries with unbound properties
In a setting with various NLP tools a Probabilistic Soft Logic  model can be operationally applied to compare the different annotation combinations
In a setting with various NLP tools a Probabilistic Soft Logic  model can be operationally applied to compare the different annotation combinations
In a setting with various NLP tools a Probabilistic Soft Logic  model can be operationally applied to possibly revise the tools best annotation choice
In a setting with various NLP tools a Probabilistic Soft Logic  model can be operationally applied to possibly revise the tools best annotation choice
In contrast textbased approaches are found to perform poorly demonstrating the importance of semantic information in devising a good solution
In domains such as Disaster Relief events rather than named entities are the primary focus of aid officials
In domains such as Humanitarian Assistance events rather than named entities are the primary focus of aid officials
In domains such as Humanitarian Assistance events rather than named entities are the primary focus of analysts
In experiments over Wikidata we demonstrate that materialisation allows for displaying faceted views over millions of diverse results in under a second while keeping index sizes relatively small
information extraction that aim to assign them predefined types
information extraction that aim to recognize named entities
In order to assess the ability of Several systems it is imperative to develop benchmarks for geospatial link discovery
In order to be computationally feasible we employ approximate GTSP solvers
In order to consider the topological relations between them to handle the vast amount of spatial data it is imperative to develop benchmarks for geospatial link discovery
In order to consider the topological relations between them to perform the much needed data integration in the it is imperative to develop benchmarks for geospatial link discovery
In order to consider the topology of the spatial resources to handle the vast amount of spatial data it is imperative to develop benchmarks for geospatial link discovery
In order to consider the topology of the spatial resources to perform the much needed data integration in the it is imperative to develop benchmarks for geospatial link discovery
In order to estimate the performance costs that would be associated with such a change in semantics for current implementations we adapt approximation techniques setting for a core fragment of SPARQL
In order to estimate the performance costs that would be associated with such a change in semantics for current implementations we evaluate approximation techniques setting for a core fragment of SPARQL
In order to illustrate the here presented approach ability to scale the approach is evaluated on the largest collection of identity links to date containing over 558M owl sameAs links scraped from the LOD CloudCaching in the context of expressive query languages such as SPARQL is complicated by the difficulty of detecting equivalent queries deciding if two conjunctive queries are equivalent is NPcomplete where adding further query features makes the problem undecidable
In order to overcome these weaknesses we propose a novel Information Extraction framework
In our solutions our need to address a few technical challenges
in particular reconcile application performance
in particular reconcile user requirements
In particular we iteratively extend rules
Inspired by evidence from applicationoriented concerns we propose an approach to encode representations of years into Typed Entity Embeddings by aggregating the representations of the entities
Inspired by evidence from cognitive sciences we propose an approach to encode representations of years into Typed Entity Embeddings by aggregating the representations of the entities
Instead the solution exploits a large corpus of text documents to train a set of deep neural network models
Instead the solution exploits a partially populated knowledge base to train a set of deep neural network models
intelligence where users is especially crucial for applications
intelligence where users need additional information for decision making
intermediate results that are common for certain
In this article
In this paper we describe a framework combines numerous extractors responses in order to produce a final entity list
In this paper we describe a framework normalizes numerous extractors responses
In this paper we describe Ensemble Nerd combines numerous extractors responses in order to produce a final entity list
In this paper we describe Ensemble Nerd normalizes numerous extractors responses
In this paper we investigate the benefit of caching some parts of an RDF cube augmented with provenance information
In this paper we present a deep learning
In this paper we present a framework approach for integrating independently developed ontologies
In this paper we present an approach
In this paper we present an approach Augmented feature sets for Structured Event Entity Resolution
In this paper we present a new approach
In this paper we present a novel approach for integrating independently developed ontologies
In this paper we present a Probabilistic Soft Logic model mentions
In this paper we present real cases by incorporating a programming flavor in the mapping process
In this paper we present Socrates
In this paper we propose a framework
In this paper we propose an RDF graph embedding
In this paper we propose relation
In This paper we propose some novel approaches including the BTransE model to detecting fake news based on news content
In this paper we propose the Pragmatic Ontology Evolution framework a novel approach for selecting from a group of candidates a set of concepts able to produce a new version of a given ontology effectively supports relevant computational tasks
In this paper we propose the Pragmatic Ontology Evolution framework a novel approach for selecting from a group of candidates a set of concepts able to produce a new version of a given ontology is parametrised with respect to a number of dimensions
In this paper we propose the Spatial Benchmark Generator
In this paper we show how network metrics such as the community structure of the owl sameAs property sameAs graph can be used in order to detect such possibly erroneous statements
In this paper we test semantic classification approaches on crosslingual datasets from 30 crisis events consisting of posts
In this paper we test semantic classification approaches on crosslingual datasets from 30 crisis events Italian
In this paper we test statistical classification approaches on crosslingual datasets from 30 crisis events consisting of posts
In this paper we test statistical classification approaches on crosslingual datasets from 30 crisis events Italian
In this work we make a first step to encode time into vectorbased entity representations
In Typed Entity Embeddings each entity is represented by a vector
KADE can potentially incorporate any Knowledge Base
KADE can potentially incorporate document embedding learning method
Knowledge Base Population is a key requirement for successful adoption of semantic technologies in many applications
Knowledge Bases contain complementary information about realworld objects
Knowledge Bases contain relations among Knowledge Bases 
Knowledge Bases contain relations among Knowledge Bases 
Knowledge Bases contain relations among textual documents
Knowledge Bases contain rich information about realworld objects
Knowledge bases such as DBpedia contain a huge number of entities
Knowledge bases such as DBpedia contain a huge number of facts
Knowledge bases such as Wikidata contain a huge number of entities
Knowledge bases such as Wikidata contain a huge number of facts
Knowledge bases such as YAGO contain a huge number of facts
knowledge expressed in RDF Knowledge Bases to enhance Truth Discovery performances
Leveraging the explanatory qualities of rulebased systems we present a finegrained evaluation
link discovery systems which deal with topological relations as proposed in the state of the art DE9IM  Dimensionally Extended nineIntersection Model 
Linked Data interfaces have been widely deployed on the web to support data integration in very diverse domains increasingly also in scenarios
Linked Data which allows for decentralised data publication
Linked Data which integrated data access
longtail entities such as the ones are rare often relevant only in specific knowledge domains yet important for exploration purposes
longtail entities such as the ones are rare often relevant only in specific knowledge domains yet important for retrieval purposes
longtail entity types that are by definition scarcely represented in KBs
macro F1 measures computed by the GERBIL framework
Main contributions of this paper include an architecture for unifying relation extraction
Main contributions of this paper include a novel approach
manual annotations which would make the solution hard to adapt to a new domain
Many approaches for Knowledge Extraction rely on NLP  tasks such as Named Entity Linking to identify
Many approaches for Knowledge Extraction rely on NLP  tasks such as Named Entity Recognition and Classification to identify
Many approaches for Knowledge Extraction rely on wellknown Natural Language Processing  tasks such as Named Entity Linking to identify
Many approaches for Knowledge Extraction rely on wellknown Natural Language Processing  tasks such as Named Entity Recognition and Classification to identify
Many approaches for Knowledge Extraction semantically characterize the entities
Many approaches for Ontology Population rely on NLP  tasks such as Named Entity Linking to identify
Many approaches for Ontology Population rely on NLP  tasks such as Named Entity Recognition and Classification to identify
Many approaches for Ontology Population rely on wellknown Natural Language Processing  tasks such as Named Entity Linking to identify
Many approaches for Ontology Population rely on wellknown Natural Language Processing  tasks such as Named Entity Recognition and Classification to identify
Many approaches often exposed as web APIs have been proposed to solve these tasks during the last years
Many citizens nowadays flock to social media during crises to share the latest information about the event
Many question answering systems over knowledge graphs rely on relation linking components in order to connect the natural language input to the underlying knowledge graph
micro F1 measures computed by the GERBIL frameworkInformation Extraction refers to automatically extracting structured relation tuples from unstructured texts
models trained on expensive typelabeled data laboriously produced by human annotators
Moreover one important factor of query answering on Web data is Web data provenance
more queries focusing on specific nodes whereas using global zoom we can answer overall more queries
Most of these methods are based on the assumption that the data is a representative sample of the studied universe
Motivated by these insights we combine both families of approaches via ensemble learning
multiple queries that share the same join patterns
named entities mentioned in text
Named entity recognition and disambiguation are subtasks of information extraction
Named Entity Recognition and Typing is a challenging task especially with longtail entities such as the ones
news content using knowledge graphs
NLP annotations that are implausible
numerical relations where ground truths existMany approaches for Ontology Population semantically characterize the entities
On a synthetic benchmark from the building automation domain the runtime scales linearly with the size of the number of Internet of Things devices
One benefit of the here presented approach is that the here presented approach can be applied to the network of owl sameAs does not rely on any additional knowledge
One benefit of the here presented approach is that the here presented approach can be applied to the network of owl sameAs links the here presented approach
One of the most promising schema languages for RDF as an independent data is a recent W3C recommendation
One of the most promising schema languages for RDF as an independent data is SHACL 
On five realworld Disaster Relief datasets Event Entity Resolution is found on average to outperform the next best baseline result by almost 15percent on the cluster purity by 3percent on the F1Measure metric
On five realworld Disaster Relief datasets Event Entity Resolution is found on average to outperform the next best baseline result by almost 15percent on the cluster purity metric on the F1Measure metric
Ontology templates are designed for practical use an Open image for terse specification of bulk instances are available including an open source implementation for using templates
Ontology templates are designed for practical use an Open image for terse specification of template definitions are available including an open source implementation for using templates
Ontology templates are designed for practical use an Open image in new window vocabulary convenient serialisation formats for the semantic web are available including an open source implementation for using templates
Ontology templates are powerful abstractions useful for constructing
Ontology templates are powerful abstractions useful for interacting with
Ontology templates are powerful abstractions useful for maintaining ontologies
Ontology templates are simple abstractions useful for constructing
Ontology templates are simple abstractions useful for interacting with
Ontology templates are simple abstractions useful for maintaining ontologies
open Information Extraction systems are severely restricted by informal relation specifications
open Information Extraction systems are severely restricted by limited relation types
open Information Extraction systems can hardly handle crosssentence tuples
original structure which may affect applications
other completion tasks that are difficult for rulebased systems
other ones which in practice may easily yield reference cycles
our approaches are evaluated with the Kaggles  Getting Real about Fake News dataset
our approaches are evaluated with the Kaggles  Getting some true articles from main stream media
Our approach is successfully tested on a realworld largescale ontology in the engineering domainKnowledge Graphs are widely used abstractions to represent entitycentric knowledge
Our framework has already been used to integrate a number of medical ontologies provided by Babylon Health
Our framework has already been used to integrate a number of support realworld healthcare services provided by Babylon Health
Our results show that models such as HolE have problems in solving certain types of completion tasks
Our results show that models such as RESCAL have problems in solving certain types of completion tasks
Our results show that models such as TransE have problems in solving certain types of completion tasks
Our results support our assumption that the two methods complement each other in a beneficial wayConverting data from diverse data sources to custom RDF datasets often faces several practical challenges transform the source data
partofspeech embeddings engineered to exploit distinctive traits of research publications
passages describing a given input relation
performance issues when faced with two challenges
pilots carrying out tasks in a virtual reality aircraft cockpit
possible facet combinations that are candidates for indexing
posts written mainly in English Spanish
process natural language questions that can not be answered by advanced data querying requiring instead adhoc code generation and execution
provenance information when answering provenanceaware SPARQL queries
QA4IE which leverages the flexible question
QA4IE which leverages the flexible question answering  approaches to produce high quality relation triples across sentences
QA4IE which leverages the flexible question answering  approaches to produce high quality relation triples across sentences
queries that impose constraints on its provenance
Query processing in such settings is challenging because SPARQL Online Analytical Processing  queries usually contain many triple patterns with aggregation
Query processing in such settings is challenging because SPARQL Online Analytical Processing  queries usually contain many triple patterns with aggregation
Query processing in such settings is challenging because SPARQL Online Analytical Processing  queries usually contain many triple patterns with grouping
Query processing in such settings is challenging because SPARQL Online Analytical Processing  queries usually contain many triple patterns with grouping
Rather than caching the final results of each query we show that caching these reductions allows reusing intermediate results across multiple queries
RDF queries that involve multiple join operations
real cases that highlight the limitations of existing languages and describe a transformationoriented RDF mapping language which addresses such practical needsvalues known to exist but
real cases that highlight the limitations of existing languages and describe D2RML addresses such practical needs
realworld queries taken from SPARQL endpoint logs
recipes published in newspapers
Relation Extraction are severely restricted by informal relation specifications
Relation Extraction are severely restricted by limited relation types
Relation Extraction can hardly handle crosssentence tuples
relation linking
relation linking as a joint task
Remarkably Socrates ranked first in both attribute validation track at the Semantic Web Challenge at ISWC 2017
Remarkably Socrates ranked first in both the knowledge base population at the Semantic Web Challenge at ISWC 2017information extraction that aim to link them with them
report results of user studies proposed model
research publications written in English
returned answers which helps users recognize users expectations
returned answers which helps users refine the original query finally
rules induced from a a Knowledge Graph by relying on feedback from a precomputed embedding model over external information sources including text corpora
rules induced from a a Knowledge Graph by relying on feedback from a precomputed embedding model over the a Knowledge Graph including text corpora
Rules over various methods for rule learning have been proposed
Running SPARQL queries over datasets with unknown values may thus lead to counterintuitive results
scalability dictates that only a small set of candidate rules could be generated
scale where large datasets generate many results
scenarios involving the Internet of Things
scientific publications that relies on minimal human input namely a small seed set of instances for the targeted entity type
Secondly it is challenging to validate the correctness of the extracted triples from news articles
semantic features extracted from external knowledge basesFaceted browsing has become a popular paradigm for user interfaces on the Web
sequences are associated with other relevant information from publication metadata
sequences stored as RDF triples in a knowledge base
setting where the data is exposed as a virtual RDF graph by means of an R2RML mapping
setting where the data is stored in a SQL relational database
several optimization techniques that enhance the performance of RDF queries
Several recent works calculate statistics on Knowledge bases  such as DBpedia
Several recent works calculate statistics on Knowledge bases  such as DBpedia
Several recent works calculate statistics on Knowledge bases  such as Wikidata
Several recent works calculate statistics on Knowledge bases  such as Wikidata
Several recent works calculate statistics on Knowledge bases  such as YAGO
Several recent works calculate statistics on Knowledge bases  such as YAGO
Several recent works induce rules
several sliding a twostage pipeline classifier
several sliding window classifiers
Several systems have been developed
Several systems that consider the topological relations between the spatial resources
Several systems that consider the topology of the spatial resources
Several systems that manage spatial data
Several systems that manage spatial data
Similarly rulebased systems have been studied for this task in the past
Since KGs are inherently incomplete rules can be used to deduce missing facts
Socrates does not require manual annotations
So far only a limited number of link discovery benchmarks target the problem of linking geospatial entities
So it is difficult to learn highquality rules from the a Knowledge Graph alone
Some applications in access control require to augment the data with provenance metadata
Some applications in access control require to run queries
Some applications in data analytics require to augment the data with provenance metadata
Some applications in data analytics require to run queries
Starting from an initial seed ontology new sources are used to iteratively enrich the seed one
Starting from an initial seed ontology new sources are used to iteratively extend the seed one
Starting from the summary she should be able to further explore the data source
Stateoftheart NER approaches employ supervised machine
stateoftheart ontology integration systems that take into account the structure and coherency of the integrated ontologies obtaining encouraging resultsRules over a Knowledge Graph capture interpretable patterns in data have been proposed
Statistical measures for learned rules such as confidence reflect rule quality well when the a Knowledge Graph is reasonably complete however Statistical measures for learned rules such as confidence might be misleading otherwise
Structured batch scenarios
Structured Event Entity Resolution can operate in both online
Studies have observed that sameAs property is sometimes used incorrectly
Studies have observed that the owl is sometimes used incorrectly
Studies that date back as far as 2009
subevents that refer to the same underlying event
Such counting quantifiers are neglected by prior workToday a wealth of knowledge are distributed using Semantic Web standards
Such counting quantifiers can help in a variety of tasks such as knowledge base curation
Such counting quantifiers can help in a variety of tasks such as query answering
supervised machine learning models
textual documents contain complementary information about realworld objects
textual documents contain relations among Knowledge Bases 
textual documents contain relations among Knowledge Bases 
textual documents contain relations among textual documents
textual documents contain rich information about realworld objects
the algorithms employedThis paper addresses the problem of fake news detection
the analyses differ
the analyses performed by NLP  tasks such as Named Classification  to identify
the analyses performed by NLP  tasks such as Named Entity Linking  to identify
the analyses performed by NLP  tasks such as Named Entity Linking  to identify
the analyses performed by NLP  tasks such as Named Entity Recognition and Classification  to identify
the analyses performed by NLP  tasks such as Named Entity Recognition  to identify
the analyses performed by wellknown Natural Language Processing  tasks such as Named Classification  to identify
the analyses performed by wellknown Natural Language Processing  tasks such as Named Entity Linking  to identify
the analyses performed by wellknown Natural Language Processing  tasks such as Named Entity Linking  to identify
the analyses performed by wellknown Natural Language Processing  tasks such as Named Entity Recognition and Classification  to identify
the analyses performed by wellknown Natural Language Processing  tasks such as Named Entity Recognition  to identify
the best candidate that is then executed to get the correct answer
the candidate annotations produced by two stateoftheart tools for Entity Linking on three different datasets
the candidate annotations produced by two stateoftheart tools for Entity Recognition and Classification on three different datasets
the canonicalised query
The challenge here is that newspaper data is often unstructured
The challenge here is that newspaper data varied
The Datalog query can be executed in a she will
The Datalog query is translated to Unix Bash
the data source requesting more detailed information for a particular part of more detailed information
the distribution expected by the facts of a relation
the entities mentioned in natural language text
the entities that occur in eventbased descriptions of the years
the entities that occur in eventbased descriptions of the years
the entity identified by the same mention
the entity type which is learned from entity
The evaluation of our system is based on a dataset
The evaluations show that some of our approaches have over 080 F1scoresConversational systems have become increasingly popular as a way for humans to interact with computers
the evidence given by the RDF RDF Knowledge Base to support a claim
the evidence is then integrated into the computation of the confidence value to improve the evidence estimation
The extracted activities are associated with other relevant information from publication metadata
The extracted activities stored as RDF triples in a knowledge base
The first strategy relies on reranking steps in order to predict entities
The first strategy relies on reranking steps in order to predict relations
The first strategy relies on three base features in order to predict entities
The first strategy relies on three base features in order to predict relations
The first strategy uses machine learning in order to exploit the connection density between nodes in the knowledge graph
the flexible question answering approaches to produce high quality relation triples across sentences
the generalized Benfords law
The intuition behind a Probabilistic Soft Logic  model is that an annotation likely implies some ontological classes on the entity
The intuition behind a Probabilistic Soft Logic  model is that an annotation likely implies some ontological classes on the entity
the joint  a posteriori  annotation revision suggested by a Probabilistic Soft Logic  model
the joint  a posteriori  annotation revision suggested by a Probabilistic Soft Logic  modelThe LOD cloud offers a plethora of RDF data sources where users discover items of interest by issuing SPARQL queries
the joint entity and relation linking tasks as an instance of the Generalised Travelling Salesman Problem
the knowledge extracted from an existing RDF
the Linked Geo Data Cloud
them matching entities in a knowledge base
Then given a SPARQL query
Then We show that using extend we can answer more queries
the ones found in scientific publications
the posts that are truly relevant to the crisis
The presented approach is based on representing the extractors responses as realvalue vectors
The presented approach is based on using the extractors responses as input samples for two Deep Learning networks ENND 
The presented approach is based on using the extractors responses as input samples for two Deep Learning networks ENNTR 
The presented approach is based on using the extractors responses as input samples for two Deep Learning networks Ensemble Neural Network for Disambiguation 
The presented approach is based on using the extractors responses as input samples for two Deep Learning networks Ensemble Neural Network for Type Recognition 
the proposed approach which led to an improvement of up to 18percent compared to the model we modified
There are many works already in this space however most of many works are for social media
There are many works already in this space however most of many works are using news content for the decision making
Therefore it is difficult to extract recipes from recipes
Therefore it is difficult to locate recipes from recipes
Therefore pruning of candidate rules are major problems
Therefore the ranking are major problems
the representations of the entities are used to define two timeaware similarity measures to control the implicit effect of time on entity similarity
The results show that our framework can significantly improve the quality of approximate answersthe ones found in scientific publications
The results show that our framework can significantly speed up the generation of alternative queries
the results show that our system achieves great improvements
The results show that the joint  a posteriori  annotation revision improves the original scores of the two tools
the same time requiring only a Bash
these approaches have become increasingly popular for these approaches ability to capture support other reasoning tasks
these approaches have become increasingly popular for these approaches ability to capture the similarity between entities
These can be integrated in order to create one large medical Knowledge Base
These can be matched in order to create one large medical Knowledge Base
These ontologies need to evolve regularly in response to changes in the domain
These ontologies need to evolve regularly in response to the emergence of new requirements
These representations are driven by the Scholarly Ontology specifically conceived for documenting research processes
These rules are used to quantify the evidence
the Spatial Benchmark Generator implements all topological relations of DE9IM between LineStrings in the twodimensional space
the Spatial Benchmark Generator implements all topological relations of DE9IM between Polygons in the twodimensional space
the Spatial Benchmark Generator that can be used to test the performance of link discovery systems
The steadilygrowing popularity of semantic data on the Web have propelled data cubes in RDF
The steadilygrowing popularity of semantic data on the Web have propelled the interest in SPARQL Online Analytical Processing 
The steadilygrowing popularity of the support for aggregation queries in SPARQL 11 have propelled data cubes in RDF
The steadilygrowing popularity of the support for aggregation queries in SPARQL 11 have propelled the interest in Online Analytical Processing 
The steadilygrowing popularity of the support for aggregation queries in SPARQL 11 have propelled the interest in SPARQL Online Analytical Processing 
the strategies significantly outperform the current stateoftheart approaches for entity linkingOntology summarization aspires to produce an abridged version of the original data source highlighting Ontology summarization most important concepts
the strategies significantly outperform the current stateoftheart approaches for relation linking
the timeaware similarity measure proposed to flatten the time effect on entity similarity
Things in which application behaviour is often specified using imperative programming languages
This benchmark contains 293K documents
This benchmark contains 2M golden relation triples
This benchmark contains 636 relation types
This is important in highrisk domains such as healthcare intelligence where machinelearned systems extract facts from an input corpus is opaque to the enduser
This is important in highrisk domains such as healthcare intelligence where working of the extractors is opaque to the enduser
This makes these two information representation forms hard to compare limiting the possibility to use these two information representation forms jointly to improve analytical tasks
This makes these two information representation forms hard to compare limiting the possibility to use these two information representation forms jointly to improve predictive tasks
This makes these two information representation forms hard to integrate limiting the possibility to use these two information representation forms jointly to improve analytical tasks
This makes these two information representation forms hard to integrate limiting the possibility to use these two information representation forms jointly to improve predictive tasks
This omission is important because SHACL by design favors constraintsReasonable Ontology Templates is a language for representing ontology modelling patterns in the form of parameterised ontologies
This operation needs to take into account a variety of factors
this operator complexity which can make efficient evaluation of queries with OPTIONAL challenging
This paper aims at approximating the representativeness of a relation within a knowledge base
This paper presents an iterative approach for NET classifiers in scientific publications
This paper presents an iterative approach for training NER in scientific publications
This preprocessing happens only once so that indexing the data in a database may be an overkill
This preprocessing happens only once so that indexing the data in triple store may be an overkill
This preprocessing happens only once so that loading the data in a database may be an overkill
This preprocessing happens only once so that loading the data in triple store may be an overkill
This task is called provenanceaware query answering
To address this challenging task our system Open image in new window performs languagetocode translation by generating a SPARQL query
To address this challenging task our system Open image in new window performs languagetocode translation by interpreting the natural language question
To address this issue we propose a rule learning method
To address two challenges we propose a faceted browsing system for heterogeneous largescale RDF graphs
To address two challenges we propose GraFa
To alleviate these challenges this paper studies the effect of several optimization techniques
To be able to provide intelligent responses conversational systems must correctly model the structure and semantics of a conversation
Today a wealth of data are distributed using Semantic Web standards
To deal with structural incompatibilities we present a novel finegrained approach
To further understand the impact that such a change in semantics may have on query solutions we analyse how this new semantics would affect the results of user queries over WikidataMany citizens nowadays flock to social media during crises to acquire the latest information about the event
To validate the effectiveness and efficiency of our framework our conduct extensive experiments on the realworld RDF dataset
Traditionally entity have been performed either as dependent sequential tasks
Traditionally entity have been performed either as independent parallel tasks
Traditionally relation have been performed either as dependent sequential tasks
Traditionally relation have been performed either as independent parallel tasks
translational embedding model which is specially designed for SPARQL queries
triples representing Java code constructs
triples representing Java code constructs
true claims  when conflicting claims are made by several sources
Truth Discovery aims to identify facts 
Truth Discovery aims to identify true claims 
two different solution strategies for which we provide a comparative analysis in this paper
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as DBpedia are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as Wikidata are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as DBpedia are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as DBpedia are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as Wikidata are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as Wikidata are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as YAGO are built from crowdsourcing
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately Knowledge bases  such as YAGO are biased because Knowledge bases  such as YAGO are built from opportunistic agglomeration of available databases
Unfortunately the specification of SHACL leaves open the problem of validation against recursive constraints
unifying relation extraction using binary
unifying relation extraction using composite contexts
unifying relation extraction using unary
Unlike relation extraction tasks we are facing textual descriptions of activities of widely variable length while pairs of successive activities often span multiple sentences
Unlike usual named entity recognition we are facing textual descriptions of activities of widely variable length while pairs of successive activities often span multiple sentences
user studies conducted to study the effectiveness of We
vectorbased entity representations using a textbased Knowledge Graphs embedding model
We address the automatic extraction from publications of two key concepts for representing research processes
We address the problem of finding descriptive explanations of facts
We address the sequence relation between successive activities
We also generate alternative queries for returned answers
We also present initial usability studies over GraFaWe address the concept of research activity
We also present the results of experiments over synthetic queries designed to stresstest the canonicalisation method highlighting difficult casesA number of synthetic benchmarks have been proposed for evaluating the performance of link discovery systems
We also use subevent clustering visualizations to illustrate the qualitative potential of Structured Event Entity ResolutionWe introduce a declarative framework for privacypreserving Linked Data publishing in which utility policies are specified as SPARQL queries
we approach also supports users in navigating the space of possible solutions by showing how certain choices such as limiting the number of concepts would reflect on the application performance
we approach also supports users in navigating the space of possible solutions by showing how certain choices such as privileging historical ones would reflect on the application performance
we approach also supports users in navigating the space of possible solutions by showing how certain choices such as privileging trendy concepts would reflect on the application performance
We approach is dataindependent
We approach leads to inspect only the privacy in order to determine the sequence of anonymization operations applicable to any graph instance for satisfying the policies
We approach leads to inspect utility policies in order to determine the sequence of anonymization operations applicable to any graph instance for satisfying the policies
web APIs classify entities
We can guarantee that the canonicalisation of two queries within a core fragment of SPARQL is equal if
We can guarantee that the canonicalisation of two queries within a core fragment of SPARQL is equal if the two queries are equivalent We also support other SPARQL features but with a weaker soundness guarantee that the is equivalent to the input query
we classifiers employ dependency embeddings
we classifiers employ partofspeech embeddings
we classifiers employ taskspecific features
we classifiers employ word embeddings
We close this gap by comparing representatives of both types of systems in a frequently used evaluation protocol
We compare the strategies
We compare We system with some Information Extraction baselines on We benchmarkThis study exploits knowledge
We demonstrate how these approaches are able to uncover different coherence patterns in conversations on the Ubuntu Dialogue CorpusIn domains such as Disaster Relief events rather than named entities are the primary focus of analysts
We demonstrate the applicability of We approach with a prototype system
We develop a declarative framework for privacypreserving Linked Data publishing in which privacy policies are specified as SPARQL queries
We develop a declarative framework for privacypreserving Linked Data publishing in which utility policies are specified as SPARQL queries
we developed with several
We enrich the recipes with links to information on the ingredients
We evaluate graphbased learningbased approaches for measuring semantic coherence using knowledge graphs as sources of background knowledge
We evaluate graphbased learningbased approaches for measuring semantic coherence using knowledge graphs vector space embeddings as sources of background knowledge
We evaluate graphbased learningbased approaches for measuring semantic coherence using word embedding models as sources of background knowledge
We evaluate machine learningbased approaches for measuring semantic coherence using knowledge graphs as sources of background knowledge
We evaluate machine learningbased approaches for measuring semantic coherence using knowledge graphs vector space embeddings as sources of background knowledge
We evaluate machine learningbased approaches for measuring semantic coherence using word embedding models as sources of background knowledge
We evaluate the strategies on a dataset with 5000 questions
We evaluate We approach on scientific publications focusing on Methods in computer science publications
We evaluate We approach on scientific publications focusing on Proteins in biomedical publicationsDealing with large tabular datasets often requires extensive preprocessing
We evaluate We approach on scientific publications focusing on the longtail entities types Datasets
We experimented applying a Probabilistic Soft Logic  model
We experimented applying a Probabilistic Soft Logic  model
we experimented with several
we experiments on multiple datasets and methods show that a solution effectively aligns document embeddings while maintaining the characteristics of the embedding models
we experiments on multiple datasets and methods show that a solution effectively aligns entities embeddings while maintaining the characteristics of the embedding modelsOver the recent years embedding methods have attracted increasing focus as a means for knowledge graph completion
we experiments on multiple datasets and methods show that KADE effectively aligns document embeddings while maintaining the characteristics of the embedding models
we experiments on multiple datasets and methods show that KADE effectively aligns entities embeddings while maintaining the characteristics of the embedding models
we experiments show that for the use case of data preprocessing we approach is competitive with stateoftheart systems in terms of scalability while at the same time she will on a Unix systemHistorical newspapers provide a lens on customs of the past
we experiments show that for the use case of data preprocessing we approach is competitive with stateoftheart systems in terms of speed while at the same time she will on a Unix system
we experiment with scenarios where the data is translated to a single language
we experiment with scenarios where the model is tested on another
we experiment with scenarios where the model is trained on one language
We first project the RDF graph into a continuous vector space by an entity context
We follow an approach inspired from information retrieval
We follow an approach propose a simple yet effective and efficient solution
We gauge our algorithms performance through experiments
We introduce a declarative framework for privacypreserving Linked Data publishing in which privacy policies are specified as SPARQL queries
We introduce different strategies for training data extraction semantic expansion
We introduce the task of measuring semantic  in  coherence in a conversation with respect to background knowledge
We optimisations capture interactions between JOIN the LEFT JOIN COALESCE
We optimisations capture interactions between JOIN the LEFT JOIN integrity constraints such as attribute nullability
We optimisations capture interactions between JOIN the LEFT JOIN integrity constraints such as foreign key constraints
We optimisations capture interactions between JOIN the LEFT JOIN integrity constraints such as uniqueness
We present an ontology for representing workflows over components with ReadWrite Linked Data interfaces
We present an unsupervised approach to process natural language questions
We present We approach automatically extracted lexicons to identify recipes in digitised historical newspapers to extract ingredient information
We present We approach automatically extracted lexicons to identify recipes in digitised historical newspapers to generate recipe tags
We present We approach based on distant supervision
We propose a benefit model for RDF cubes
We propose a caching approach
We propose a model RDF Knowledge Base in the form of These rules
we propose a solution
We propose graphbased learningbased approaches for measuring semantic coherence using knowledge graphs as sources of background knowledge
We propose graphbased learningbased approaches for measuring semantic coherence using knowledge graphs vector space embeddings as sources of background knowledge
We propose graphbased learningbased approaches for measuring semantic coherence using word embedding models as sources of background knowledge
we propose KADE
We propose machine learningbased approaches for measuring semantic coherence using knowledge graphs as sources of background knowledge
We propose machine learningbased approaches for measuring semantic coherence using knowledge graphs vector space embeddings as sources of background knowledge
We propose machine learningbased approaches for measuring semantic coherence using word embedding models as sources of background knowledge
We propose provenanceaware caching
We propose SPARQL queries with aggregation
We prove the soundness of We algorithmsIncreasingly organizations are adopting These ontologies to describe organizations large catalogues of items
We provide an extensive evaluation of the system across three different benchmarks with different characteristics
We provide approximations for these operators calculation
We provide OCR quality indicators
We provide OCR quality indicators impact on the extraction process
We research shows how machine learning can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food culture
We research shows how natural language processing can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food culture
We research shows how semantic web can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food cultureThe steadilygrowing popularity of semantic data on the Web have propelled the interest in Online Analytical Processing 
We result entity filtering
We results on real data show that provenanceaware caching outperforms significantly the Jena TDB native caching in terms of hitrate
We results on real data show that provenanceaware caching outperforms significantly the Jena TDB native caching in terms of response timeWith the popularity of RDF as an independent data model came the need for mechanisms to detect violations of such constraints
We results on real data show that provenanceaware caching outperforms significantly the LRU strategy
We results on synthetic data show that provenanceaware caching outperforms significantly the Jena TDB native caching in terms of hitrate
We results on synthetic data show that provenanceaware caching outperforms significantly the Jena TDB native caching in terms of response time
We results on synthetic data show that provenanceaware caching outperforms significantly the LRU strategy
We show that calculating these operators is NPcomplete
we show that the addition of semantic features improve accuracy over a purely statistical model
We show that the models produced outperform each single extractor responses in terms of macro F1 measures
We show that the models produced outperform each single extractor responses in terms of micro F1 measures
We show that We ontology is expressive enough to cover the basic workflow patterns
We start with a succinct translation of a SPARQL fragment into SQL
we study this problem
We tackle this problem in the OntologyBased Data Access  OBDA  setting
We test We approach using Wikidata as the knowledge baseMany question answering systems over knowledge graphs rely on entity linking components in order to connect the natural language input to the underlying knowledge graph
We test We approach using Wikipedia as the source corpus
We then compute the minimum number of facts
We then propose optimisation techniques for improving the structure of generated SQL queries
We then propose optimisation techniques for reducing the size
We thus explore the feasibility of an alternative SPARQL semantics based on certain answers
We train two Deep Learning networks ENND 
We train two Deep Learning networks ENNTR 
We train two Deep Learning networks Ensemble Neural Network for Disambiguation 
We train two Deep Learning networks Ensemble Neural Network for Type Recognition 
What is missing so far is a common evaluation
Whenever multiple names are used to denote owl sameAs statements are needed in order to link foster reuse
Whenever multiple names are used to denote owl sameAs statements are needed in order to link the data
Whenever multiple names are used to denote the same thing  sameAs statements are needed in order to link foster reuse
Whenever multiple names are used to denote the same thing  sameAs statements are needed in order to link the data
While text documents describe entities in freeform Knowledge Bases organizes such information in a structured way
While this operator is used extensively this operator is also known for this operator complexity
window classifiers using Logistic Regression
window classifiers using Random Forests
window classifiers using SVMs
With ontology templates modelling patterns can be broken down into convenient pieces
With ontology templates modelling patterns can be broken down into manageable pieces
With ontology templates modelling patterns can be encapsulated
With ontology templates modelling patterns can be instantiated
With ontology templates modelling patterns can be uniquely identified
With ontology templates modelling patterns can be used as queries
With the popularity of RDF as an independent data model came the need for specifying constraints on RDF as an independent data graphs
With We work We aim to combine workflow languages which allow for the highlevel specification of application behaviour by nonexpert users with Linked Data
word embeddings engineered to exploit distinctive traits of research publications
Workflow languages have been successfully applied for modelling behaviour in enterprise information systems
years obtained using our model
Yet texts often also contain Counting information stating that a subject is in a specific relation with a number of objects without mentioning the objects a number of objects
