Structured scene descriptions of images are useful for the automatic processing and querying of large image databases
We show how the combination of a visual model can improve on the task of mapping images to images
images associated scene description
We show how the combination of a statistical semantic model can improve on the task of mapping images to images
subject predicate object  where each triple consists of a pair of visual objects
In this paper we consider scene descriptions
scene descriptions which are represented as a set of subject predicate object 
scene descriptions which are represented as a set of triples 
visual objects which appear in the image
In this paper we consider the relationship between them 
triples  where each triple consists of a pair of visual objects
In this paper we consider the relationship between eg manridingelephant manwearinghat 
object detection based on convolutional neural networks with a latent variable model for link prediction
We combine a standard visual model for object detection
We apply multiple stateoftheart link prediction methods
We compare We capability for visual relationship detection
One of the main advantages of link prediction methods is that We can also generalize to triples
triples which have never been observed in the training data
a statistical semantic model using link prediction methods
We experimental results on the recently published Stanford Visual Relationship a challenging real world dataset show that the integration of a statistical semantic model can significantly improve visual relationship detection
We experimental results on the recently published Stanford Visual Relationship dataset  show that the integration of a statistical semantic model can significantly improve visual relationship detection
We combined approach achieves superior performance compared to the stateoftheart method from the Stanford computer vision group