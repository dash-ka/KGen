owl is recognized as the de facto standard notation for ontology engineering
it is believed to be more effective for users
the manchester owl syntax was developed as an alternative to symbolic description logic
this paper sets out to test that belief from two perspectives by evaluating how accurately and quickly people understand the informational content of axioms
this paper sets out to derive inferences from people
by conducting a betweengroup empirical study involving 60 novice participants we found that symbolic description logic is just as effective as the manchester owl syntax for peoples understanding of axioms
moreover for two types of inference problems symbolic description logic supported significantly better task performance than the manchester owl syntax yet the manchester owl syntax never significantly outperformed symbolic description logic
these surprising results suggest that the belief that the manchester owl syntax is more effective than symbolic description logic at least for these types of task is unfounded
an outcome of this research is the suggestion that ontology axioms may be better presented in the manchester owl syntax
an outcome of this research is the suggestion that ontology axioms may be better presented in symbolic description logic
ontology axioms when presented to nonexperts
further empirical studies are needed to see whether further empirical studies hold for other types of task
further empirical studies are needed to explain these surprising resultsa growing number of highly optimized reasoning algorithms have been developed to allow inference tasks on expressive ontology languages such as owl
nevertheless there is broad agreement that a reasoner could be optimized for some
nevertheless there is broad agreement that a reasoner could be optimized for all the ontologies
this particular fact makes it hard to select the best performing reasoner to handle a given ontology especially for novice users
in this paper we present our introduced ranking method
we generates a recommendation in the form of reasoner ranking
the efficiency as well as the correctness are our main ranking criteria
our solution combines and adjusts multitarget regression techniques
our solution combines and adjusts multilabel classification
a large collection of 10 wellknown reasoners are studied
a large collection of ontologies are studied
the experimental results show that our introduced ranking method performs significantly better than several stateoftheart ranking solutions
furthermore the experimental results proves that our introduced ranking method could effectively be evolved to a competitive metareasonera granular manner which has been successfully applied to a number of scientific datasets
nanopublications are a concept to represent linked data in a provenanceaware manner
nanopublications are a concept to represent linked data in a granular manner
a provenanceaware manner which has been successfully applied to a number of scientific datasets
we demonstrated in previous work how we can establish reliable identifiers for sets thereof
we demonstrated in previous work how we can establish verifiable identifiers for nanopublications thereof
we demonstrated in previous work how we can establish reliable identifiers for nanopublications thereof
we demonstrated in previous work how we can establish verifiable identifiers for sets thereof
further adoption of these techniques however was probably hindered by the fact that nanopublications can lead to an explosion in the number of triples due to auxiliary information about the structure of each nanopublication
further adoption of these techniques however was probably hindered by the fact that nanopublications can lead to an explosion in the number of triples due to auxiliary information about the structure of repetitive provenance and metadata
we demonstrate here that this significant overhead disappears once we take the version history of nanopublication datasets into account calculate incremental updates
we demonstrate here that this significant overhead disappears once we allow users to deal with the specific subsets users need
typical subsets that researchers use for the total size and overhead of evolving scientific datasets analyses
typical subsets can be retrieved efficiently with optimized precision
typical subsets can be referenced efficiently with optimized reliability
typical subsets can be retrieved efficiently with optimized reliability
typical subsets can be referenced efficiently with optimized persistence
typical subsets can be referenced efficiently with optimized precision
typical subsets can be retrieved efficiently with optimized persistence
we show that the total size and overhead of evolving scientific datasets is reducedthe task of answering natural language questions over rdf data has received wide interest in recent years in particular in the context of the series of qald benchmarks
the task of answering natural language questions over rdf data consists of mapping a natural language question to an executable form  so that answers from a given kb can be extracted
the task of answering natural language questions over rdf data consists of mapping a natural language question to sparql so that answers from a given kb can be extracted
so far most systems proposed are monolingual and rely on a set of hardcoded rules to interpret questions most systems into a sparql query
so far most systems proposed are monolingual and rely on a set of hardcoded rules to interpret map most systems into a sparql query
we present the first multilingual qald pipeline
the first multilingual qald pipeline that induces a model from training data for mapping a natural language question into logical form as probabilistic inference
a languageindependent logical form based on dudes 
dudes  that are then mapped to a sparql query as a deterministic second step
a languageindependent logical form based on dependencybased underspecified discourse representation structures 
in particular we approach learns to map universal syntactic dependency representations to a languageindependent logical form
dependencybased underspecified discourse representation structures  that are then mapped to a sparql query as a deterministic second step
features extracted from corresponding semantic representations
features extracted from the dependency graph
factor graphs that rely on features
we model builds on factor graphs
parameters using a ranking objective
we rely on approximate inference techniques markov chain monte carlo methods in particular rank to update parameters
we rely on approximate inference techniques markov chain monte carlo methods in sample rank to update parameters
we focus lies on developing methods
developing methods that present a novel combination of word embedding approaches for this purpose
developing methods that overcome the lexical gap
developing methods that present a novel combination of machine translation for this purpose
as a proof of concept for we approach we evaluate our approach on the qald6 datasets for english german spanishin this paper we present an empirical comparison of user performance
in this paper we present perceived usability for sparql
in this paper we present perceived usability for semwidgql
in this paper we present perceived usability for a pathoriented rdf query language
we developed semwidgql to facilitate the formulation of rdf queries
we developed semwidgql to enable nonspecialist developers to integrate linked data into standard web applications
we developed semwidgql to enable nonspecialist developers to integrate other semantic data sources into standard web applications
we developed semwidgql to enable web authors to integrate other semantic data sources into standard web applications
we developed semwidgql to enable web authors to integrate linked data into standard web applications
we performed a user study
a user study in which participants wrote a set of queries in both languages
we measured both objective performance to a set of questionnaire items
we measured both subjective responses to a set of questionnaire items
results indicate that semwidgql is easier to learn more efficient
results indicate that semwidgql is easier to learn preferred by learners
the typical queries performed on linked data
to assess the applicability of semwidgql in real applications we analyzed its expressiveness based on a large corpus of observed sparql queries showing that the language covers more than 90percent of the typical queriesthis paper investigates meta structures schemalevel graphs that abstract connectivity information among a set of entities in a knowledge graph
knowledge discovery tasks ranging from relatedness explanation to data retrieval
meta structures are useful in a variety of knowledge discovery tasks
we devise efficient automatabased algorithms
we formalize the meta structure computation problem
we introduce a meta structurebased relevance measure
a meta structurebased relevance measure which can retrieve entities related to those in input
we implemented we machineries in a visual tool
a visual tool called mekong
we report on an extensive experimental evaluation
an extensive experimental evaluation which confirms the suitability of we proposal from both the efficiency of view
an extensive experimental evaluation which confirms the suitability of we proposal from both effectiveness point of viewthe disjunctive skolem chase is a sound
the disjunctive skolem chase is complete  algorithm that can be used to solve conjunctive query answering over dl ontologies and programs with disjunctive existential rules
the disjunctive skolem chase is albeit nonterminating  algorithm that can be used to solve conjunctive query answering over dl ontologies and programs with disjunctive existential rules
even though acyclicity notions can be used to ensure chase termination for a large subset of realworld knowledge bases the complexity of reasoning over acyclic theories still remains high
hence we study these general restrictions
we include an evaluation
an evaluation that shows that almost all acyclic dl ontologies do indeed satisfy these general restrictionsa prominent approach to implementing ontologymediated queries is to rewrite into a firstorder query
a firstorder query which is then executed using a conventional sql database system
we consider the case where the actual query is a conjunctive query
we consider the case where the ontology is formulated in the description logic
we show that rewritings of such ontologymediated queries can be efficiently computed in practice in a sound
we show that rewritings of such ontologymediated queries can be efficiently computed in practice in complete way
ontologymediated queries that are based on the simpler atomic queries also illuminating the relationship between firstorder rewritings of ontologymediated queries
a prominent approach to implementing ontologymediated queries combines a reduction with a
ontologymediated queries based on atomic queries
a prominent approach to implementing ontologymediated queries combines a reduction with a
ontologymediated queries based on conjunctive
a decomposed backwards chaining algorithm for ontologymediated queries
experiments with realworld ontologies show promising resultsthe linked data fragment  framework has been proposed as a uniform view to explore the tradeoffs of consuming linked data
ldf  framework has been proposed as a uniform view to explore the tradeoffs of consuming linked data
linked data when servers provide different interfaces to access servers data
performance bandwidth needs caching several practical challenges
this new ldf interface has this new ldf interface own particular properties regarding performance bandwidth needs arise
for example before exposing a new type of ldfs in some server can we formally say something about how this new ldf interface compares to other interfaces previously implemented in the same server
from the client side given a client with some restricted capabilities in terms of network connection which is the best type of ldfs to complete a given task
from the client side given a client with some restricted capabilities in terms of time constraints which is the best type of ldfs to complete a given task
from the client side given a client with some restricted capabilities in terms of computational power which is the best type of ldfs to complete a given task
today there are only a few formal theoretical tools to help answer other practical questions
today there are only a few formal theoretical tools to help answer these
researchers have embarked in solving researchers mainly by experimentationwhen crises hit many flog to social media to consume information related to the event
when crises hit many flog to social media to share information related to the event
social media these posts tend to provide valuable reports on affected people donation offers help requests advice provision automatically identifying the category of reports on affected individuals donations and volunteers  contained in these posts is vital for these posts efficient handling and consumption by effected communities organisations
social media these posts tend to provide valuable reports on affected people donation offers help requests advice provision automatically identifying the category of information  contained in these posts is vital for these posts efficient handling and consumption by effected communities organisations
social media these posts tend to provide valuable reports on affected people donation offers help requests advice provision automatically identifying the category of information  contained in these posts is vital for these posts efficient handling and consumption by concerned organisations
social media these posts tend to provide valuable reports on affected people donation offers help requests advice provision automatically identifying the category of reports on affected individuals donations and volunteers  contained in these posts is vital for these posts efficient handling and consumption by concerned organisations
a deep convolutional neural network  cnn  model designed for identifying the category of information
in this paper we introduce semcnn a wide convolutional neural network  cnn  model contained in crisisrelated social media content
in this paper we introduce semcnn a deep convolutional neural network  cnn  model contained in crisisrelated social media content
a wide convolutional neural network  cnn  model designed for identifying the category of information
unlike previous models the proposed model integrates an additional layer of semantics into a deep cnn network
semantics that represents the named entities in the text
unlike previous models the proposed model integrates an additional layer of semantics into a wide cnn network
previous models which mainly rely on the lexical representations of words in the text
the baselines which consist of statistical deep learning models
the baselines which consist of nonsemantic deep learning models
results show that the proposed model consistently outperforms the baselinesknowledge graphs are huge collections of primarily encyclopedic facts
knowledge graphs are widely used in other important tasks
knowledge graphs are widely used in structured search
knowledge graphs are widely used in question answering
knowledge graphs are widely used in question answering
knowledge graphs are widely used in entity recognition
knowledge graphs are widely used in other important tasks
knowledge graphs are widely used in entity recognition
knowledge graphs are widely used in structured search
rule mining is commonly applied to discover patterns in knowledge graphs
hockey players do not have children
incompleteness which may result in the wrong estimation of the quality of mined rules
leading to erroneous beliefs such as all artists have won an award
however
unlike in traditional association rule mining knowledge graphs provide a setting with a high degree of incompletenessontologybased systems that process data is received over time as in contextaware systems
data stemming from different sources
data stemming from that
in ontologybased systems reasoning should be resilient against inconsistencies in the data
in ontologybased systems reasoning needs to cope with the temporal dimension
motivated by such settings this paper addresses the problem of handling inconsistent data in a temporal version of ontologybased query answering
three inconsistencytolerant semantics that have been introduced for querying inconsistent description logic knowledge bases
this setting three inconsistencytolerant semantics
proposed temporal query language that combines conjunctive queries with operators of propositional linear temporal logic
proposed temporal query language that extend to this
we consider a recently proposed temporal query language
we furthermore complete the picture for the consistent case
we investigate their complexity for dllite  temporal knowledge bases
we investigate their complexity for  mathcal r  temporal knowledge basesdata streams emanating from sensors
realtime processing of data streams is becoming a common task in internet of things scenarios
the key implementation goal consists supporting advanced data analytics services like anomaly detection
the key implementation goal consists in efficiently handling massive incoming data streams
in an ongoing industrial project a 24 7 available stream processing engine usually faces dynamically changing data
in an ongoing industrial project a 24 7 available stream processing engine usually faces dynamically changing workload characteristics
these changes impact a 24 7 available stream processing engine usually performance
these changes impact a 24 7 available stream processing engine usually reliability
we propose strider
stream processing engine that optimizes logical query plan according to the state of data streams
we propose a hybrid adaptive distributed rdf stream processing engine
strider has been designed to guarantee important industrial properties such as high throughput
a hybrid adaptive distributed rdf stream processing engine has been designed to guarantee important industrial properties such as acceptable latency
a hybrid adaptive distributed rdf stream processing engine has been designed to guarantee important industrial properties such as scalability
strider has been designed to guarantee important industrial properties such as acceptable latency
strider has been designed to guarantee important industrial properties such as fault tolerance
a hybrid adaptive distributed rdf stream processing engine has been designed to guarantee important industrial properties such as fault tolerance
a hybrid adaptive distributed rdf stream processing engine has been designed to guarantee important industrial properties such as high availability
strider has been designed to guarantee important industrial properties such as scalability
stream processing engine that optimizes logical query plan according to the state of data streams
strider has been designed to guarantee important industrial properties such as high availability
a hybrid adaptive distributed rdf stream processing engine has been designed to guarantee important industrial properties such as high throughput
these guarantees are obtained by designing a 24 7 available stream processing engine usually architecture with stateoftheart apache components such as kafka
these guarantees are obtained by designing a 24 7 available stream processing engine usually architecture with stateoftheart apache components such as spark
we highlight on a single machine machine up to 60x gain on throughput compared to stateoftheart systems on a 9 machines cluster a major breakthrough in this systems category  of strider on synthetic data sets
we highlight the efficiency  of strider on synthetic data sets
we highlight on a single machine machine up to 60x gain on throughput compared to stateoftheart systems on a 9 machines cluster a major breakthrough in this systems category  of strider on realworld sets
we highlight the efficiency  of strider on realworld sets
we highlight on a single machine machine up to 60x gain on throughput compared to a throughput of 31 million triplessecond on a 9 machines cluster a major breakthrough in this systems category  of strider on realworld sets
we highlight on a single machine machine up to 60x gain on throughput compared to a throughput of 31 million triplessecond on a 9 machines cluster a major breakthrough in this systems category  of strider on synthetic data setswikipedia infoboxes are thus a very rich source of structured knowledge
wikipedia infoboxes contain information about article entities in the form of attributevalue pairs
however as the different language versions of wikipedia evolve independently it is a challenging problem to find correspondences between infobox attributes in different language editions
however as the different language versions of wikipedia evolve independently it is a promising problem to find correspondences between infobox attributes in different language editions
8 effective features for cross lingual infobox attribute matching containing categories templates attribute values
8 effective features for cross lingual infobox attribute matching containing categories templates attribute labels
in this paper we propose 8 effective features for cross lingual infobox attribute matching
we propose entityattribute factor graph to consider not only individual features
we propose entityattribute factor graph to consider not only the correlations among attribute pairs
experiments on the two wikipedia data sets of englishchinese show that proposed approach can achieve high f1measure 855 percent
experiments on the two wikipedia data sets of englishchinese show that proposed approach can achieve high f1measure 854 percent respectively on the two data sets
experiments on the two wikipedia data sets of englishfrench show that proposed approach can achieve high f1measure 854 percent respectively on the two data sets
experiments on the two wikipedia data sets of englishfrench show that proposed approach can achieve high f1measure 855 percent
our proposed approach finds 23923 new infobox attribute mappings between english wikipedia
our proposed approach finds 23923 new infobox attribute mappings between chinese wikipedia
31576 between english based on no more than six thousand existing matched infobox attributes
31576 between french based on no more than six thousand existing matched infobox attributes
our conduct an infobox completion experiment on complement 76498  more than 30percent of enzh wikipedia  pairs of corresponding articles with more than one attributevalue pairs
more than 30percent of enzh wikipedia existing crosslingual links
our conduct an infobox completion experiment on englishchinese wikipedia  more than 30percent of enzh wikipedia  pairs of corresponding articles with more than one attributevalue pairsknowledge graphs effectively capture explicit relational knowledge about individual entities
however visual attributes of those entities like those entities like their shape concerning their usage in color concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in color concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like their shape concerning their usage in natural language shape concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like their shape concerning their usage in pragmatic aspects concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in natural language shape concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like their shape concerning their usage in pragmatic aspects concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in color concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like their shape concerning their usage in color concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in natural language shape concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in pragmatic aspects concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in pragmatic aspects concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in color concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in natural language shape concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like their shape concerning their usage in pragmatic aspects concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like their shape concerning their usage in natural language shape concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like their shape concerning their usage in color concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in color concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in natural language shape concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in natural language shape concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in pragmatic aspects concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in pragmatic aspects concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in color concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in color concerning those entities like their shape concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in natural language shape concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like their shape concerning their usage in natural language shape concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like pragmatic aspects concerning their usage in pragmatic aspects concerning those entities like color concerning their usage in natural language usage in natural language are not covered
however visual attributes of those entities like those entities like color concerning their usage in pragmatic aspects concerning those entities like pragmatic aspects concerning their usage in natural language usage in natural language are not covered
recent approaches encode such knowledge by learning latent representations  separately in computer vision visual object features are learned from large image collections
recent approaches encode such knowledge by learning latent representations  separately in computational linguistics word embeddings are extracted from huge text corpora
recent approaches encode such knowledge by learning em beddings  separately in computational linguistics word embeddings are extracted from huge text corpora
recent approaches encode such knowledge by learning em beddings  separately in computer vision visual object features are learned from large image collections
huge text corpora which capture word embeddings distributional semantics
the relational knowledge captured in knowledge graph
a shared latent representation that integrates information across those modalities
we investigate the potential of complementing the relational knowledge embeddings with knowledge from text documents and images by learning a shared latent representation
we empirical results show that a joined concept representation provides measurable benefits for semantic similarity benchmarks since a joined concept representation shows a higher correlation with the human notion of similarity than uni or bimodal representations and entitytype prediction tasks since a joined concept representation clearly outperforms plain knowledge graph embeddings
knowledge that go beyond todays knowledge graphs
these findings encourage further research towards capturing types of knowledgea dedicated programming language enabling semantic web programmers to directly define functions on rdf graphs
a dedicated programming language enabling semantic web programmers to directly define functions on sparql results
in addition to the existing standards semantic web programmers could really benefit from a dedicated programming language
the existing standards dedicated to querying
a dedicated programming language enabling semantic web programmers to directly define functions on rdf terms
the existing standards dedicated to representation
instance when defining sparql extension functions
this is especially the case for instance
real cases where a dedicated language can support modularity of the code
the ability to define dedicated aggregates are real cases
the ability to reuse dedicated aggregates are real cases
real cases where a dedicated language can support maintenance of the code
the ability to capitalize complex sparql filter expressions into extension functions are real cases
functions assigned to owl classes with the selection of the function to be applied to a resource depending on the type of a resource
functions assigned to rdfs classes with the selection of the function to be applied to a resource depending on the type of a resource
functional properties associated to rdf resources as functions
functional properties associated to the definition of procedural attachments as functions
other families of use cases include the definition of functional properties
to address these needs we define a linked data script language on top of the sparql filter expression language
to address these needs we define ldscript 
we provide the formal grammar of the syntax inference rules of the semantics of the language
we provide the formal grammar of the natural semantics inference rules of the semantics of the language
an evaluation using real test bases from w3c
different implementations and approaches comparing in particular
we also provide a benchmark and perform an evaluation with different implementations and approaches
different implementations and approaches comparing in java compilation
different implementations and approaches comparing in script interpretationsubjectpropertyvalue triples which can be enhanced with references to add provenance information
wikidata is a collaborativelyedited knowledge graph wikidata expresses knowledge in the form of subjectpropertyvalue triples
understanding the quality of wikidata is key to wikidata widespread adoption as a knowledge resource
we analyse one aspect of wikidata quality provenance in terms of relevance of wikidata external references
we analyse one aspect of wikidata quality provenance in terms of authoritativeness of wikidata external references
we follow a twostaged approach
first we perform a crowdsourced evaluation of references
second we use the judgements
the judgements collected in the first stage to train a machine
a machine learning model to predict reference quality on a largescale
the features learning models were related to reference editing the triples referred to
the features learning models were related to reference the semantics of the triples the triples referred to
the features chosen for the machine
 61percent  of the references evaluated were authoritative
 61percent  of the references evaluated were relevant
bad references were often links
links that changed
links that either stopped working or pointed to other pages
the machine learning models were able to accurately predict nonrelevant references
the machine learning models were able to accurately predict nonauthoritative references
the machine learning models outperformed the baseline
further work should focus on implementing our approach in wikidata to help editors find bad referencescrosslingual taxonomy alignment refers to mapping each category in the source taxonomy of one language onto a ranked list of most relevant categories in the target taxonomy of another language
recently vector similarities depending on bilingual topic models have achieved the stateoftheart performance on crosslingual taxonomy alignment
the categories cooccurring words in text
however bilingual topic models ignore explicit category correlations such as the categories among the categories of ancestordescendant relationships in a taxonomy
however bilingual topic models only model the textual context of categories
however bilingual topic models ignore explicit category correlations such as correlations between the categories among the categories of ancestordescendant relationships in a taxonomy
the categories cooccurring words in correlations
crosslingual taxonomy alignment which brings two novel category correlation based called ccbilda
crosslingual taxonomy alignment which brings two novel category correlation based ccbibtm
crosslingual taxonomy alignment which brings two novel category correlation based bilingual topic models
in this paper we propose a unified solution to encode category correlations into bilingual topic modeling for crosslingual taxonomy alignment
experiments on two realworld datasets show our proposed models significantly outperform the stateoftheart baselines on crosslingual taxonomy at least 109 percent in each evaluation metric 
experiments on two realworld datasets show our proposed models significantly outperform the stateoftheart baselines on crosslingual taxonomy alignment the idea to execute queries over several distributed knowledge bases lies at the core of the semantic web vision
federated querying  lies at the core of the semantic web vision
the service keyword that allows one to allocate subqueries to servers
to accommodate the semantic web vision sparql provides the service keyword
multiple sources resulting in a combinatorially growing number of alternative allocations of subqueries to sources
in many cases however data may be available from multiple sources
running a federated query on all possible sources might not be very lucrative from a users point of view if extensive execution times or fees are involved in accessing all possible sources data
this shortcoming federated joincardinality approximation techniques
to address this shortcoming have been proposed to narrow down the number of possible allocations to a few most promising  ones
to address this shortcoming have been proposed to narrow down the number of possible allocations to or resultsyielding  onesanswering queries over a federation of sparql endpoints requires combining data from more than one data source
optimizing queries in such scenarios is particularly challenging not only because of the large variety of possible query execution plans that correctly answer the query
optimizing queries in such scenarios is particularly challenging not only because of the large variety of possible query execution plans also because there is only limited access to statistics about schema data of remote sources
optimizing queries in such scenarios is particularly challenging not only because of the large variety of possible query execution plans also because there is only limited access to statistics about instance data of remote sources
to overcome these challenges most federated query engines rely on dynamic programming strategies to produce optimal plans
to overcome these challenges most federated query engines rely on heuristics to reduce the space of possible query execution plans
nevertheless optimal plans may still exhibit a high number of high execution times because of inaccurate cost estimations
nevertheless optimal plans may still exhibit a high number of intermediate results because of inaccurate cost estimations
nevertheless optimal plans may still exhibit a high number of high execution times because of heuristics
nevertheless optimal plans may still exhibit a high number of intermediate results because of heuristics
statistics that therefore enables odyssey to produce better query execution plans
in this paper we present odyssey
an approach that uses statistics
in this paper we present an approach
statistics that allow for a more accurate cost estimation for federated queries
query execution plans that are better in terms of data execution time than stateoftheart optimizers
our experimental results show that odyssey produces query execution plans
query execution plans that are better in terms of data transfer time than stateoftheart optimizers
our experiments using the fedbench benchmark
our experiments show execution time gains of at least 25 times on averagestructured scene descriptions of images are useful for the automatic processing and querying of large image databases
we show how the combination of a visual model can improve on the task of mapping images to images
images associated scene description
we show how the combination of a statistical semantic model can improve on the task of mapping images to images
in this paper we consider the relationship between manridingelephant manwearinghat 
in this paper we consider scene descriptions
in this paper we consider the relationship between them 
triples  where each triple consists of a pair of visual objects
subject predicate object  where each triple consists of a pair of visual objects
visual objects which appear in the image
scene descriptions which are represented as a set of triples 
scene descriptions which are represented as a set of subject predicate object 
object detection based on convolutional neural networks with a latent variable model for link prediction
we combine a standard visual model for object detection
we apply multiple stateoftheart link prediction methods
we compare we capability for visual relationship detection
one of the main advantages of link prediction methods is that we can also generalize to triples
triples which have never been observed in the training data
we experimental results on the recently published stanford visual relationship dataset  show that the integration of a statistical semantic model can significantly improve visual relationship detection
a statistical semantic model using link prediction methods
we experimental results on the recently published stanford visual relationship a challenging real world dataset show that the integration of a statistical semantic model can significantly improve visual relationship detection
we combined approach achieves superior performance compared to the stateoftheart method from the stanford computer vision groupthe most relevant knowledge contained in large ontologies
ensuring access to the most relevant knowledge has been identified as an important challenge
subontologies that preserve all entailments over a given vocabulary
to this end subontologies  have been proposed
to this end excerpts  have been proposed
the knowledge regarding the vocabulary by allowing for a degree of semantic loss
to this end minimal modules  have been proposed
to this end certain small number of axioms that best capture the knowledge  have been proposed
in this paper we introduce the notion of subsumption justification as an extension of justification  a minimal set of axioms needed to preserve a logical consequence  to capture the subsumption knowledge between a term in the vocabulary
in this paper we introduce the notion of subsumption justification as an extension of justification  a minimal set of axioms needed to preserve a logical consequence  to capture the subsumption knowledge between all other terms in the vocabulary
we present algorithms for computing subsumption justifications based on a simulation notion developed for the problem of deciding the logical difference between ontologies
we show how subsumption justifications can be used to obtain minimal modules
we show how subsumption justifications can be used to compute best excerpts by additionally employing a partial maxsat solver
this yields two stateoftheart methods for computing all best excerpts which we evaluate over large biomedical ontologies
this yields two stateoftheart methods for computing all minimal modules excerpts which we evaluate over large biomedical ontologiessparql query answering in ontologybased data access is carried out by translating into sql queries over the data source
standard translation techniques try to transform the user query into a union of conjunctive queries following the heuristic argument that ucqs can be efficiently evaluated by modern relational database engines
in this work we show that under certain conditions on the interplay between the statistics of the data alternative translations can be evaluated much more efficiently
in this work we show that under certain conditions on the interplay between the mappings alternative translations can be evaluated much more efficiently
in this work we show that under certain conditions on the interplay between the ontology alternative translations can be evaluated much more efficiently
in this work we show that translating to ucqs is not always the best choice
a novel cardinality estimation that takes into account all such sparql query answering in ontologybased data access components
to find the best translation we devise a cost model together with a novel cardinality estimation
queries that are orders of magnitude the cost model we propose
queries that are orders of magnitude more efficient
we experiments confirm that alternatives to the best translation might produce hence is well suited to select the best translation
we experiments confirm that alternatives to the best translation might produce queries is faithful to the actual query evaluation costthe web of data is an inherently distributed environment where ontologies are subject to constant changes
the web of data is an inherently distributed environment where ontologies are located in remote locations
the extent and significance of this dependency is not wellstudied yet
reasoning is affected by constant changes
to address this problem this paper presents an empirical study on how the distribution of ontological data on the web of data affects the outcome of reasoning
we study to what extent the inclusion of additional ontological information via iri dereferencing leads to new derivations
we study to what extent the inclusion of imports directive to the input datasets leads to new derivations
we study to what extent the inclusion of the owl leads to new derivations
we study to what degree datasets depend on external ontologieswe introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and global objectification of able to express inclusion dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and local objectification of functional dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and local objectification of key dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and global objectification of key dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and global objectification of external uniqueness dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and global objectification of relations dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and local objectification of able to express inclusion dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and global objectification of functional dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and local objectification of relations dependencies
we introduce  the language decidable with the same computational complexity as  an extension of the nary propositionally closed description logic  the language decidable with the same computational complexity as  to deal with attributelabelled tuples  generalising the positional notation  projections of relations and local objectification of external uniqueness dependencies
global objectification of external uniqueness dependencies is equipped with both abox axioms
local objectification of functional dependencies is equipped with both tbox axioms
local objectification of able to express inclusion dependencies is equipped with both abox axioms
global objectification of relations dependencies is equipped with both tbox axioms
global objectification of key dependencies is equipped with both abox axioms
global objectification of key dependencies is equipped with both tbox axioms
local objectification of external uniqueness dependencies is equipped with both tbox axioms
local objectification of external uniqueness dependencies is equipped with both abox axioms
local objectification of relations dependencies is equipped with both tbox axioms
global objectification of external uniqueness dependencies is equipped with both tbox axioms
local objectification of able to express inclusion dependencies is equipped with both tbox axioms
global objectification of functional dependencies is equipped with both abox axioms
local objectification of key dependencies is equipped with both tbox axioms
global objectification of functional dependencies is equipped with both tbox axioms
global objectification of able to express inclusion dependencies is equipped with both tbox axioms
local objectification of relations dependencies is equipped with both abox axioms
global objectification of able to express inclusion dependencies is equipped with both abox axioms
local objectification of functional dependencies is equipped with both abox axioms
description logic the language decidable with the same computational complexity as to deal with attributelabelled tuples generalising the positional notation projections of relations
global objectification of relations dependencies is equipped with both abox axioms
local objectification of key dependencies is equipped with both abox axioms
we show how a simple syntactic restriction on the appearance of projections makes reasoning in the language decidable with the same computational complexity as
projections sharing common attributes in a  the language decidable with the same computational complexity as  knowledge base
mathcaldlrpm  nary description logic is able to encode more thoroughly conceptual data models such as orm
the  nary description logic is able to encode more thoroughly conceptual data models such as ever
mathcaldlrpm  nary description logic is able to encode more thoroughly conceptual data models such as uml
mathcaldlrpm  nary description logic is able to encode more thoroughly conceptual data models such as ever
the  nary description logic is able to encode more thoroughly conceptual data models such as uml
the  nary description logic is able to encode more thoroughly conceptual data models such as orm
the obtainedan increasing number of use cases require a timely extraction of nontrivial knowledge from semantically annotated data streams especially on the web and for the internet of things
expressive reasoning which is challenging to compute on large streams
often a timely extraction of nontrivial knowledge requires expressive reasoning
a new reasoner that supports a pragmatic nontrivial fragment of the logic lars
the logic lars which extends answer set programming for streams
we propose laser a new reasoner
a new reasoner that supports a pragmatic nontrivial fragment of the logic lars
a novel evaluation procedure which annotates formulae to avoid the recomputation of duplicates at multiple time points
the logic lars which extends answer set programming for streams core
the logic lars which extends programming for streams core
at laser a new reasoner laser implements a novel evaluation procedure
a novel evaluation procedure is responsible for significantly better runtimes than the ones of an implementation of lars solver clingo
other stateoftheart systems like cqels which runs on the programming
a novel evaluation procedure is responsible for significantly better runtimes than the ones of other stateoftheart systems like cqels solver clingo
a novel evaluation procedure which annotates formulae to avoid the recomputation of duplicates at multiple time points
a novel evaluation procedure is responsible for significantly better runtimes than the ones of other stateoftheart systems like csparql solver clingo
other stateoftheart systems like csparql which runs on the programming
an implementation of lars which runs on the programming
this enables the application of expressive logicbased reasoning to large streams
this opens the door to a wider range of stream reasoning use casesvector space embeddings have been shown to perform well when using rdf data in data machine learning tasks
vector space embeddings have been shown to perform well when using rdf data in data mining
existing approaches such as rdf2vec use local information rely on local sequences
local sequences generated for nodes in the rdf graph
existing approaches use local information rely on local sequences
for word embeddings global techniques such as glove have been proposed as an alternative
in this paper we show how the idea of global embeddings can be transferred to rdf embeddings
in this paper we show that the results are competitive with traditional local techniques like rdf2vecjeuxdemots is a rich collaborative lexical network in french represented in an adhoc tabular format
french built on a crowdsourcing principle as a game with a purpose
in the interest of interoperability we propose a conversion algorithm for jeuxdemots following the ontolex model along with a word sense alignment algorithm anchors jeuxdemots senserefinements to synsets in the lemon edition of babelnet
a word sense alignment algorithm called jdmbabelizer that
in the interest of reuse we propose a conversion algorithm for jeuxdemots following the ontolex model along with a word sense alignment algorithm anchors jeuxdemots senserefinements to the linguistic linked open data cloud
in the interest of reuse we propose a conversion algorithm for jeuxdemots following the ontolex model along with a word sense alignment algorithm anchors jeuxdemots senserefinements to synsets in the lemon edition of babelnet
in the interest of interoperability we propose a conversion algorithm for jeuxdemots following the ontolex model along with a word sense alignment algorithm anchors jeuxdemots senserefinements to the linguistic linked open data cloud
a conversion algorithm for jeuxdemots following the ontolex model along with a word sense alignment algorithm in terms of weighted semanticlexical relations particularly the inhibition relation between senses
the ontolex model along with a word sense alignment algorithm called jdmbabelizer that anchors jeuxdemots senserefinements to synsets in the lemon edition of babelnet exploits the richness of jeuxdemots
the ontolex model along with a word sense alignment algorithm called jdmbabelizer that anchors jeuxdemots senserefinements to the linguistic linked open data cloud exploits the richness of jeuxdemots
weighted semanticlexical relations particularly the inhibition relation between senses that are specific to jeuxdemots
the ontolex model along with a word sense alignment algorithm called jdmbabelizer that anchors jeuxdemots senserefinements to the linguistic linked open data cloud
our produce a reference alignment dataset for jeuxdemots that our use to evaluate the quality of a conversion algorithm for jeuxdemots following the ontolex model along with a word sense alignment algorithm
the ontolex model along with a word sense alignment algorithm called jdmbabelizer that anchors jeuxdemots senserefinements to synsets in the lemon edition of babelnet
our produce a reference alignment dataset for babelnet that our use to evaluate the quality of a conversion algorithm for jeuxdemots following the ontolex model along with a word sense alignment algorithm
the ontolex model along with a word sense alignment algorithm called jdmbabelizer that that our make available to the community
the obtained results are comparable to those of state of the art approachesprivacy audit logs are used to capture the actions of participants in a data sharing environment in order for auditors to check compliance with privacy policies
however collusion may occur between the auditors and participants to obfuscate actions
actions that should be recorded in privacy audit logs
a linked data based method of utilizing blockchain technology to create tamperproof audit logs
in this paper we propose a linked data
tamperproof audit logs that provide proof of log manipulation and nonrepudiation
we also provide experimental validation of the scalability of we solution using an existing linked data privacy audit log modelfinding the commonalities between descriptions of knowledge is a foundational reasoning problem of machine learning
finding the commonalities between descriptions of data is a foundational reasoning problem of machine learning
it was formalized in the early 70s as computing a least general generalization   mathtt lgg   of such descriptions
we revisit this wellestablished problem in the sparql query language for rdf graphs
in particular
in by contrast to the literature
we address the literature for the entire class of conjunctive sparql queries aka basic graph pattern queries
crucially when background knowledge is available as rdf schema ontological constraints we take advantage of the literature to devise mathtt lgg  s as we experiments on the popular dbpedia dataset show
crucially when background knowledge is available as rdf schema ontological constraints we take advantage of the literature to devise much more precise  s as we experiments on the popular dbpedia dataset showautomated acquisition of ontologies from data has attracted research interest because automated acquisition automated acquisition of ontologies from data can complement manual expensive construction of ontologies
the problem of general terminology induction in owl acquiring general expressive tbox axioms from an abox 
the problem of general terminology induction in owl acquiring general expressive tbox axioms from data 
we investigate the problem of general terminology induction in owl ie
novel measures designed to rigorously evaluate the quality of general expressive tbox axioms while respecting the standard semantics of owl
we define novel measures
we propose an informed datadriven algorithm that constructs class expressions for general expressive tbox axioms in guarantees completeness
we propose an informed datadriven algorithm that constructs class expressions for general expressive tbox axioms in owl completeness
we empirically evaluate the quality measures on two corpora of ontologies and run a case study with a domain expert to gain insight into applicability of the measures and acquired general expressive tbox axioms
the results show that the measures capture different quality aspects
only correct general expressive tbox axioms can be interestingthe present paper explores whether authoring tests accurately represent the expectations of ontology authors
authoring tests derived from competency questions
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of the interface to test whether cq is able to be answered by the ontology at a given stage of cq construction 
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of the interface to test whether cq is able to be answered by the ontology at a given stage of a given competency question construction 
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of a given competency question construction 
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of a given competency question construction 
an approach known as cqdriven ontology authoring
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of the interface to test whether a given competency question is able to be answered by the ontology at a given stage of a given competency question construction 
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of the interface to test whether a given competency question is able to be answered by the ontology at a given stage of cq construction 
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of a given competency question an approach 
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of the interface to test whether cq is an approach 
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of the interface to test whether a given competency question is an approach 
an approach known as cqdriven ontology authoring
in earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given competency question is able to be answered by the ontology at a given stage of a given competency question an approach 
the experiments presented in the present paper suggest that the interface to test whether cq is able to be answered by the ontology at a given stage of a given competency question construction an approach
the experiments presented in the present paper suggest that the interface to test whether a given competency question is able to be answered by the ontology at a given stage of a given competency question construction an approach
the experiments presented in the present paper suggest that the interface to test whether a given competency question is able to be answered by the ontology at a given stage of cq construction an approach
the experiments presented in the present paper suggest that the interface to test whether cq is able to be answered by the ontology at a given stage of cq construction an approach
an approach known as cqdriven ontology authorings understanding of cqs matches users understanding quite well especially for inexperienced ontology authorsprefer  clause that expresses
in this paper we present sprefql an extension of the sparql language  soft  preferences over the query results obtained by the main body of the query
the sparql language that allows appending a  prefer  clause
an extension of the sparql language does not add expressivity
the sparql language that allows appending a  prefer  clause
a  prefer  clause that expresses  soft  preferences over the query
any sprefql query can be transformed to an equivalent standard sparql query
however clearly separating preferences from the  hard  patterns  where  clause gives queries where the intention of the client is more cleanly expressed an advantage for both machine optimization
however clearly separating preferences from the  hard  patterns  where  clause gives queries where the intention of the client is more cleanly expressed an advantage for both human readability
however clearly separating preferences from the  filters in the  where  clause gives queries where the intention of the client is more cleanly expressed an advantage for both machine optimization
however clearly separating preferences from the  filters in the  where  clause gives queries where the intention of the client is more cleanly expressed an advantage for both human readability
we also provide empirical evidence that optimizations specific to sprefql improve runtime efficiency by comparison to the usually applied optimizations on the equivalent standard sparql query
in this paper we present sprefql we formally define the semantics of an extension of the sparql language
the sparql language that allows appending a  prefer  clause
in this paper we present sprefql we formally define the syntax
a  prefer  clause that expresses  soft  preferences over the querywe present proof of soundness for an expressive schema language for rdf graphs
rdf graphs that is the foundation of shape expressions language 20
we present a formal semantics for an expressive schema language for rdf graphs
we present a formal semantics for shapes schemas
we present proof of soundness for shapes schemas
it can be used to constrain the admissible properties for nodes in that graph
it can be used to describe the vocabulary
it can be used to constrain the admissible values for nodes in that graph
it can be used to describe the structure of an rdf graph
a typing mechanism called shapes against which nodes of the graph can be checked
the language defines a typing mechanism
the language includes cardinality constraints for the number of allowed occurrences of a property
the language includes an algebraic grouping operator for the number of allowed occurrences of a property
the language includes a choice operator for the number of allowed occurrences of a property
shapes can be combined using boolean operators
shapes can use possibly recursive references to other shapesweb tables constitute valuable sources of information for various applications
valuable sources of information for various applications ranging from web search to knowledge base  kb  augmentation
an underlying common requirement is to annotate the rows of web tables with semantically rich descriptions of entities
entities published in web kbs
an ontology matching method which exploits instance information of entities available both in a web table
the minimal entity context provided in web tables to discover correspondences to the kb
in this paper we evaluate three unsupervised annotation methods a semantic embeddings method
an ontology matching method which exploits schematic information of entities available both in a web table
in this paper we evaluate three unsupervised annotation methods a lookupbased method
an ontology matching method which exploits instance information of entities available both in a kb
in this paper we evaluate three unsupervised annotation methods an ontology matching method
a semantic embeddings method that exploits a vectorial representation of the rich entity context in a kb to identify the most relevant subset of entities in the web table
a lookupbased method which relies on the minimal entity context
an ontology matching method which exploits schematic information of entities available both in a kb
we experimental evaluation is conducted using two existing benchmark data sets in addition to a new largescale benchmark
a new largescale benchmark created using wikipedia tables
one benchmark data set
we results show that we novel lookupbased method outperforms stateoftheart lookupbased methods the semantic embeddings method outperforms lookupbased methods in one benchmark data and the lack of a rich schema in web tables can limit the ability of ontology matching tools in performing highquality table annotation
as a result we propose a hybrid method
a hybrid method that significantly outperforms individual methods on all the benchmarksa key constraint that is valid in only a part of the data
a conditional key is a key constraint
in this paper we show how such keys can be mined automatically on large knowledge bases
for this we combine techniques from key mining with techniques from rule mining
we show that we method can scale to large knowledge bases of millions of facts
we also show that the conditional keys we we can improve the quality of entity linking by up to 47percent pointstwo knowledge bases that represent the same realworld object
entity alignment is the task of finding entities in two knowledge bases
when facing entities in two knowledge bases in different natural languages conventional crosslingual entity alignment methods rely on machine translation to eliminate the language barriers
these approaches often suffer from the uneven quality of translations between languages
while recent embeddingbased techniques do not need machine translation for crosslingual entity alignment a significant number of attributes remain largely unexplored
while recent embeddingbased techniques encode relationships in the entities in two knowledge bases a significant number of attributes remain largely unexplored
while recent embeddingbased techniques encode entities in the entities in two knowledge bases a significant number of attributes remain largely unexplored
in this paper we propose a joint attributepreserving embedding model for crosslingual entity alignment
a joint attributepreserving embedding model for crosslingual entity alignment jointly embeds the structures of two entities in two knowledge bases into a unified vector space
further refines a joint attributepreserving embedding model for crosslingual entity alignment by leveraging attribute correlations in the entities in two knowledge bases
methods based on machine translation
our experimental results on realworld datasets show that this approach could be complemented with methods
our experimental results on realworld datasets show that this approach significantly outperforms the stateoftheart embedding approaches for crosslingual entity alignmentontologybased data access is gaining importance both scientifically
ontologybased data access is gaining importance both practically
however little attention has been paid so far to the problem of updating ontologybased data access systems
this is an essential issue if we want to be able to cope with modifications of data both at the ontology and at the source level while maintaining the independence of the data sources
in this paper we propose mechanisms to properly handle updates in this context
we show that updating data both at the source level is firstorder rewritable
we show that updating data both at the ontology level is firstorder rewritable
such updating mechanisms based on nonrecursive datalog
we also provide a practical implementation of suchin many applications there is an increasing need for the new types of rdf data analysis
rdf data analysis that are not covered by standard reasoning tasks such as sparql query answering
one such important analysis task is determining what are differences between two given entities in an rdf graph
one such important analysis task is entity comparison
one such important analysis task is determining what are similarities between two given entities in an rdf graph
in an rdf graph about drugs we may want to compare ibuprofen
in an rdf graph about drugs we may want to find out that ibuprofen are similar in that ibuprofen are both analgesics
in an rdf graph about drugs we may want to find out that metamizole are similar in that metamizole are both analgesics
in an rdf graph about drugs we may want to find out that ibuprofen are similar in that metamizole are both analgesics
in an rdf graph about drugs we may want to compare metamizole
in an rdf graph about drugs we may want to find out that metamizole are similar in that ibuprofen are both analgesics
for instance
 in contrast to metamizole ibuprofen also has a considerable antiinflammatory effect
entity comparison is a widely used functionality available in many information systems such as product comparison websites
entity comparison is a widely used functionality available in many information systems such as universities
however comparison is typically domainspecific
however comparison depends on a fixed set of aspects to compare
in this paper we propose a formal framework for domainindependent entity comparison over rdf graphs
we model similarities between entities as sparql queries satisfying certain additional properties and propose algorithms for computing certain additional properties
we model differences between entities as sparql queries satisfying certain additional properties and propose algorithms for computing certain additional propertiesthe federation of different data sources gained increasing attention due to the continuously growing amount of data
but the more data are available from heterogeneous sources the higher the risk is of inconsistency
to tackle this challenge in federated knowledge bases we propose a fully automated approach for computing trust values at different levels of granularity
gathering both the conflict graph generated by inconsistency detection and resolution we create a markov network to facilitate the application of gibbs sampling to compute a probability for each conflicting assertion
gathering both statistical evidence generated by inconsistency detection and resolution we create a markov network to facilitate the application of gibbs sampling to compute a probability for each conflicting assertion
based on which trust values for each integrated data source respective signature elements are computed
based on which trust values for each integrated data source respective signature elements are computed
we evaluate we approach on a large distributed dataset from the domain of library sciencelargescale knowledge graphs such as dbpedia can be enhanced by relation extraction from text using the data in the knowledge graph as training data
largescale knowledge graphs such as wikidata can be enhanced by relation extraction from text using distant supervision
largescale knowledge graphs such as wikidata can be enhanced by relation extraction from text using the data in the knowledge graph as training data
largescale knowledge graphs such as dbpedia can be enhanced by relation extraction from text using distant supervision
largescale knowledge graphs such as yago can be enhanced by relation extraction from text using distant supervision
largescale knowledge graphs such as yago can be enhanced by relation extraction from text using the data in the knowledge graph as training data
a languageagnostic approach that builds machine learning models only from languageindependent features
a languageagnostic approach that exploits background knowledge from the graph instead of languagespecific techniques
while most existing approaches use languagespecific methods  usually for english  we present a languageagnostic approach
we demonstrate the extraction of relations from wikipedia abstracts
the extraction of relations from wikipedia abstracts using the twelve largest language editions of wikipedia
from those we can extract 16 m new relations in dbpedia at a level of precision of 95percent using a randomforest classifier
a randomforest classifier trained only on languageindependent features
the information extracted
furthermore we show an exemplary geographical breakdown of the informationfaceted search is the de facto approach for exploration of data in ecommerce the de facto approach for exploration of data in ecommerce allows users to construct queries in an intuitive way without a prior knowledge of formal query languages
the de facto approach for exploration of data in ecommerce has been recently adapted to the context of rdf
recursion which poses limitations in practice
existing faceted search systems however do not allow users to construct queries with recursion
existing faceted search systems however do not allow users to construct queries with aggregation
aggregation which poses limitations in practice
in this work we study the corresponding query language
in this work we extend faceted search over rdf with these functionalities
in particular we investigate complexity of query containment problems
in particular we investigate complexity of the query answeringontology alignment is an area of active research where many algorithms and approaches are being developed
approaches performance is usually evaluated by comparing the produced alignments to a reference alignment in terms of precision
many algorithms is usually evaluated by comparing the produced alignments to a reference alignment in terms of recall
many algorithms is usually evaluated by comparing the produced alignments to a reference alignment in terms of fmeasure
many algorithms is usually evaluated by comparing the produced alignments to a reference alignment in terms of precision
approaches performance is usually evaluated by comparing the produced alignments to a reference alignment in terms of recall
approaches performance is usually evaluated by comparing the produced alignments to a reference alignment in terms of fmeasure
these measures however do not reveal differences between alignments at a finergrained level such as
these measures however only provide an overall assessment of the quality of the alignments
these measures however do not reveal regions
these measures however do not reveal commonalities between alignments at a finergrained level such as
these measures however do not reveal individual mappings
furthermore reference alignments are often unavailable which makes the comparative exploration of alignments at different levels of granularity even more important
making such comparisons efficient calls for a  humanintheloop  approach best supported through interactive visual representations of alignments
our approach extends matrix cubes used for visualizing dense dynamic networks
our approach extends a recent tool  used for visualizing dense dynamic networks
our first identify use cases for ontology alignment evaluation
then detail how our alignment cubes support interactive exploration of multiple ontology alignments
ontology alignment evaluation that can benefit from interactive visualization
support common tasks identified in the use cases
our demonstrate the usefulness of alignment cubes by describing visual exploration scenarios showing how alignment cubes support common tasksin modelling realworld knowledge there often arises a need to represent
in modelling realworld knowledge there often arises reason with metaknowledge
to equip description logics for dealing with such ontologies we allow concept inclusions to express constraints on annotations
to equip description logics for dealing with such ontologies we enrich description logic concepts and roles with finite sets of attributevalue pairs called annotations
we show that this may lead to even undecidability
we identify cases where this increased expressivity can be achieved without incurring increased complexity of reasoning
we show that this may lead to increased complexity
in particular
we cover  mathcal sroiq 
the description logic underlying owl 2 description logic
we describe a tractable fragment based on mathcal el 
we describe a tractable fragment based on the lightweight description logic 
we cover the description logic
