operations that access remote data
many scientific problems can be represented as computational workflows of operations
many scientific problems can integrate heterogeneous data
many scientific problems can analyze new data
many scientific problems can derive new data
even when the data access are implemented as web services workflows are often constructed manually in languages such as bpel
even when processing operations are implemented as grid services workflows are often constructed manually in languages such as bpel
even when processing operations are implemented as web services workflows are often constructed manually in languages such as bpel
even when the data access are implemented as grid services workflows are often constructed manually in languages such as bpel
adding semantic descriptions of the services enables automatic composition
adding semantic descriptions of the services enables mixedinitiative composition
in most previous work semantic descriptions of the services consists of semantic types for inputs of a type for the service as a whole
in most previous work semantic descriptions of the services consists of semantic types for outputs of services for the service as a whole
in most previous work semantic descriptions of the services consists of semantic types for inputs of services for the service as a whole
in most previous work semantic descriptions of the services consists of semantic types for outputs of a type for the service as a whole
while this is certainly useful we argue that is not enough to model complex data workflows
while this is certainly useful we argue that is not enough to construct complex data workflowsrdf allow efficient storage and access to rdf statements
applications are able to use expressive query languages in order to retrieve relevant metadata to perform different tasks
however access to metadata may not be public to just any application or service
instead powerful mechanisms for protecting sets of rdf statements are required for many semantic web applications
instead flexible mechanisms for protecting sets of rdf statements are required for many semantic web applications
unfortunately current rdf stores do not provide finegrained protection
this paper presents a mechanism by which complex can be specified in order to protect access to metadata in multiservice environments
this paper fills this gap
this paper presents a mechanism by which expressive policies can be specified in order to protect access to metadata in multiservice environmentsin recent years controlled natural language  has received much attention with regard to ontologybased knowledge acquisition systems
in recent years cnl  has received much attention with regard to ontologybased knowledge acquisition systems
cnls as subsets of natural languages can be useful for both humans and computers by eliminating ambiguity of natural languages
rdf  triples using a domainspecific ontology as natural languagelike narratives resource description framework triples language constituents
resource description framework  triples using a domainspecific ontology as natural languagelike narratives resource description framework triples language constituents
our previous work ontopath proposed to edit natural languagelike narratives
natural languagelike narratives that are structured in rdf
natural languagelike narratives that are structured in rdf  triples
natural languagelike narratives that are structured in resource description framework  triples
however other systems have difficulties in enlarging the expression capacity
other systems employing cfg for grammar definition
however our previous work have difficulties in enlarging the expression capacity
cfgld  that includes sequential structures of the grammars
a newly developed editor which our propose in this paper
a newly developed editor permits grammar definitions through contextfree grammar with lexical dependency 
a newly developed editor permits grammar definitions through cfgld 
contextfree grammar with lexical dependency  that includes semantic structures of the grammars
cfgld  that includes semantic structures of the grammars
contextfree grammar with lexical dependency  that includes sequential structures of the grammars
cfg describing the sequential structure of grammar
with cfg lexical dependencies between sentence elements can be designated in the definition system
through the defined grammars translates the content into rdf triples
through the defined grammars the implemented editor guides users narratives in more familiar expressions with a domainspecific ontologyas an extension to the semantic web semantic web will not only contain structured data with machine understandable semantics
as an extension to the semantic web semantic web will not only contain textual information
while structured queries can be used to find information more precisely on the semantic web keyword searches are still needed to help exploit textual information
it thus becomes very important that we can combine precise structured queries with imprecise keyword searches to have a hybrid query capability
in addition due to the huge volume of information on the semantic web the hybrid query must be processed in a very scalable way
a hybrid query capability that combines unary treeshaped structured queries with keyword searches
in this paper we define such a hybrid query capability
we show how the hybrid query is evaluated on the index structure
the index structure using existing information retrieval engines in an scalable manner
we show how existing information retrieval index structures and functions can be reused to index semantic web data
the index structure using existing information retrieval engines in an efficient manner
we show how existing information retrieval index structures and functions can be reused to semantic web data textual information
an engine called semplore
we implemented this existing information retrieval approach in an engine
comprehensive experiments on semplore performance show that semplore is a promising approach
semplore leads us to believe that it may be possible to evolve current web search engines to query the semantic web
semplore leads us to believe that it may be possible to evolve current web search engines to search the semantic web
finally us breifly describe how semplore is used for searching wikipedia
finally us breifly describe how semplore is used for searching an ibm customers product informationfinding the justifications of an entailment  that is all the minimal set of axioms sufficient to produce an entailment  has emerged as a key inference service for the web ontology language
justifications are essential for debugging unsatisfiable classes and contradictions
the availability of justifications as explanations of entailments improves the understandability of complex ontologies
the availability of justifications as explanations of entailments improves the understandability of large ontologies
in this paper we present several algorithms for computing all the justifications of an entailment in an owldl ontology and show by an empirical evaluation that even a reasoner independent approach works well on real ontologieshierarchical classifications are used pervasively by humans as a means to organize hierarchical classifications data about the world
hierarchical classifications are used pervasively by humans as a means to organize knowledge about the world
natural language labels used to describe hierarchical classifications contents
one of their main advantages is that natural language labels are easily understood by human users
however at the same time this is also one of their main disadvantages as natural language labels are very hard to be reasoned about by software agents
natural language labels used to describe their contents
however at the same time this is also one of their main disadvantages as natural language labels are ambiguous
this fact creates an insuperable hindrance for classifications to being embedded in the semantic web infrastructure
the main nlp problems related to the conversion process on dmoz data
the main nlp problems related to the conversion process
these problems which are especially effective in this domain
this paper presents an approach to converting classifications into lightweight ontologies and this paper makes the following contributions this paper shows how the main nlp problems are different from the classical problems of nlp this paper proposes heuristic solutions to these problems and this paper evaluates the proposed solutions by testing the main nlp problems
this paper presents an approach to converting classifications into lightweight ontologies and this paper makes the following contributions this paper identifies the main nlp problems this paper proposes heuristic solutions to these problems and this paper evaluates the proposed solutions by testing the main nlp problems
the main nlp problems related to the conversion processmany of these ontologies contain overlapping information
in different areas these ontologies have been developed
often we would therefore want to be able to use multiple ontologies
to obtain good results we need to find the relationships between terms in the different ontologies we need to align the different ontologies
currently there already exist a number of different alignment strategies
a user that needs to align two ontologies to decide which of the different available strategies are the most suitable
however it is usually difficult for a user
a method that provides recommendations on alignment strategies for a given alignment problem
in this paper we propose a method
a method that provides recommendations on alignment strategies for a given alignment problem
a method uses the evaluation of the different available alignment strategies results to provide recommendations
a method is based on the evaluation of the different available alignment strategies on several small selected pieces from the ontologies
a method that provides recommendations on alignment strategies for a given alignment problem
illustrate
discuss a method
in this paper we give the basic steps of a method
a method that provides recommendations on alignment strategies for a given alignment problem in the setting of an alignment problem with two wellknown biomedical ontologies
a method that provides recommendations on alignment strategies for a given alignment problem
we also experiment with different implementations of the steps in a methodas more reusable structured data appears on the web casual users will want to take into casual users own hands the task of mashing up data rather than wait for mashup sites to be built that address exactly casual users individually unique needs
as more structured data appears on the web casual users will want to take into casual users own hands the task of mashing up data rather than wait for mashup sites to be built that address exactly casual users individually unique needs
in this paper we present potluck
in this paper we present a web user interface
a web user interface that let us casual users
in this paper we present data modeling expertisemash up data data themselves
in this paper we present those without programming skillsthe current wikipedia has embraced the power of collaborative editing to harness collective intelligence
the current wikipedia can also serve as an ideal semantic web data source due to high quality
the current wikipedia can also serve as an ideal semantic web data source due to influence
the current wikipedia can also serve as an ideal semantic web data source due to the current wikipedia abundance
the current wikipedia can also serve as an ideal semantic web data source due to wellstructuring
however the heavy burden of upbuilding still rests on a very small group of people
however the heavy burden of maintaining such an enormous and evergrowing online encyclopedic knowledge base still rests on a very small group of people
many casual users may still feel difficulties in writing high quality the current wikipedia articles
in this paper we use rdf graphs to model the key elements in the current wikipedia authoring and propose an integrated solution to make the current wikipedia authoring easier based on rdf graph matching
rdf graph matching expecting making more wikipedians
we solution facilitates semantics reuse and provides users with 1  a link suggestion module a category suggestion module
a category suggestion module that helps the user place articles in correct categories
1  a link suggestion module that suggests internal links between the current wikipedia articles for the user 2  a category suggestion module rrb
1  a link suggestion module that autocompletes internal links between the current wikipedia articles for the user 2  a category suggestion module rrb
2  a category suggestion module that helps the user place 2
a prototype system is implemented
experimental results show significant improvements over existing solutions to link suggestion tasks
experimental results show significant improvements over existing solutions to category suggestion tasks
the proposed enhancements can be applied to relieve the burden of professional editors thus enhancing the current wikipedia to make the current wikipedia an even better semantic web data source
the proposed enhancements can be applied to attract more contributorsnatural language interfaces offer endusers a familiar option for querying ontologybased knowledge bases
natural language interfaces offer endusers a convenient option for querying ontologybased knowledge bases
several studies have shown that several studies can achieve domain independence
several studies have shown that several studies can achieve high retrieval performance
this paper investigates if nlis are useful from an endusers point of view
this paper focuses on usability
to that end we introduce four interfaces each present a usability study benchmarking four interfaces
to that end we introduce four interfaces each allowing a different query language
the results of a usability study confirm that nlis are useful for querying semantic web data
the results of a usability study reveal a clear preference for full sentences as query languagesemantic web tasks that often require similarity measures such as semantic web service matchmaking
this research explores three sparqlbased techniques to solve semantic web tasks
semantic web tasks that often require similarity measures such as ontology mapping
semantic web tasks that often require similarity measures such as semantic data integration
our aim is to see how far it is possible to integrate customized similarity functions into sparql to achieve good results for these tasks
our first approach exploits virtual triples calling property functions to establish virtual relations among resources under comparison
our first approach exploits virtual triples the second approach uses extension functions to filter out resources finally our third technique applies new solution modifiers to postprocess a sparql solution sequence
resources that do not meet the requested similarity criteria
the semantics of the three approaches are formally elaborated
the semantics of the three approaches are formally discussed
our close the paper with a demonstration of the usefulness of our isparql framework in the context of a data integration experiment
our close the paper with a demonstration of the usefulness of our isparql framework in the context of an ontology mapping experimentsemantic search promises to provide more accurate result than presentday keyword search
however progress with semantic search has been delayed due to the complexity of progress with semantic search query languages
in this paper we explore a novel approach of adapting keywords to querying the semantic web the approach automatically translates keyword queries into formal logic queries so that end users can use familiar keywords to perform semantic search
a prototype system has been implemented in light of the approach
a prototype system named  spark 
given a keyword query spark outputs a ranked list of sparql queries as the translation result
the translation in spark consists of three major steps query ranking
the translation in spark consists of three major steps query graph construction
the translation in spark consists of three major steps term mapping
specifically a probabilistic query ranking model is proposed to select the most likely sparql query
in the experiment spark achieved an encouraging translation resultontology mapping is the key to data interoperability in the semantic web
this problem has received a lot of research attention however the research emphasis has been mostly devoted to automating the mapping process even though the creation of mappings often involve the user
as industry interest in semantic web technologies grows we must begin to support the user
as the number of widely adopted semantic web applications increases we must begin to support the user
in this paper we combine data
data gathered from theories of cognitive support to propose a theoretical framework for cognitive support in ontology mapping tools
data gathered from theories of decision making to propose a theoretical framework for cognitive support in ontology mapping tools
data gathered from background literature to propose a theoretical framework for cognitive support in ontology mapping tools
data gathered from an observational case study to propose a theoretical framework for cognitive support in ontology mapping tools
a tool called cogz
we also describe a tool
cogz that is based on this frameworkmetadata conforming to heterogeneous schemas semantically interoperable
this paper presents a method for making metadata
the knowledge embedded in the schema structures explicit by transforming the schemas into a shared eventbased representation of knowledge about the real world
the knowledge embedded in the schema structures interoperable by transforming the schemas into a shared eventbased representation of knowledge about the real world
the idea is to make the knowledge
this enables accurate reasoning services such as crossdomain semantic search browsing
this enables accurate reasoning services such as crossdomain semantic search recommending
this simplifies accurate reasoning services such as crossdomain semantic search recommending
this simplifies accurate reasoning services such as crossdomain semantic search browsing
a case study of transforming three different schemas and datasets is presented
knowledgebased recommender system utilizing the results in the semantic portal culturesampo
an implemented knowledgebased recommender system was found useful in a preliminary user studythe approach of using ontology reasoning to cleanse the output of information extraction tools was first articulated in semanticlean
a limiting factor in applying the approach of using ontology reasoning to cleanse the output of information extraction tools has been that ontology reasoning to find inconsistencies does not scale to the size of data
data produced by information extraction tools
in this paper we illustrate the use of we techniques to produce a consistent subset of a knowledge base with several thousand inconsistencies
in this paper we describe techniques to scale inconsistency detectionin this paper we address the instance migration problem
the instance migration problem arises when one wants to reclassify a set of instances of a source ontology into a semantically related target ontology
ontologies which are used to reconcile both individual level heterogeneity
we approach exploits mappings between ontologies
ontologies which are used to reconcile both conceptual
ontologies which are used to draw the migration process
a distributed description logic in which ontologies are formally encoded as dl knowledge bases and mappings as bridge rules
we ground the approach on a distributed description logic
a distributed description logic in which ontologies are formally encoded as dl knowledge bases and mappings as individual correspondences
from the theoretical side we study the task of reasoning with instance data in a distributed description logic composed of shiq ontologies
from the theoretical side we study the task of reasoning with instance data in a distributed description logic define a complete distributed tableaux inference procedure
from the theoretical side we study the task of reasoning with instance data in a distributed description logic define a correct distributed tableaux inference procedure
from the practical side we upgrade the drago a distributed description logic define a complete distributed tableaux inference procedure reasoner for dealing with instances
further show how it can be used to drive the migration of instances between heterogeneous ontologies
from the practical side we upgrade the drago a distributed description logic define a correct distributed tableaux inference procedure reasoner for dealing with instances
from the practical side we upgrade the drago a distributed description logic composed of shiq ontologies
from the practical side we upgrade the drago a distributed description logic define further showfor the development of semantic web technology researchers in the semantic web community need to focus on the areas
for the development of semantic web technology developers in the semantic web community need to focus on the areas
the areas in which human reasoning is particularly difficult
two studies in this paper demonstrate that people are predisposed to use classinclusion labels for inductive judgments
this tendency appears to stem from a this general characteristic of human reasoning using heuristics to solve problems
interface designs that incorporate human reasoning need to integrate this general characteristic underlying human induction
the inference engines that incorporate human reasoning need to integrate this general characteristic underlying human induction
interface designs
the inference enginesautomatic knowledge reuse for semantic web applications imposes several challenges on ontology search
existing ontology retrieval systems merely return a lengthy list of relevant single ontologies
relevant single ontologies which may not completely cover the specified user requirements
ontologies which can entirely fulfill the requirements
therefore there arises an increasing demand for a tool or algorithm with a mechanism to check concept adequacy of existing ontologies with respect to a user query
therefore there arises combination of ontologies
therefore there arises an increasing demand for a tool or algorithm with a mechanism to recommend a single
a single ontology that guarantees query coverage
thus this paper develops an algorithm namely combisqore to determine whether the available collection of ontologies is able to completely satisfy a submitted query
thus this paper develops an algorithm namely combisqore to determine whether the available collection of ontologies is able to return a combinative ontology
a combinative ontology that guarantees query coverage
thus this paper develops an algorithm namely combisqore to determine whether the available collection of ontologies is able to return a single ontology
the returned answers based on the returned answers based on query coverage conceptual closeness
in addition this paper ranks the returned answers
the returned answers based on the returned answers based on query coverage query coverage
the returned answers based on the returned answers based on their conceptual closeness query coverage
the returned answers based on the returned answers based on their conceptual closeness conceptual closeness
the experimental results show that the proposed algorithm is efficient
the experimental results show that the proposed algorithm is effective
the experimental results show that the proposed algorithm is simpleobo is an ontology language
an ontology language that has often been used for modeling ontologies in the life sciences
obo definition is relatively informal so in this paper we provide a clear specification for obo syntax and semantics via a mapping to owl
a mapping also allows us to apply existing semantic web tools
a mapping also allows us to apply techniques to obo
we show that semantic web reasoners can be used to efficiently reason with obo ontologies
furthermore we show that grounding the obo language in formal semantics is useful for the ontology development process using an owl reasoner we detected a likely modeling error in one obo ontologythis paper presents a tableau approach for deciding description logics outside the scope of owl dl1 1
this paper presents a tableau approach for deciding description logics outside the scope of current stateoftheart tableaubased description logic systems
in particular we define a sound for the description logic albo
in particular we show that the description logic albo provides a basis for decision procedures for this logic with full role negation
in particular we show that the description logic albo provides a basis for decision procedures for numerous other description logics with full role negation
in particular we define complete tableau calculus for the description logic albo
the description logic albo is the extension of alc with the boolean role operators inverse of range restriction operators
the description logic albo includes full support for nominals
the description logic albo is the extension of alc with the boolean role operators inverse of roles restriction operators
the description logic albo is the extension of alc with the boolean role operators inverse of domain restriction operators
a very expressive description logic which subsumes the twovariable fragment of firstorder logic and reasoning in the twovariable fragment of firstorder logic and reasoning
a very expressive description logic which subsumes boolean modal logic in the twovariable fragment of firstorder logic and reasoning
the description logic albo is a very expressive description logic is nexptimecomplete
an important novelty is the use of a generic unrestricted blocking rule as a replacement for standard loop checking mechanisms
standard loop checking mechanisms implemented in description logic systems
an implementation of our approach exists in the extscmettel systemthe ability to compute the differences is an important step to cope with the evolving nature of the semantic web
the differences that exist between two rdf models
data that need to be managed over the network
in particular rdf deltas can be employed to reduce the amount of data advanced the semantic web synchronization
in particular rdf deltas can be employed to reduce the amount of data advanced versioning services
data that need to build
data that need to be exchanged over the network
formally analyze the underlying change operations possible combinations in terms of redundancy properties
in this paper we study various rdf comparison functions in conjunction with the semantics of the underlying change operations
by considering deltas as sets of change operations
formally analyze the underlying change operations possible combinations in terms of correctness minimality semantic identitywe approach and present a prototype tool to help users identify reasoner performance bottlenecks with respect to reasoner performance bottlenecks ontologies
we describe the challenges
we then describe 4 case studies on synthetic ontologies
we then describe 4 case studies on realworld ontologies
while the anecdotal evidence suggests that the service can be useful for both ontology developers much more is desired
while the anecdotal evidence suggests that the service can be useful for both reasoner implementors much more is desiredinstancebased ontology mapping is a promising family of solutions to a class of ontology alignment problems
it crucially depends on measuring the similarity between sets of annotated instances
in this paper we study how the choice of cooccurrence measures affects the performance of instancebased mappingontologies play a core role in the success of the semantic web as the semantic web provide a shared vocabulary for different resources and applications
developing an errorfree ontology is a difficult task
a common kind of error for an ontology is logical contradiction or incoherence
in this paper we propose some approaches to measuring incoherence in dlbased ontologies
these measures give an ontology engineer important information for evaluating ontologies
these measures give an ontology engineer important information for maintaining ontologies
we implement the proposed approaches but encouraging empirical results
we provide some preliminary but encouraging empirical results
the proposed approaches using the kaon2 reasonerin open environments ontology mapping provides interoperability between interacting actors
in distributed environments ontology mapping provides interoperability between interacting actors
whole ontologies which is infeasible in open systems
however conventional mapping systems focus on mapping whole ontologies
however conventional mapping systems focus on acquiring static information
this paper shows that the interactions the interactions themselves between the actors between the interactions themselves between the actors can be used to predict mappings simplifying dynamic ontology mapping
the intuitive idea is that similar interactions follow similar conventions and patterns
similar conventions and patterns which can be analysed
the computed model can be used to suggest the possible mappings for the exchanged messages in new interactions
the suggestions can be evaluate by any standard ontology matcher if the suggestions are accurate the matchers avoid evaluating mappings unrelated to the interactionthis paper presents a controlled language for ontology editing
this paper presents a software implementation
a software implementation based partly on standard nlp tools for processing that language
a software implementation based partly on standard nlp tools for processing that manipulating an ontology
the input sentences are analysed compositionally with respect to a given ontology this allows the user to learn fewer syntactic structures since some of fewer syntactic structures can be used to refer to either classes for example
the input sentences are analysed deterministically with respect to a given ontology this allows the user to learn fewer syntactic structures since some of fewer syntactic structures can be used to refer to either instances for example
the input sentences are analysed compositionally with respect to a given ontology this allows the user to learn fewer syntactic structures since some of fewer syntactic structures can be used to refer to either instances for example
the input sentences are analysed deterministically with respect to a given ontology this allows the user to learn fewer syntactic structures since some of fewer syntactic structures can be used to refer to either classes for example
a given ontology which the software consults in order to interpret the inputs semantics
a repeatedmeasures taskbased evaluation has been carried out in comparison with a wellknown ontology editor the software received favourable results for basic tasks
this paper also discusses work in future plans for developing this language and tool
this paper also discusses work in progress plans for developing this language and toolweb search personalization based on user collaboration and sharing of information about web documents
in this paper we present a new approach to web search personalization
the proposed personalization technique separates data user profiling from the information system whose contents are being searched for the search engines
the proposed personalization technique separates data collection profiling from the information system whose indexed documents are being searched for the search engines
the proposed personalization technique tagging to rerank web search results
the proposed personalization technique uses social bookmarking
the proposed personalization technique separates data collection profiling from the information system whose contents are being searched for the search engines
the proposed personalization technique separates data user profiling from the information system whose indexed documents are being searched for the search engines
the search engine being used so users are free to choose the one users prefer even if users favorite search engine does not natively support personalization
the proposed personalization technique is independent of the search engine
we show how to implement such a system in practice
we show how to investigate such a system feasibility and usefulness with large sets of realword data
we show how to investigate such a system feasibility and usefulness with large sets of a user study
we show how to design such a system in practicethe amount of meta data available on the semantic web is constantly growing
the amount of ontologies available on the semantic web is constantly growing
the successful application of machine learning techniques for learning of ontologies from textual data mining for the semantic web contributes to this trend
however no principal approaches exist so far for mining from the semantic web
we investigate how machine can be made amenable for directly taking advantage of the rich knowledge
machine learning algorithms
the rich knowledge expressed in ontologies
the rich knowledge expressed in associated instance data
various learning tasks
kernel methods have been provide a clean framework for interfacing between nonvectorial data
kernel methods have been successfully employed in various
kernel methods have been provide a clean framework for interfacing between machine learning algorithms
in this spirit we express the problem of mining instances in ontologies as the problem of defining valid corresponding kernels
we present a principled framework for designing such kernels by means of decomposing the kernel computation into specialized kernels for selected characteristics of an ontology
an ontology which can be tuned
an ontology which can be flexibly assembled
initial experiments on real world semantic web data enjoy promising results approach
initial experiments on real world semantic web data show the usefulness of we approachsemantic descriptions of nontextual media available on the web can be used to facilitate retrieval of documents containing documents
semantic descriptions of nontextual media available on the web can be used to facilitate retrieval of documents containing media assets
semantic descriptions of nontextual media available on the web can be used to facilitate retrieval of media assets containing media assets
semantic descriptions of nontextual media available on the web can be used to facilitate presentation of documents containing media assets
semantic descriptions of nontextual media available on the web can be used to facilitate presentation of media assets containing documents
semantic descriptions of nontextual media available on the web can be used to facilitate presentation of media assets containing media assets
semantic descriptions of nontextual media available on the web can be used to facilitate retrieval of media assets containing documents
semantic descriptions of nontextual media available on the web can be used to facilitate presentation of documents containing documents
while technologies for multimedia existing descriptions already exist there is as yet no formal description of a high quality multimedia ontology
a high quality multimedia ontology that is compatible with existing web technologies
the problem using an annotation scenario
we explain the complexity of the problem
we then derive a number of requirements for specifying a formal multimedia ontology before we evaluate comm with respect to we requirements
we then derive a number of requirements for specifying a formal multimedia ontology before we present comm
we then derive a number of requirements for specifying a formal multimedia ontology before we present the developed ontology 
we then derive a number of requirements for specifying a formal multimedia ontology before we evaluate the developed ontology  with respect to we requirements
multimedia annotations that conform to comm
we provide an api for generating multimedia annotationsdesign patterns are widelyused software engineering abstractions
widelyused software engineering abstractions which define guidelines for modeling common application scenarios
ontology design patterns are the extension of software patterns for knowledge acquisition in the semantic web
in this work we present a design pattern for representing relevance depending on context in owl ontologies to assert which knowledge from the domain ought to be considered in a given scenario
besides the formal semantics we describe a reasoning procedure to extract relevant knowledge in the resulting ontology
besides the features of the pattern we describe a reasoning procedure to extract relevant knowledge in a plugin for protege pattern use
besides the formal semantics we describe a reasoning procedure to extract relevant knowledge in a plugin for protege pattern use
besides the features of the pattern we describe a reasoning procedure to extract relevant knowledge in the resulting ontology
protege which assistswe present a simple method to extract information from search engine snippets
the techniques presented
although the techniques are domain independent this work focuses on extracting biographical information of historical persons from multiple unstructured sources on the web
we first similarly find a list of persons of life by querying the periods
we first similarly find a list of persons periods of life by querying the periods
we first similarly find a list of persons periods of life by scanning the retrieved snippets for person names
we first similarly find a list of persons of life by scanning the retrieved snippets for person names
subsequently we find biographical information for the persons
the persons extracted
the persons identified
in order to get insight in the mutual relations among the persons we create a social network
a social network using cooccurrences on the web
although we use uncontrolled web sources the information is reliable
although we use unstructured web sources the information is reliable
the information extracted
moreover we show that web information extraction can be used to create both enjoyable applications
moreover we show that web information extraction can be used to create both informative applicationsdevelopers of semantic web applications face a challenge with respect to the decentralised publication model where to find statements about encountered resources
the  linked data  approach helps only a partial solution
the  linked data  approach is only a partial solution
approach which mandates that resource uris should be dereferenced metadata about the resource
approach which mandates that resource uris should be yield metadata about the resource
we present a lookup index over resources crawled on the semantic web
we present sindice crawled on the semantic web
our index allows applications to automatically retrieve sources with information about a given resource
in addition our allow resource retrieval through inversefunctional properties offer fulltext search and index sparql endpointscurrent information retrieval approaches provide a comfortable way for the user to specify information needs on the basis of keywords
current information retrieval approaches do not formally capture the explicit meaning of a keyword query
ontologybased approaches allow for sophisticated semantic search
ontologybased approaches impose a query syntax more difficult to handle
in this paper we present an approach for translating keyword queries to dl conjunctive queries using background knowledge available in ontologies
an implementation which shows that this interpretation of keywords can then be used for both exploration of for a semanticsbased declarative query answering process
an implementation which shows that this interpretation of keywords can then be used for both exploration of asserted knowledge
we present an implementation
we also present an evaluation of a discussion of the limitations of the approach with respect to we underlying assumptions
we also present an evaluation of we system to we underlying assumptions
underlying assumptions which directly points to issues for future workontologies proliferate with the growth of the semantic web
however most of data on the semantic web are still stored in relational databases
therefore it is important to establish interoperability between relational databases and ontologies for creating a web of data
an effective way to achieve interoperability is finding mappings between relational database schemas and ontologies
in this paper we propose a new approach to discovering simple mappings between an ontology
in this paper we propose a new approach to discovering simple mappings between a relational database schema
this paper eliminates incorrect mappings via validating mapping consistency
this paper exploits simple mappings based on virtual documents
contextual mappings which is useful for practical applications
additionally this paper called contextual mappings
experimental results demonstrate that our approach performs well on several data sets from real world domainsthe process of instantiating an ontology with highquality manually is prone to error
the process of instantiating an ontology with highquality manually is both time consuming
the process of instantiating an ontology with uptodate instance information manually is both time consuming
the process of instantiating an ontology with uptodate instance information manually is prone to error
automatic ontology instantiation from web sources is one of the possible solutions to this problem supported population of an ontology through the exploitation of information available on the web
automatic ontology instantiation from web sources aims at the computer supported population of an ontology through the exploitation of information available on the webin this paper we present a solution for  weaving the claim web  the creation of knowledge networks via socalled claims stated in scientific publications
scientific publications created with the salt  semantically annotated  framework
to attain this objective we provide support for claim identification
to attain this objective we defined reference mechanism
to attain this objective we evolved the appropriate ontologies
to attain this objective we defined a claim citation
a prototypical claim search engine which allows to reference to existing claims and hence weave the web
we also describe a prototypical claim search engine
finally we performed a smallscale evaluation of the salt semantically annotated framework with a quite promising outcomeextracting semantic relations is of great importance for the creation of the semantic web content
it is of great benefit to semiautomatically extract relations from the free text of wikipedia
the free text of wikipedia using the structured content readily available in it
pattern matching methods can not work well since there is not much redundancy information in wikipedia compared to semantic web
pattern matching methods that employ information redundancy
multiclass classification methods are not reasonable since no classification of relation types is available in wikipedia
in this paper we propose positiveonly relation extraction  for relation extraction from wikipedia text
in this paper we propose pore  for relation extraction from wikipedia text
a stateoftheart positiveonly learning algorithm using bootstrapping to work with fewer positive training exam ples
the core algorithm bpol extends a stateoftheart positiveonly learning algorithm
a stateoftheart positiveonly learning algorithm using strong negative identifi cation to work with fewer positive training exam ples
a stateoftheart positiveonly learning algorithm using transductive inference to work with fewer positive training exam ples
we conducted experiments on several relations with different amount of training data
the experimental results show that bpol can work effectively given only a small amount of positive training examples a multiclass svm
the experimental results show that bpol can work effectively given the experimental results significantly out per forms the original positive learning approaches
the experimental results show that bpol can work effectively given the experimental results significantly out per forms a multiclass svm
the experimental results show that bpol can work effectively given only a small amount of positive training examples the original positive learning approaches
furthermore although pore is applied in the context of wiki pedia the core algorithm bpol is a general approach for ontology population
furthermore although pore is applied in the context of wiki pedia the core algorithm bpol can be adapted to other domainsan important open question in the semantic web is the precise relationship between the rdf  s  semantics
an important open question in the semantic web is the semantics of standard knowledge representation formalisms such as logic programming
an important open question in the semantic web is the semantics of standard knowledge representation formalisms such as description logics
in this paper we address this issue by considering these embeddings in logic
using these embeddings combined with existing results about various fragments of logic we establish several novel complexity results
these embeddings we consider show how techniques from description logics can be used for reasoning with rdf
these embeddings we consider show how techniques from deductive databases can be used for reasoning with rdf
finally we consider establish the data complexity of conjunctive querying for the various rdf entailment regimes
finally we consider querying rdf graphsan endtoend semantic search engine that uses a graph data model to enable interactive query answering over structured data
interlinked data collected from many disparate sources on the web
structured data collected from many disparate sources on the web
we present the architecture of an endtoend semantic search engine
an endtoend semantic search engine that uses a graph data model to enable interactive query answering over interlinked data
in particular we study parallel query evaluation methods on a cluster of computers
in particular we study distributed indexing methods for graphstructured data
we evaluate the system on a dataset with 430 million statements
provide scaleup experiments on 7 billion synthetically generated statements
430 million statements collected from the webmerging of rdf models
in this paper we describe rdfsync
in this paper we describe a methodology for efficient synchronization
rdfsync is based on decomposing a model into minimum selfcontained graphs
after deriving properties of minimum selfcontained graphs we show how a rdf model can be represented by a list of hashes of such information fragments
after illustrating theory we show how a rdf model can be represented by a list of hashes of such information fragments
the synchronization procedure here described is based on the evaluation comparison of these
these ordered lists
the synchronization procedure here described is based on the remote comparison of these
experimental results show that the synchronization procedure here described provides very significant savings on network traffic compared to the fileoriented synchronization of serialized rdf graphs
finally we provide the design
finally we report the implementation of a protocol for executing the synchronization procedure here described over httpseveral proposals have been put forward to support distributed agent cooperation in the semantic web by allowing roles in one ontology be reused in another ontology
several proposals have been put forward to support distributed agent cooperation in the semantic web by allowing concepts in one ontology be reused in another ontology
in general several proposals reduce the autonomy of each ontology by defining the semantics of the ontology to depend on the semantics of the other ontologiesontologybased applications play an increasingly important role in the public semantic web
ontologybased applications play an increasingly important role in the corporate semantic web
while today there exist a range of tools to support specific ontology engineering architectural design guidelines for building ontologybased applications are missing
while today there exist a range of technologies to support management activities architectural design guidelines for building ontologybased applications are missing
while today there exist a range of tools to support management activities architectural design guidelines for building ontologybased applications are missing
while today there exist a range of technologies to support specific ontology engineering architectural design guidelines for building ontologybased applications are missing
ontologybased applications covering the complete ontologylifecycle that is intended to support software engineers in developing ontologybased applications
ontologybased applications covering the complete ontologylifecycle that is intended to support software engineers in designing ontologybased applications
in this paper we present an architecture for ontologybased applications covering the complete ontologylifecycle
we illustrate the use of the architecture in a concrete case study
a concrete case study using the neon toolkit as one implementation of the architecturethe development of ontologies involves relatively small modifications
the development of ontologies involves continuous modifications
existing ontology reasoners however do not take advantage of the similarities between different versions of an ontology
reasoning that reuses information
in this paper we propose a technique for incremental reasoning that is reasoning  based on the notion of a module
information obtained from previous versions of an ontology
we technique does not depend can be used in combination with any reasoner
we technique does not depend on a particular reasoning calculus
we have applied we found significant improvement over regular classification time on a set of realworld ontologies
we have applied we results to incremental classification of owl dl ontologiesthe increased availability of online knowledge has led by dynamically exploring a multitude of online ontologies
the increased availability of online knowledge has led by dynamically selecting a multitude of online ontologies
several algorithms that solve a variety of tasks by harvesting the semantic web
the increased availability of online knowledge has led to the design of several algorithms
our hypothesis is that the performance of such novel algorithms thus opens the way to a taskbased evaluation of the semantic web
our hypothesis is that the performance of such novel algorithms implicitly provides an insight into the quality of the used ontologies
our have investigated our hypothesis by studying ontology matching folksonomy enrichment
the lessons learnt about online ontologies
our have investigated our hypothesis by studying word sense disambiguation
our have investigated our hypothesis by studying the lessons
online ontologies when used to solve three tasks
our analysis complement the findings of other analysis of the the semantic web landscape
the semantic web which highlight a number of weaknesses of the semantic information available online
the semantic web which highlight a number of strengths of the semantic information available online
our analysis leads to a suit of conclusions about the status of the semantic webwe present a semanticbased approach to multiissue bilateral negotiation for ecommerce
we use description logics to model relations among issues as axioms in a tbox
we use description logics to model advertisements as axioms in a tbox
we then introduce a logicbased alternatingoffers protocol able to handle conflicting information that merges nonstandard reasoning services in description logics with utility thoery to find the most suitable agreements
we motivate the logical language
we motivate the theoretical framework
we motivate the negotiation protocol
we illustrate the theoretical framework
we illustrate the negotiation protocol
we illustrate the logical languagedespite the success of the web ontology language owl the development of expressive means for querying owl knowledge bases is still an open issue
in this paper we investigate how a very natural and desirable form of queries namely conjunctive ones can be used in conjunction with owl such that one of the major design criteria of the latter namely decidability can be retained
more precisely we show that querying unrestricted el is decidable
we also provide a complexity analysis
we also show that querying unrestricted el is undecidablethe discovery of suitable web services for a given task is one of the central operations in serviceoriented architectures
research on semantic web services aims at automating this step
semantic matchmaking become important
for the large amount of available web services that can be expected in realworld settings
automated discovery based on semantic matchmaking
for the large amount of available web services that can be expected in the computational costs of automated discovery
to make a discovery engine a reliable software component we must thus aim at minimizing both the mean of the duration of the discovery task
to make a discovery engine a reliable software component we must thus aim at minimizing both the variance of the duration of the discovery task
environments that exploits previous discovery results for reducing the search space of consequent discovery operations
for this we present an extension for discovery engines in research on semantic web services environments
environments that exploits structural knowledge results for reducing the search space of consequent discovery operations
we prototype implementation shows significant improvements when applied to the stanford research on semantic web services challenge scenario and datasetwe study the continuous evaluation of conjunctive triple pattern queries over rdf data stored in distributed hash tables
rdf triples satisfying longstanding queries queries
in a continuous query scenario network nodes subscribe with longstanding queries
in a continuous query scenario network nodes receive answers whenever rdf triples are published
we analyze two novel query processing algorithms for this scenario properties formally
we present two novel query processing algorithms for this scenario
we performance goal is to distribute query processing load evenly
we performance goal is to incur as little network traffic as possible
we performance goal is to distribute the storage evenly
we performance goal is to have algorithms
algorithms that scale to large amounts of rdf data
we discuss the various performance tradeoffs
the various performance tradeoffs that occur through a detailed experimental evaluation of the proposed algorithmsrecently the world wide web consortium produced a standard set of  semantic annotations for wsdl   sawsdl 
recently the world wide web consortium produced a standard set of  semantic annotations for xml schema   sawsdl 
sawsdl provides a standard means  owl for services 
a standard means by which wsdl documents can be related to semantic descriptions such as those
sawsdl provides other semantic web services frameworks
those provided by owls
we argue that the value of sawsdl can not be realized until sawsdl use is specified
sawsdl benefits explained in connection with a particular framework
meeting that need with respect to owls
this paper is an important first step toward meeting
provide a rationale and guidelines for owls constructs use
we explain what owls constructs are appropriate for use with the various sawsdl annotations
in addition we discuss some weaknesses of sawsdl
in addition we identify some ways
some ways in which owls could evolve so as to integrate more smoothly with sawsdlthis paper deals with the problem of exploring hierarchical semantics from social annotations
recently social annotation services have become more popular in semantic web
recently social annotation services have become more popular in semantic web
social annotation services allows users to arbitrarily annotate web resources thus largely lowers the barrier to cooperation
furthermore through providing abundant metadata resources social annotation might become a key to the development of semantic web
however on the other hand social annotation has 2  lack of hierarchical information
however on the other hand social annotation has 1  ambiguity and synonym phenomena
however on the other hand social annotation has the other hand own apparent limitations for instance
in this paper we propose an unsupervised model to automatically derive hierarchical semantics from social annotations
the derived hierarchical semantics
using a social bookmark service delicious as example we demonstrate that the has the ability to compensate those shortcomings
we further apply we model on another data set from flickr to testify we models applicability on different environments
the experimental results demonstrate our models efficiency
