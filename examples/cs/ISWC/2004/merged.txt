a paper entitled small can be beautiful in knowledge representation in order to guarantee good computational properties
knowledge representation in which peter patelschneider advocated for limiting the expressive power of knowledge representation formalisms
in 1984 peter patelschneider published a paper
in 1984 peter patelschneider thus make knowledgebased systems usable as part of larger systems
in a paper we have to limit the expressive power of the ontologies
the ontologies serving as semantic marking up of resources
a paper entitled small can be beautiful in knowledge representation in order to guarantee good computational properties peter patelschneider aim at showing that the same argument holds for the future semantic web if we want to give a chance for the future semantic web to scale up
a paper entitled small can be beautiful in knowledge representation in order to guarantee good computational properties peter patelschneider aim at showing that the same argument holds for the future semantic web if we want to give a chance for the future semantic web to be broadly used
knowledge representation in which he advocated for limiting the expressive power of knowledge representation formalisms
in addition due to the scale of the future semantic web users it is unavoidable to have to deal with distributed heterogeneous ontologies
in addition due to the disparity of the future semantic web users it is unavoidable to have to deal with distributed heterogeneous ontologies
in a paper peter patelschneider will argue that a peertopeer infrastructure enriched with simple distributed ontologies  is appropriate and scalable for supporting the future semantic web
in a paper peter patelschneider will argue that a peertopeer infrastructure enriched withtaxonomies  is appropriate and scalable for supporting the future semantic web
a paper entitled small can be beautiful in knowledge representation in order to guarantee good computational properties
knowledge representation in which he advocated for limiting the expressive power of knowledge representation formalisms
simple distributed ontologies  reconciled through mappings
eg taxonomies  reconciled through mappingsthis talk explores aspects relevant for peertopeer search infrastructures
peertopeer search infrastructures which we think are better suited to semantic web search than centralized approaches
this talk does so in the form of an  incomplete  cookbook recipe
an  incomplete  cookbook recipe listing necessary ingredients for putting together a distributed search infrastructure
the reader has to be aware though that the reader needs quite a few more research papers on these aspects before we can really serve the final infrastructure
the reader has to be aware though that many of these ingredients are research questions before we can really serve the final infrastructure
the reader has to be aware though that many of these ingredients are solutions before we can really serve the final infrastructure
the reader has to be aware though that many of these ingredients are research questions before we can really cook the final infrastructure
the reader has to be aware though that many of these ingredients are solutions before we can really cook the final infrastructure
the reader has to be aware though that the reader needs quite a few more research papers on these aspects before we can really cook the final infrastructure
appropriate references as examples for the aspects discussed
we will include appropriate references as examples for the aspects  with some bias to we own work at l3s  though a complete literature overview would go well beyond cookbook recipe length limitswe present a framework to facilitate automated synthesis of scientific experiment workflows in semantic grids
semantic grids based on highlevel goal specification
we framework has two main features
two main features which distinguish our framework from other work in this area
first we propose a adaptive mechanism for automating the construction of experiment workflows
first we propose a dynamic mechanism for automating the construction of experiment workflows
second we distinguish between different levels of abstraction of loosely coupled experiment workflows to facilitate reuse of experiments
second we distinguish between different levels of abstraction of loosely coupled experiment workflows to facilitate sharing of experiments
we illustrate our framework using a real world application scenario in the physics domain
the physics domain involving the detection of gravitational waves from astrophysical sourcessemiautomatic mapping of ontologies is a core task to achieve interoperability when two agents use different ontologies
semiautomatic mapping of ontologies is a core task to achieve interoperability when services use different ontologies
in the existing literature the focus has so far been on improving the quality of mapping results
we here consider qom quick ontology mapping as a way to trade off between effectiveness of the mapping generation algorithms
we here consider qom quick ontology mapping as a way to trade off between efficiency of the mapping generation algorithms
we show that qom has lower runtime complexity than existing prominent approaches
then we show in experiments that this theoretical investigation translates into practical benefits
while qom gives up some of the possibilities for producing highquality results in favor of efficiency we experiments show that this loss of quality is marginalin this paper we present an evaluation of four knowledge base systems with respect to use in large owl applications
data used here
to our knowledge no experiment has been done with the scale of data
the smallest dataset used consists of 15 owl files totaling 8 mb while the smallest dataset consists of 999 files
999 files totaling 583 mb
our evaluated memorybased sesame 
our evaluated owljesskb 
our evaluated two systems with persistent storage 
our evaluated two memorybased systems 
our evaluated two systems with databasebased sesame 
our evaluated two systems with dldbowl 
our describe what factors our have considered in the evaluation
our describe how our have performed the evaluation
our show the results of the experiment
our discuss the performance of each system
in particular our have concluded that existing systems need to place a greater emphasis on scalabilitytools that can validate ontological consistency
as the use of semantic web ontologies continues to expand there is a growing need for tools
tools that can provide guidance in the correction of detected defects and errors
a number of tools already exist as evidenced by the ten systems
the ten systems participating in the w3cs evaluation of the owl test cases
for the most part these first generation tools focus on experimental approaches to consistency checking how the systems might interoperate
for the most part these first generation tools focus on experimental approaches to consistency checking while minimal attention is paid to how the results will be used
for this reason very few of the systems produce results in a machinereadable format  for example as owl annotations  and there is no shared notion across the tools of how to identify what it is that makes a specific ontology inconsistent
for this reason very few of the systems produce results in a machinereadable format  for example as owl annotations  and there is no shared notion across the tools of how to identify what it is that makes annotation inconsistent
for this reason very few of the systems produce results in a machinereadable format  for example as owl annotations  and there is no shared notion across the tools of how to describe what it is that makes a specific ontology inconsistent
for this reason very few of the systems produce results in a machinereadable format  for example as owl annotations  and there is no shared notion across the tools of how to describe what it is that makes annotation inconsistent
in this paper we propose the development of a symptom ontology for the semantic web we refer to such errors and warnings as symptoms
the semantic web that would serve as a common language for identifying semantic errors and warnings
semantic errors and warnings that may be indicative of inconsistencies in annotations
the semantic web that would serve as a common language for describing semantic errors and warnings
semantic errors and warnings that may be indicative of inconsistencies in ontologies
we offer the symptom ontology currently used by the consvisor consistencychecking tool as the starting point for a discussion on the desirable characteristics of such an ontology
included among the desirable characteristics of such an ontology are 2  clear associations between specific symptoms 3  a means for relating individual symptoms back to the specific constructs in the input file
included among the desirable characteristics of such an ontology are the axioms of the languages specific symptoms violate
included among the desirable characteristics of such an ontology are 1  a hierarchy of common symptoms 3  a means for relating individual symptoms back to the specific constructs in the input file
included among the desirable characteristics of such an ontology are 2  clear associations between specific symptoms specific symptoms violate
included among the desirable characteristics of such an ontology are the axioms of the languages 3  a means for relating individual symptoms back to the specific constructs in the input file
included among the desirable characteristics of such an ontology are 1  a hierarchy of common symptoms specific symptoms violate
the input file through which specific symptoms were implicated
its extension to syntactic symptoms extension to syntactic symptoms
this work
we conclude with a number of suggestions for future directions of
we conclude with a number of suggestions for future directions of this work
this workinformation retrieval can contribute towards the effective usage of ontologies
information retrieval can contribute towards the construction of ontologies
we use collocationbased keyword extraction to suggest new concepts
we use collocationbased keyword extraction to study the generation of hyperlinks to automate the population of ontologies with instances
methods within the setting of digital library project using information retrieval evaluation methodology
we evaluate we methods within the setting of digital library project
retrieval methods that complement the navigational support
the navigational support offered by the semantic relations in most ontologies to help users explore the ontology
within the same setting we study retrieval methodsontotrack is editing  inoneview  ontology authoring tool
ontotrack is a new browsing  ontology authoring tool
ontology authoring tool that combines a hierarchical graphical layout for  the most rational fraction of  owl lite
ontology authoring tool that combines instant reasoning feedback for  the most rational fraction of  owl lite
ontotrack provides an animated view with context sensitive features like selective detail views together with draganddrop editing
ontotrack provides an zoomable view with context sensitive features like selective detail views together with draganddrop editing
ontotrack provides an animated view with context sensitive features like clickable miniature branches together with draganddrop editing
ontotrack provides an zoomable view with context sensitive features like clickable miniature branches together with draganddrop editing
each editing step is instantly synchronized with an external reasoner in order to provide appropriate graphical feedback about relevant modeling consequences
the most recent feature of ontotrack is an on demand textual explanation for subsumption between
the most recent feature of ontotrack is unsatisfiability of classes
the most recent feature of ontotrack is an on demand textual explanation for equivalence between
this paper discusses some development issues
this paper describes the key features of the current implementation
this paper discusses future workthis paper describes the design and implementation of a peertopeer system for exchanging bibliographic data among researchers
this paper describes the design and implementation of bibster 
a peertopeer system for exchanging bibliographic data among researchers answer presentation when bibliographic entries are made available for use in bibster  bibliographic entries are structured according to two different ontologies
a peertopeer system for exchanging bibliographic data among researchers exploits ontologies in data storage query formulation query when bibliographic entries are made available for use in bibster  bibliographic entries are structured according to two different ontologies
bibster exploits ontologies in data storage query formulation query when bibliographic entries are made available for use in a peertopeer system for exchanging bibliographic data among researchers bibliographic entries are classified according to two different ontologies
a peertopeer system for exchanging bibliographic data among researchers exploits ontologies in data storage query formulation query when bibliographic entries are made available for use in a peertopeer system for exchanging bibliographic data among researchers bibliographic entries are classified according to two different ontologies
query routing
bibster answer presentation when bibliographic entries are made available for use in bibster  bibliographic entries are classified according to two different ontologies
a peertopeer system for exchanging bibliographic data among researchers exploits ontologies in data storage query formulation query when bibliographic entries are made available for use in bibster  bibliographic entries are classified according to two different ontologies
a peertopeer system for exchanging bibliographic data among researchers answer presentation when bibliographic entries are made available for use in bibster  bibliographic entries are classified according to two different ontologies
bibster answer presentation when bibliographic entries are made available for use in a peertopeer system for exchanging bibliographic data among researchers bibliographic entries are classified according to two different ontologies
bibster answer presentation when bibliographic entries are made available for use in a peertopeer system for exchanging bibliographic data among researchers bibliographic entries are structured according to two different ontologies
a peertopeer system for exchanging bibliographic data among researchers answer presentation when bibliographic entries are made available for use in a peertopeer system for exchanging bibliographic data among researchers bibliographic entries are structured according to two different ontologies
bibster exploits ontologies in data storage query formulation query when bibliographic entries are made available for use in bibster  bibliographic entries are structured according to two different ontologies
bibster exploits ontologies in data storage query formulation query when bibliographic entries are made available for use in bibster  bibliographic entries are classified according to two different ontologies
bibster answer presentation when bibliographic entries are made available for use in bibster  bibliographic entries are structured according to two different ontologies
a peertopeer system for exchanging bibliographic data among researchers answer presentation when bibliographic entries are made available for use in a peertopeer system for exchanging bibliographic data among researchers bibliographic entries are classified according to two different ontologies
a peertopeer system for exchanging bibliographic data among researchers exploits ontologies in data storage query formulation query when bibliographic entries are made available for use in a peertopeer system for exchanging bibliographic data among researchers bibliographic entries are structured according to two different ontologies
bibster exploits ontologies in data storage query formulation query when bibliographic entries are made available for use in a peertopeer system for exchanging bibliographic data among researchers bibliographic entries are structured according to two different ontologies
this ontological structure is then exploited to help users formulate users queries
subsequently two different ontologies are used to improve query
query routing across the peertopeer network
finally two different ontologies are used to postprocess the returned answers in order to do duplicate detection
this paper describes each of these ontologybased aspects of bibster 
this paper describes each of these ontologybased aspects of a peertopeer system for exchanging bibliographic data among researchers
a peertopeer system for exchanging bibliographic data among researchers is a fully implemented open source solution
bibster is a fully implemented open source solution
open source solution built on top of the jxta platformthe increasing awareness of the benefits of these ontologies has lead to the creation of a number of large ontologies about realworld domains
the size of these ontologies monolithic character because serious problems in handling these ontologies
the size of these ontologies because serious problems in handling these ontologies
in other areassoftware engineering these problems are tackled by partitioning monolithic entities into sets of meaningful modules
in other areassoftware engineering these problems are tackled by partitioning monolithic entities into sets of mostly selfcontained modules
in this paper we suggest a similar approach for ontologies
we propose a method for automatically partitioning large ontologies into smaller modules
smaller modules based on the structure of the class hierarchy
we show that the structurebased method performs surprisingly well on realworld ontologies
sumo
realworld ontologies
we support this claim by these experiments carried out on
the nci cancer ontology
we support this claim by these experiments carried out on realworld ontologies
the results of these experiments are available online at http swservercsvunlpartitioninga central requirement for achieving the vision of runtime discovery is the provision of appropriate descriptions of the operation of a service
a service that is how a service interacts with other services
a service that is how a service interacts with agents
a central requirement for achieving dynamic composition of services is the provision of appropriate descriptions of the operation of a service
in this paper we use experience gained through the development of reallife grid applications to produce a set of those requirements to match those requirements against the offerings of existing work such as owls
in this paper we use experience gained through the development of reallife grid applications to produce a set of those requirements to match those requirements against the offerings of existing work such as irsii
based on this analysis we identify which requirements are not addressed by current research and in response produce a model for describing the interaction protocol of a service in response
interactions initiated by the service
the main contributions of a model are the ability to describe the interactions of multiple parties with respect to a single service distinguish between interactions the service
other cooperating services
parties that are either a result of internal service events or interactions
interactions that are initiated by clients
the main contributions of a model are the ability to describe the interactions of multiple parties with respect to a single service distinguish between interactions interactions or other and capture within the description service state changes relevant to interacting parties
to inform the further development of such work
the aim of a model is not to replace existing work since a model only focuses on the description of the interaction protocol of a servicea knowledgebased environment that enables information to be retrieved effectively
semantic web provides a knowledgebased environment
a knowledgebased environment that enables information to be shared effectively
in this research we propose the scholarly semantic web for the sharing reuse and management of scholarly information
to support the scholarly semantic web for the sharing reuse and management of scholarly information we need to construct ontology from data
data which is a tedious task
data which is a difficult task
to generate ontology automatically formal concept analysis is an effective technique
an effective technique that can formally abstract data as conceptual structures
to enable formal concept analysis to interpret the concept hierarchy reasonably we propose to incorporate fuzzy logic into formal concept analysis for automatic generation of ontology
to enable formal concept analysis to deal with uncertainty in data we propose to incorporate fuzzy logic into formal concept analysis for automatic generation of ontology
the proposed new framework is known as fuzzy formal concept analysis
in this paper we will discuss the scholarly semantic web for the sharing
in this paper we will discuss reuse of scholarly information
in this paper we will discuss management of scholarly information
in this paper we will discuss the ontology generation process from the proposed new framework
in addition the performance of the fuzzy formal concept analysis framework for ontology generation will also be evaluated
in addition the performance of the fuzzy formal concept analysis framework for ontology generation will also be presentedthis paper addresses the problem of building scalable semantic overlay networks
our approach follows the principle of data independence by separating the semantic overlay for mapping data from a physical layer
a physical layer consisting of a structured peertopeer overlay network for efficient routing of messages
our approach follows the principle of data independence by separating the semantic overlay for managing data from a physical layer
our approach follows the principle of data independence by separating metadata schemas from a physical layer
our approach follows the principle of data independence by separating a logical layer from a physical layer
a physical layer consisting of a structured peertopeer overlay network for efficient routing of messages
a physical layer is used to implement various functions at a physical layer including schema management
a physical layer is used to implement various functions at a physical layer including schema mapping management
a physical layer is used to implement various functions at a physical layer including attributebased search
a physical layer consisting of a structured peertopeer overlay network for efficient routing of messages
the semantic overlay using different physical execution strategies
the separation of a physical from a logical layer allows us to process logical operations in the semantic overlay
in particular we identify recursive strategies for the traversal of semantic overlay networks as two important alternatives
in particular we identify iterative strategies for the traversal of semantic overlay networks as two important alternatives
at a physical layer we support semantic interoperability through schema inheritance
at a physical layer we support semantic interoperability through semantic gossiping
a physical layer consisting of a structured peertopeer overlay network for efficient routing of messages
thus our system provides a complete solution to the implementation of semantic overlay networks
semantic overlay networks supporting both scalability
semantic overlay networks supporting both interoperabilityin this paper we describe we experience in applying kaos services to ensure policy compliance for semantic web services workflow composition and enactment
we are developing these capabilities within the context of two applications coalition search
we are developing these capabilities within rescue
we are developing these capabilities within semantic firewall
we describe how we are extending we reasoning mechanisms in a carefully controlled manner to that end
we describe how we are extending we representation mechanisms in a carefully controlled manner to that end
we describe how this work has uncovered requirements for increasing the expressivity of policy beyond what can be done withrolevaluemaps 
we describe how this work has uncovered requirements for increasing the expressivity of policy beyond what can be done with description logic 
since kaos employs owl for policy representation policy representation fits naturally with the use of owls workflow descriptions
owls workflow descriptions generated by the aiai ix planning system in the rescue application
the advanced reasoning mechanisms of kaos are enable the analysis of classes of processes from a policy perspective
the advanced reasoning mechanisms of kaos are enable the analysis of instances of processes from a policy perspective
the advanced reasoning mechanisms of kaos are based on the jtp inference engine
as the result of analysis kaos concludes whether a particular workflow step is allowed by policy
as the result of analysis kaos concludes whether the performance of a particular workflow step would incur additional policygenerated obligations
issues in the representation of processes within owls are described
besides what is done during workflow composition aspects of policy compliance can be checked at runtime when a workflow is enacted
we illustrate these capabilities through two application examples
finally we outline plans for future workdata forcing life science researchers to apply the same rigor to life science researchers
life science researchers increasingly rely on the web as a primary source of data use as to an experiment in the laboratory
the mygrid project is developing the use of provenance to describe why results were produced
the mygrid project is developing the use of provenance to describe how results were produced
the mygrid project is developing the use of workflows to explicitly capture webbased procedures
heterogenous resources that impact on the production of a result
experience within mygrid has shown that this provenance metadata is formed from a complex web of heterogenous resources
therefore we have used existing initiatives such as jena to generate such material
therefore we have used existing initiatives such as lsid to generate such material
therefore we have used existing initiatives such as lsid to store such material
therefore we have explored the use of semantic web technologies such as rdf and ontologies to support ontologies representation
therefore we have used existing initiatives such as jena to store such material
the effective presentation of complex rdf graphs is challenging
provenance metadata that can be further annotated
haystack has been used to provide multiple views of provenance metadata
this work therefore forms a case study
a case study showing how existing semantic web tools can effectively support the emerging requirements of life science researcha description logic that is a fragment of classical firstorder predicate logic
owl a description logic corresponds to a description logic
therefore the standard methods of automated reasoning for full classical firstorder predicate logic can potentially be dedicated a description logic reasoners to solve owl a description logic reasoning tasks
therefore the standard methods of automated reasoning for full classical firstorder predicate logic can potentially be used a description logic reasoners to solve owl a description logic reasoning tasks
some experiments designed to explore the feasibility of using existing generalpurpose classical firstorder predicate logic provers to reason with owl a description logic
in this paper we report on some experiments
we also extend we approach to a proposed rule language extension to owl
we also extend we approach to swrl magpie is a suite of tools magpie avoids the need for manual annotation by automatically associating an ontologybased semantic layer to web resources
tools supporting a  zerocost  approach to semantic web browsing
the association between items on semantic concepts is the enabling condition for locating these services
the association between items on semantic concepts is the enabling condition for making these services available to a user
an important aspect of magpie is that the association between items on a web page is not merely a mechanism for dynamic
an important aspect of magpie is that the association between items on semantic concepts is not merely a mechanism for dynamic
the association between items on a web page is the enabling condition for locating these services
dynamic linking
the association between items on a web page is the enabling condition for making these services available to a user
magpie which differentiates magpie from superficially similar hypermedia systems
these services can be triggered when the appropriate web entities are encountered during a browsing session 
these services can be manually activated by pull services 
these services can be triggered when the appropriate web entities are encountered during push services 
these services can be manually activated by a user 
open as to services  which is a key aspect of the emerging semantic web
in this paper we analyze magpie from the perspective of building semantic web applications
we note that earlier implementations did not fulfill the criterion of  open as to services 
a novel architecture which is open both with respect to ontologies
a radical redesign of magpie resulting in a novel architecture
a novel architecture which is open both with respect to semantic web services
for this reason in the past twelve months we have carried out a radical redesign of magpie
a novel architecture which is open both with respect to ontologies
a novel architecture goes beyond the idea of merely providing support for semantic web browsing
a novel architecture which is open both with respect to semantic web services
a novel architecture can be seen as a software framework for designing semantic web applications
a novel architecture can be seen as a software framework for implementing semantic web applicationsa personal knowledge publishing system called semblog is realized with integration of weblog tools
a personal knowledge publishing system called semblog is realized with integration of semantic web techniques
we propose a personal knowledge publishing system
making human relationship seamlessly to enable people to exchange information with easy fashion
making human relationship seamlessly to enable people to exchange information with casual fashion
making human relationship seamlessly to enable people to exchange knowledge with casual fashion
making human relationship seamlessly to enable people to exchange knowledge with easy fashion
semblog suite provides an integrated environment for gathering authoring publishing
we activities
we use a lightweight metadata format like rss to activate the information flow
we define three level of interest of publishing  check   clip  and  post 
we define three level of interest of information gathering   clip  and  post 
we provide suitable ways to distribute information depending on the interest level
we system consists of two types of information retrieval recommendation applications
we system consists of two types of extended content aggregator
we system called semblog platform
personal ontology that realizes semantic relations among people
we also design a new metadata module to define personal ontology
personal ontology that realizes semantic relations among weblog sitesturning the current web into a semantic web requires automatic approaches for annotation of existing data since manual approaches will not scale in general
tables which subsequently supports the automatic population of ontologies from tablelike structures
we here present an approach for automatic generation of flogic frames out of tables
an approach consists of an accompanying implementation
an approach consists of a methodology
an approach consists of a thorough evaluation
cognitive table model which is stepwise instantiated by we
an approach is based on a grounded cognitive table model methodologyowls allows selecting composing web services at different levels of abstraction selection uses high level abstract descriptions invocation uses low level while composition needs to consider both high and low level descriptions
low level grounding ones
owls allows selecting invoking web services at different levels of abstraction selection uses high level abstract descriptions invocation uses low level while composition needs to consider both high and low level descriptions
in our setting two web services are to be composed so that output from the upstream one is used to create input for the downstream one
two web services may have different data models
two web services are related to each other through low level descriptions
two web services are related to each other through high level descriptions
correspondences must be found between the components
low level data transformation functions may be required unit conversions data type conversions 
the components may be arranged in different xml tree structures
thus multiple data transformations are necessary matching leaves by corresponding types
thus multiple data transformations are necessary reshaping the message tree
thus multiple data transformations are necessary translating through ontologies
thus multiple data transformations are necessary calling conversion functions
our prototype compiles these transformations into a set of data transformation rules using our tableaubasedan analysis of owl ontologies shows that a majority are owl full
owl ontologies represented in rdfxml on the web
in many cases this is rather due to syntactic errors
in many cases this may not be through a desire to use the expressivity provided by owl full
in many cases this is rather due to accidental misuse of the vocabulary
we present a  rogues gallery  of common errors describe
we present a  rogues gallery  of common errors encountered
rogues gallery  of common errors encountered how robust parsers can be produced
robust parsers that attempt to cope with such errors
rogues gallery  of common errors describe how robust parsers can be producedthe standardization of owl leaves reason with multiple linked ontologies
the standardization of the second generation web ontology language  leaves a crucial issue for webbased ontologies unsatisfactorily resolved how to represent
the standardization of the second generation web ontology language  leaves reason with multiple distinct
the standardization of the second generation web ontology language  leaves reason with multiple linked ontologies
the standardization of owl leaves a crucial issue for webbased ontologies unsatisfactorily resolved how to represent
the standardization of owl leaves reason with multiple distinct
owl provides the owl imports construct which roughly allows web ontologies to include other web ontologies but only by merging all the linked ontologies into a single logical  space 
contextual logics have tried to affect each other
the like have tried to affect each other
recent work on other combinations of modal logics distributed have tried to find these formalisms wherein knowledge bases  are kept more distinct
the like have tried to find these formalisms wherein knowledge bases  are kept more distinct
the like have tried to find these formalisms wherein and the like logic  are kept more distinct
contextual logics have tried to find these formalisms wherein and the like logic  are kept more distinct
contextual logics have tried to find these formalisms wherein knowledge bases  are kept more distinct
recent work on fusions distributed have tried to find these formalisms wherein and the like logic  are kept more distinct
recent work on fusions distributed have tried to affect each other
recent work on other combinations of modal logics distributed have tried to affect each other
recent work on multidimensional logics distributed have tried to find these formalisms wherein and the like logic  are kept more distinct
recent work on multidimensional logics distributed have tried to find these formalisms wherein knowledge bases  are kept more distinct
recent work on fusions distributed have tried to find these formalisms wherein knowledge bases  are kept more distinct
recent work on other combinations of modal logics distributed have tried to find these formalisms wherein and the like logic  are kept more distinct
recent work on multidimensional logics distributed have tried to affect each other
these formalisms have various degrees of robustness in these formalisms modularity these formalisms expressivity
these formalisms have various degrees of robustness in these formalisms intuitiveness to modelers
these formalisms have various degrees of robustness in these formalisms computational complexity
a novel subformalism that seems very straightforward to implement on existing tableau owl reasoners as witnessed by we implementation of this formalism in the owl reasoner pellet
such formalisms grounded in econnections as extensions to owl with emphasis on a novel subformalism
in this paper we explore a family of such formalisms
we discuss how to integrate these formalisms into owl that we expect to be the norm on the semantic web
we discuss how to integrate these formalisms into some of the issues that modelers have to face when using such formalisms in the context of a large number of heterogeneous independently developed richly interconnected ontologies that we expect to be the norm on the semantic webas ontology development becomes a collaborative process developers face the problem of maintaining versions of ontologies akin to maintaining versions of software code
as ontology development becomes a collaborative process developers face the problem of maintaining versions of ontologies akin to maintaining versions of documents in large projects
traditional versioning systems enable users to examine changes
traditional versioning systems enable users to accept changes
traditional versioning systems enable users to compare versions
traditional versioning systems enable users to reject changes
however while versioning systems usually treat software code as text files a versioning system for ontologies must changes in text representation of ontologies
however while versioning systems usually treat software code as text files a versioning system for ontologies must compare
however while versioning systems usually treat text documents as text files a versioning system for ontologies must changes in text representation of ontologies
however while versioning systems usually treat text documents as text files a versioning system for ontologies must present structural changes in text representation of ontologies
however while versioning systems usually treat text documents as text files a versioning system for ontologies must compare
however while versioning systems usually treat software code as text files a versioning system for ontologies must present structural changes in text representation of ontologies
the promptdiff ontologyversioning environment which address these challenges
in this paper we present the promptdiff ontologyversioning environment
promptdiff includes an efficient versioncomparison algorithm
an efficient versioncomparison algorithm that produces a structural diff between ontologies
the changes that enables users to view concepts of concepts
the results are presented to the users through an intuitive user interface for analyzing the changes
concepts that were added deleted
direct access to additional information characterizing the change
the changes that enables users to view groups of concepts
an intuitive user interface for analyzing the changes distinguished by users appearance
the results are presented to the users with direct access to additional information
the changes that moved
the users can then act on the changes accepting users
the users can then act on the changes rejecting users
a pilot user study that demonstrate the effectiveness of the tool for change management
we present results of a pilot user study
position ontology versioning as a component in a general ontologymanagement framework
we discuss design principles for an endtoend ontologyversioning environment
an endtoend ontologyversioning environment versioning as a component in a general ontologymanagement framework
we discuss design principles for position ontologyin this paper we present an abstract conceptual architecture for semantic web services
we define requirements on an abstract conceptual architecture for semantic web services by analyzing a set of case studies developed as part of the eu semantic webenabled web services project
an abstract conceptual architecture for semantic web services is developed as a refinement and extension of the w3c web services architecture
we provide an analysis of owls
we assess the w3c web services architecture against the requirementsa central theme of the semantic web is that programs should be able to easily aggregate data from different sources
unfortunately even if two sites provide two sites data using vocabulary subtle differences in two sites use of terms make pose challenges for aggregation
unfortunately even if two sites provide two sites data using the same data model subtle differences in two sites use of terms make pose challenges for aggregation
unfortunately even if two sites provide two sites data using the same data model subtle differences in two sites in the assumptions two sites make pose challenges for aggregation
unfortunately even if two sites provide two sites data using vocabulary subtle differences in two sites in the assumptions two sites make pose challenges for aggregation
the phenomena that pose obstacles to a simplistic model of aggregation
experiences with the tap project reveal some of the phenomena
cyc which has led to the development and use of various context mechanisms
similar experiences have been reported by ai projects such as cyc
in this paper we report on some of the problems with aggregating independently published data
in this paper we propose a context mechanism to handle some of the problems
the context mechanisms developed in ai with the requirements of a context mechanism for the semantic web
the context mechanisms developed in contrast with the requirements of a context mechanism for the semantic web
we briefly survey the context mechanisms developed in ai the context mechanisms
we briefly survey the context mechanisms developed in contrast the context mechanisms
finally we present a context mechanism for the semantic web yet simple from both model theoretic perspectives
the semantic web that is adequate to handle the aggregation tasks
finally we present a context mechanism for the semantic web yet simple from both computationalincreasing the number of peers in a peertopeer network usually increases the number of answers to a given query as well
while having more answers is nice in principle users are not interested in arbitrarily large answer sets answers
while having more answers is nice in principle users are not interested in unordered answer sets answers
while having more answers is nice in principle users are not interested in a small set of  best  answers
peertopeer networks which makes use of local rankings rank merging
inspired by the success of ranking algorithms in topk query evaluation algorithms in databases we propose a decentralized topk query evaluation algorithm for peertopeer networks
peertopeer networks which makes use of local rankings rank optimized routing based on peer ranks
peertopeer networks which minimizes both answer
both answer set network traffic among peers
both answer set size traffic among peers
inspired by the success of ranking algorithms in web search engine in databases we propose a decentralized topk query evaluation algorithm for peertopeer networks
as our algorithm is based on dynamically collected query statistics only no continuous index update processes are necessary allowing our algorithm to scale easily to large numbers of peersdifferent planning techniques have been applied to the problem of automated composition of web services
complex goals expressing temporal conditions
however in realistic cases this planning problem is far from trivial the planner needs to deal with with complex goals
complex goals expressing preference requirements
however in realistic cases this planning problem is far from trivial the planner needs to deal with the nondeterministic behavior of web services
however in realistic cases this planning problem is far from trivial the planner needs to deal with the partial observability of web services internal status
we propose a planning technique for the automated composition of web services
web services described in owls process models
owls process models which can deal effectively with complex goals
owls process models which can deal effectively with nondeterminism
owls process models which can deal effectively with partial observability
plans that encode compositions of web services with the usual programming constructs like conditionals
owls process models which can deal effectively with partial observability
plans that encode compositions of web services with the usual programming constructs like iterations
a planning technique for the automated composition of web services allows for the synthesis of plans
web services described in owls process models
owls process models which can deal effectively with nondeterminism
owls process models which can deal effectively with complex goals
the generated plans can thus be translated into executable processes
the generated plans can thus be translated into bpel4ws programs
we do the gain in performance of automating the composition at the semantic level wrt the automated composition at the level of executable processes
we do some preliminary experimental evaluations
some preliminary experimental evaluations that show the potentialities of we approach
we implement we solution in a plannerrdf graphs are sets of assertions in the form of subjectpredicateobject triples of information resources
although for simple examples simple examples can be understood intuitively as directed labeled graphs this representation does not scale well for more complex cases particularly regarding the central notion of connectivity of resourcesthis paper describes a method for converting existing thesauri native format to owl
this paper describes a method for converting related resources from existing thesauri native format to owl
this paper describes a method for converting related resources from existing thesauri native format to rdf
this paper describes a method for converting existing thesauri native format to rdf
a method identifies four steps in the conversion process
in each step decisions have to be taken with respect to the syntax or semantics of the resulting representation
each step is supported through a number of guidelines
a method is illustrated through conversions of two large thesauri wordnet
a method is illustrated through conversions of two large thesauri meshwe show that the semantic web needs a formal semantics for the various kinds of links between ontologies
we show that the semantic web needs a formal semantics for the various kinds of links between other documents
we provide a model theoretic semantics
theoretic semantics that takes into account ontology extension
theoretic semantics that takes into ontology versioning
since the semantic web is the product of a diverse community as opposed to a single agent this semantics accommodates different viewpoints by having different entailment relations for different ontology perspectives
we discuss how this theory can be practically applied to owl
we discuss how this theory can provide a theorem
a theorem that shows how to compute perspectivebased entailment using existing logical reasoners
we discuss how this theory can be practically applied to rdf
we conclude with a discussion of future work
we illustrate these concepts using exampleswe argue that in a distributed context such as the semantic web ontology engineers and data creators often can not control  or even imagine  the possible uses creators data might have
we argue that in a distributed context such as the semantic web ontology engineers and data creators often can not control  or even imagine  the possible uses creators ontologies might have
therefore ontologies are unlikely to identify every useful classification possible in a problem domain
for example these might be of a personalised nature
therefore ontologies are unlikely to identify every interesting classification possible in a problem domain
these might be of a different granularity than the initial scope of the ontology
for example these might be appropriate for a certain user in a certain context
we argue that machine learning techniques will be essential within the semantic web context to allow these unspecified classifications to be identified
in this paper we explore the application of machine learning methods to foaf highlighting the challenges
the challenges posed by the characteristics of such data
specifically we use clustering to identify classes of people to learn descriptions of these groups
specifically we use clustering to identify classes of inductive logic programming to learn descriptions of these groups
we argue that these new descriptions constitute reusable first class knowledge
reusable first class knowledge that is neither explicitly stated
reusable first class knowledge that deducible from the input data
these new descriptions can be represented as simple owl class restrictions using swrl
these new descriptions can be represented as more sophisticated descriptions using swrl
these are then suitable either for onthefly use for personalisation tasks
these are then suitable either for incorporation into future versions of ontologiesthe emergent semantic web despite being in the emergent semantic web infancy has already received a lot of attention from academia
the emergent semantic web despite being in the emergent semantic web infancy has already received a lot of attention from industry
this resulted in an abundance of prototype systems
discussion most of which are centred around the underlying infrastructure
this resulted in discussion
the work done to date
however when we critically review the work we realise that there is little discussion with respect to the vision of the emergent semantic web
in particular there is an observed dearth of discussion on how to deliver knowledge sharing in an environment such as the semantic web in effective manners
in particular there is an observed dearth of discussion on how to deliver knowledge sharing in an environment such as the semantic web in efficient manners
there are a lot of overlooked issues
hidden assumptions made with respect to knowledge representation
hidden assumptions made with respect to robust reasoning in a distributed environment
overlooked issues associated with agents to hidden assumptions
overlooked issues associated with trust to hidden assumptions
overlooked issues could potentially hinder further development if not considered at the early stages of designing semantic web systems
hidden assumptions made with respect to knowledge representation
overlooked issues associated with trust to hidden assumptions
overlooked issues associated with agents to hidden assumptions
hidden assumptions made with respect to robust reasoning in a distributed environment
in this perspectives paper we aim to help practitioners of the emergent semantic web by raising awareness of overlooked issues
overlooked issues associated with agents to hidden assumptions
in this perspectives paper we aim to help engineers of the emergent semantic web by raising awareness of overlooked issues
hidden assumptions made with respect to knowledge representation
hidden assumptions made with respect to robust reasoning in a distributed environment
overlooked issues associated with trust to hidden assumptionsfunctionfree horn rules are decidable logics with interesting yet orthogonal expressive power from the rules perspective owldl is restricted to treelike rules but provides both existentially and universally quantified variables
both owldl are decidable logics with interesting yet orthogonal expressive power from the rules perspective owldl is restricted to treelike rules but provides both existentially and universally quantified full monotonic negation
functionfree horn rules are decidable logics with interesting yet orthogonal expressive power from the rules perspective owldl is restricted to treelike rules but provides both existentially and universally quantified full monotonic negation
both owldl are decidable logics with interesting yet orthogonal expressive power from the rules perspective owldl is restricted to treelike rules but provides both existentially and universally quantified variables
from the description logic perspective rules are restricted to universal quantification
from the description logic perspective rules allow for the interaction of variables in arbitrary ways
clearly
several such combinations have already been discussed
a combination of rules is desirable for building semantic web ontologies
a combination of owldl is desirable for building semantic web ontologies
however such a combination might easily lead to the undecidability of interesting reasoning problems
a decidable such combination which is to the best of we knowledge more general than similar decidable combinations
here we present a decidable such combination
similar decidable combinations proposed so far
decidability is obtained by restricting rules to socalled dlsafe ones requiring each variable in a rule to occur in a nondlatom in the rule body
we show that query answering in such a combined logic is decidable
we discuss query answering in such a combined logic expressive power by means of a nontrivial example
finally we present an algorithm for query answering in shiq extended with dlsafe rules based on the reduction to disjunctive datalogwe introduce the the web ontology language plugin a semantic web extension of the protege ontology development platform
the the web ontology language plugin a semantic web extension of the protege ontology development platform can be used to acquire instances for semantic markup
the the web ontology language plugin a semantic web extension of the protege ontology development platform can be used to edit ontologies in the web ontology language to access description logic reasoners
in many of these features the the web ontology language plugin a semantic web extension of the protege ontology development platform has created new practices for building semantic web contents feedback from we users
in many of these features the the web ontology language plugin a semantic web extension of the protege ontology development platform has created new practices for building semantic web contents often driven by the needs of
in many of these features the the web ontology language plugin a semantic web extension of the protege ontology development platform has facilitated new practices for building semantic web contents often driven by the needs of
in many of these features the the web ontology language plugin a semantic web extension of the protege ontology development platform has facilitated new practices for building semantic web contents feedback from we users
furthermore proteges flexible opensource platform means that it is easy to integrate customtailored components to build realworld applications
this document describes the architecture of the the web a semantic web extension of the protege ontology development platform discusses some of we design decisions
this document describes the architecture of the the web ontology language plugin  walks through the the web ontology language plugin a semantic web extension of the protege ontology development platform most important features
this document describes the architecture of the the web ontology language plugin  discusses some of we design decisions
this document describes the architecture of the the web a semantic web extension of the protege ontology development platform walks through the the web ontology language plugin a semantic web extension of the protege ontology development platform most important featuresbuilding userfriendly guis for browsing rdfs description bases while exploiting in a transparent way the expressiveness of declarative queryview languages is vital forelearning escience 
building userfriendly guis for filtering rdfs description bases while exploiting in a transparent way the expressiveness of declarative queryview languages is vital forelearning escience 
building userfriendly guis for filtering rdfs description bases while exploiting in a transparent way the expressiveness of declarative queryview languages is vital for various semantic web applications 
building userfriendly guis for browsing rdfs description bases while exploiting in a transparent way the expressiveness of declarative queryview languages is vital for various semantic web applications 
which relies on the full power of the rdfs data model for constructing on the fly queries
the fly queries expressed in rql
a novel interface called grql
grql
in this paper we present a novel interface
path expressions required to access the resources of interest
more precisely a user can generate transparently the rql path expressions
more precisely a user can navigate graphically through the individual rdfs class
more precisely a user can navigate graphically through property definitions
path expressions required to access the resources of interest capture accurately the meaning of its navigation steps through the class  or property  subsumption andor associations
the resources class appearing in the query result
additionally users can enrich the generated queries with filtering conditions on the attributes of the currently visited class while users can easily specify the resources class
to the best of our knowledge grql is the first applicationindependent gui able to generate a unique rql query
a unique rql query which captures the cumulative effect of an entire user navigation sessionthis paper introduces a visual umlbased notation for owl ontologies
a standard mof2 compliant metamodel which captures the language primitives
the language primitives offered by owl dl
we provide a standard mof2 compliant metamodel
similarly we invent a uml profile
a uml profile which allows to visually model owl ontologies in a notation
a notation that is close to the uml notation
this allows to develop ontologies
ontologies using uml tools
throughout this paper the significant differences to some earlier proposals for a visual umlbased notation for ontologies are discussedan open environment populated by large numbers of heterogeneous information services
in an open environment integration is a major challenge
in such a setting the efficient coupling between service composition engines is crucial
in such a setting the efficient coupling between directorybased service discovery is crucial
a directory service that offers specific functionality in order to enable efficient service integration
in this paper we present a directory service
the directory implementation relies on a multidimensional index structure
the directory implementation relies on a compact numerical encoding of service parameters
the directory implementation supports isolated service integration sessions
isolated service integration sessions providing a consistent view of the directory data
during a session a client may retrieve the results incrementally
during a session a client may issue multiple queries to the directory
in order to optimize the interaction of the directory with different service composition algorithms the directory supports custom ranking functions
custom ranking functions that are dynamically installed with the aid of mobile code
the directory service imposes severe restrictions on the programming model in order to protect the directory service against erroneous code 
the directory service imposes severe restrictions on the programming model in order to protect the directory service againstdenialofservice attacks 
the directory service imposes severe restrictions on the programming model in order to protect the directory service against malicious code 
the ranking functions are written in java
applicationspecific ordering heuristics
with the aid of userdefined ranking functions applicationspecific can be deployed directly
experiments on randomly generated problems show that randomly generated problems significantly reduce the number of query results that have to be transmitted to the client by up to 5 timeswe complement the rdf semantics specification of the w3c by proving decidability of rdfs entailment
rdfs extended with datatypes
rdfs extended with a propertyrelated subset of owl
furthermore we show decidability of entailment for rdfs
furthermore we show completeness of entailment for rdfsthe phenomenon has helped realize an initial goal of the semantic web
the phenomenon known as web logging 
the phenomenon known as  blogging  
as the semantic web unfolds we feel there are two questions worth posing exploited
as the semantic web unfolds we feel there are two questions worth posing do blog entries have semantic structure
semantic structure that can be usefully captured
is blogging a natural way to encourage growth of the semantic web
we explore empirical evidence for answering two questions worth posing do blog entries have semantic structure and propose means to bring blogging into the mainstream of the semantic web including ontologies 09 x2 0 files
ontologies that extend an xsl transform for handling rss
ontologies that extend the rss 10 specification transform for handling rss
semantic structure that can be usefully captured in the affirmative
semantic structure that can be usefully exploited in the affirmative
to demonstrate the validity of our approach our have constructed a semantic blogging environment based on haystack
our argue that with tools such as haystack semantic blogging will be an important paradigm
an important paradigm by which metadata authoring will occur in the futuresemantic web resources that is knowledge representation formalisms existing in a distributed hypermedia system require different addressing models than the typical kinds of world wide web resources
semantic web resources that is knowledge representation formalisms existing in a distributed hypermedia system require different processing models than the typical kinds of world wide web resources
semantic web resources that is knowledge representation formalisms existing in a distributed hypermedia system require different addressing capacities than the typical kinds of world wide web resources
semantic web resources that is knowledge representation formalisms existing in a distributed hypermedia system require different processing capacities than the typical kinds of world wide web resources
we describe an approach to building a semantic web resource protocol a scalable extensible logical addressing scheme protocol by using existing specifications and technologies
we describe an approach to building a semantic web resource protocol a scalable extensible logical addressing transport protocol by using existing specifications and technologies
we describe an approach to building a semantic web resource protocol a scalable extensible logical addressing transport protocol by extending existing specifications and technologies
we describe an approach to building a semantic web resource protocol a scalable extensible logical addressing scheme protocol by extending existing specifications and technologies
we introduce some infrequently used features of http1 1 in order to support addressing
we introduce server side processing of resource operations
we introduce some infrequently useful features of http1 1 in order to support addressing
we introduce xpointer
we introduce server side processing of subresource operations
we consider applications of the xpointer framework for use in the semantic web particularly for owl resources
we consider applications of the xpointer framework for use in the semantic web particularly for subresources
we consider applications of the xpointer framework for use in the semantic web particularly for rdf resources
we describe rdf subresource selection
we describe two initial implementations filtering of rss resources by date range
rdf subresource selection using rdql
we describe two initial implementations filtering of rss resources by item range
finally we describe possible application to the problem of owl importsthe available ontologies are seen as the solution to data heterogeneity on the web
however the available ontologies are the available ontologies source of heterogeneity
this can be overcome by finding the correspondence between the available ontologies components
this can be overcome by aligning ontologies
these alignments deserve to be treated as objects these alignments can be referenced on the web as such
these alignments deserve to be treated as objects these alignments can be compared with other alignments
these alignments deserve to be treated as objects these alignments can be transformed into a set of axioms
these alignments deserve to be treated as objects these alignments can be transformed into a set of a translation program
these alignments deserve to be treated as objects these alignments can be completed by an algorithm
an algorithm that improves a particular alignment
we present here a format for expressing alignments in rdf so that these alignments can be published on the web
an alignment api which shares some design goals with the owl api
then we propose an implementation of this format as an alignment api
an alignment api which can be seen as an extension of the owl api
we show how the owl api can be used for effectively aligning ontologies
we show how the owl api can be used for completing partial alignments generating axioms and transformations
we show how the owl api can be used for completing partial alignments thresholding alignmentsimages represent a key source of the ability to exploit many domains through many domains integration by agents on the semantic web is a challenging problem
images represent a key source of the ability to exploit many domains through many domains integration by services on the semantic web is a significant problem
images represent a key source of information in many domains is a challenging problem
images represent a key source of the ability to exploit many domains through many domains discovery by services on the semantic web is a challenging problem
images represent a key source of the ability to exploit many domains through many domains analysis by agents on the semantic web is a significant problem
images represent a key source of the ability to exploit many domains through many domains integration by services on the semantic web is a challenging problem
images represent a key source of the ability to exploit many domains through many domains analysis by services on the semantic web is a challenging problem
images represent a key source of the ability to exploit many domains through many domains discovery by services on the semantic web is a significant problem
images represent a key source of the ability to exploit many domains through many domains discovery by agents on the semantic web is a significant problem
images represent a key source of the ability to exploit many domains through many domains analysis by agents on the semantic web is a challenging problem
images represent a key source of the ability to exploit many domains through many domains integration by agents on the semantic web is a significant problem
images represent a key source of the ability to exploit many domains through many domains discovery by agents on the semantic web is a challenging problem
images represent a key source of the ability to exploit many domains through many domains analysis by services on the semantic web is a significant problem
images represent a key source of information in many domains is a significant problem
to date the semantic indexing of images has concentrated on applying machinelearning techniques to a set of manuallyannotated images in order to automatically label images with keywords
rulesbyexample which is based on a combination of ruleml
rulesbyexample which is based on a combination of querybyexample
in this paper we propose a new hybrid userassisted approach rulesbyexample
domainspecific rules that can infer highlevel semantic descriptions of images from combinations of lowlevel visual features
we rulesbyexample user interface enables domainexperts to graphically define domainspecific rules color texture shape size of regions 
domainspecific rules color texture shape size of regions  which have been specified through examples
domainspecific rules lrbcolor texture shape size of regions which have been specified through examples
semantically meaningful labels using terms
using domainspecific rules lrbcolor texture shape size of regions the system is able to analyze the visual features of any given image from this domain
domainspecific rules that can infer highlevel semantic descriptions of images from combinations of lowlevel visual features
terms defined in the domainspecific ontology
using domainspecific rules lrbcolor texture shape size of regions the system is able to generate semantically meaningful labels
we believe that this approach in combination with traditional solutions will maximize more flexible costeffective semantic indexing of images potential for processing by the semantic web services tools and agents
we believe that this approach in combination with traditional solutions will maximize more flexible costeffective semantic indexing of images potential for reuse by the semantic web services tools and agents
we believe that this approach in combination with traditional solutions will maximize more flexible accurate semantic indexing of images potential for processing by the semantic web services tools and agents
we believe that this approach in combination with traditional solutions will enable faster more flexible costeffective and accurate semantic indexing of images
we believe that this approach in combination with traditional solutions will maximize more flexible costeffective semantic indexing of images potential for integration by the semantic web services tools and agents
we believe that this approach in combination with traditional solutions will maximize more flexible costeffective semantic indexing of images potential for discovery by the semantic web services tools and agents
we believe that this approach in combination with traditional solutions will maximize more flexible accurate semantic indexing of images potential for discovery by the semantic web services tools and agents
we believe that this approach in combination with traditional solutions will maximize more flexible accurate semantic indexing of images potential for reuse by the semantic web services tools and agents
we believe that this approach in combination with traditional solutions will maximize more flexible accurate semantic indexing of images potential for integration by the semantic web services tools and agentsthe development of intelligent agents is a key part of the semantic web vision
how does an ordinary person tell an agent
an agent what to do
rdf templates that are authored once
one approach to this problem is to use rdf templates
rdf templates that then instantiated many times by ordinary users
one approach to this problem however raises a number of challenges
for instance how can templates concisely represent a broad range of potential uses yet ensure that each possible instantiation will function properly
the humans involved
and how does an agent explain an agent
an agent what to do actions to the humans
an agent what to do
this paper addresses these challenges in the context of a case study carried out on our fullydeployed system for semantic email agents
our describe how highlevel features of our template language enable the concise specification of flexible goals
in response to the first question our show that it is possible to verify in polynomial time that a given template will always produce a valid instantiation
second our show can be computed in polynomial time
an agent what to identify cases where explanations
our show how to automatically generate explanations for an agent
an agent what to do actions
these results both improve the usefulness of semantic email
these results both suggest general issues and techniques
general issues and techniques that may be applicable in other semantic web systemswe focus on the induction and revision of terminologies from metadata
operators that traverse the search space
the search space expressed in a structural representation
following a machine learning approach this setting can be cast as a search problem to be solved employing operators aiming at correct concept definitions
the progressive refinement of such definitions in a terminology is driven by the available extensional knowledge
description logics which are endowed with wellfounded reasoning capabilities
a knowledgeintensive inductive approach to this task is presented that can deal with on the expressive semantic web representations
the expressive semantic web representations based on description logics
the core inferential mechanism can be used for either inducing new concept descriptions or refining existing ones
the core inferential mechanism based on multilevel counterfactuals
the soundness of the approach applicability are also proved
the soundness of the approach applicability are also discussed
the soundness of the approach are also proved
the soundness of the approach are also discussedsuccessful employment of semantic web services depends on the availability of high quality ontologies to describe the domains of semantic web services
as always building such ontologies is costly thus hampering web service deployment
as always building such ontologies is difficult thus hampering web service deployment
our hypothesis is that since the functionality is reflected by the underlying software the extracted ontologies could be built by analyzing the documentation of the underlying software
the functionality offered by a web service
our verify this hypothesis in the domain of rdf ontology storage tools
our implemented
our finetuned a semiautomatic method to extract domain ontologies from software documentation
the quality of the extracted ontologies was verified against a high quality handbuilt ontology of the same domain
despite the low linguistic quality of the corpus our method allows extracting a considerable amount of information for a domain ontologythe semantic web services vision requires that each service be annotated with semantic metadata
manually creating such metadata is errorprone
tools that automatically generate wsdl
manually creating such metadata is tedious
many software engineers accustomed to tools
many software engineers might not want to invest the additional effort
a tool that assists a user in creating semantic metadata for web services
we therefore propose assams a tool
assams is intended for service consumers
service consumers who want to integrate a number of services
service consumers who therefore must annotate a number of services according to some shared ontology
assams want to make assams compatible with an existing ontology
service producers who have deployed a web service
assams is also relevant for service producers
assams capabilities to automatically create semantic metadata are supported by two machine learning algorithms
first assams have developed an iterative relational classification algorithm for semantically classifying input messages
first assams have developed an iterative relational classification algorithm for semantically classifying output messages
first assams have developed an iterative relational classification algorithm for semantically classifying web services
first assams have developed an iterative relational classification algorithm for semantically classifying assams operations
second to aggregate the data have developed a schema mapping algorithm
a schema mapping algorithm that is based on an ensemble of string distance metrics
the data returned by multiple semantically related web services assamsthe purpose of this paper is to provide a rigorous comparison of six query languages for rdf
we categorize features that any rdf query language should provide the individual languages along these features
we outline features that any rdf query language should compare the individual languages along these features
we categorize features that any rdf query language should compare the individual languages along these features
we outline features that any rdf query language should provide the individual languages along these features
we conclude with a comparison of the expressiveness of the particular query languages
we describe several practical usage examples for rdf queries
the use cases sample data and queries for the respective languages are available on the webhierarchical tasknetwork based planning techniques
web services especially when described using the owl
web services especially when described using s service ontologies
hierarchical tasknetwork have been applied to the problem of composing web services
many of the existing web services depend on informationproviding services
many of the existing web services are either exclusively information providing
thus many interesting service compositions involve collecting information either during the composition process the composition process itself
thus many interesting service compositions involve collecting information either during execution the composition process itself
in this paper we focus on the latter issue
planning domains in which the information about the initial state of the world may not be complete
in particular we present enquirer
an htnplanning algorithm designed for planning domains
the information about the initial state of the world is discoverable through plantime informationgathering queries
in particular we present an htnplanning algorithm
the plan found
the plan found
we have derived the quality of the plan
we have shown that enquirer is complete
the planner finding the plan
we have derived the likelihood of the planner
we have shown that enquirer is sound
we have derived several mathematical relationships among the amount of available information
we have performed experimental tests that demonstrated how enquirer can be used in web service composition
we have performed experimental tests that confirmed we theoretical resultswe have seen an increasing amount of interest in the application of semantic web technologies to web services
the services allowing seamless interoperability
the aim is to support automated discovery and composition of the services
the services allowing transparent interoperability
in this paper we discuss three projects mygrid
in this paper we discuss three projects semanticmoby
in this paper we discuss three projects mobyservices
three projects that are applying such technologies to bioinformatics
through an examination of the differences and similarities between the solutions produced we suggest that the experiences with three projects semanticmoby have implications for the development of semantic web services as a whole
through an examination of the differences and similarities between the solutions produced we highlight some of the practical difficulties in developing semantic web services
through an examination of the differences and similarities between the solutions produced we suggest that the experiences with three projects mygrid have implications for the development of semantic web services as a whole
three projects that are applying such technologies to bioinformatics
through an examination of the differences and similarities between the solutions produced we suggest that the experiences with three projects mobyservices have implications for the development of semantic web services as a wholeone of the original motivations behind ontology research was the belief
the belief that ontologies can help with reuse in knowledge representation
however many of the ontologies are extremely large while the users often need to reuse only a small part of these resources in many of the ontologies such as standard reference ontologies
the ontologies that are developed with reuse in mind such as standard reference ontologies
the ontologies that are developed with reuse in mind
the ontologies that are developed with reuse in mind such as controlled terminologies
however many of the ontologies are extremely large while the users often need to reuse only a small part of these resources in many of the ontologies such as controlled terminologies work
specifying various views of an ontology enables users to limit the set of concepts that users see
a view where a user specifies concepts of interest
in this paper we develop the concept of a traversal view a view
in this paper we develop the concept of a traversal view the relationships to traverse to find other concepts to include in the view
a view where a user specifies the central concept
in this paper we develop the concept of a traversal view the depth of the traversal
organs that surround heart
organ parts that are contained in heart
for example given a large ontology of anatomy a user may use a traversal view to extract a concept of heart
for example given a large ontology of anatomy a user may use a traversal view to extract organs
organs that are contained in heart
for example given a large ontology of anatomy a user may use a traversal view to extract organ parts
organ parts that surround heart
we define the notion of traversal views formally present a strategy for maintaining a traversal view through ontology evolution tool for defining traversal views
we define the notion of traversal views formally discuss traversal views formally properties tool for defining traversal views
we define the notion of traversal views formally describe we tool for defining traversal views
we define the notion of traversal views formally describe we tool for extracting traversal views
we define the notion of traversal views formally discuss traversal views formally properties tool for extracting traversal views
we define the notion of traversal views formally present a strategy for maintaining a traversal view through ontology evolution tool for extracting traversal views
