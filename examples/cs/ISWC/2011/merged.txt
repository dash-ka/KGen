following the design rules of linked data the number of available sparql endpoints is quickly growing however because of the lack of adaptivity query executions may frequently be unsuccessful
available sparql endpoints that support remote query processing
fixed plans identified following the traditional optimizethenexecute paradigm
first fixed plans may timeout as a consequence of endpoint availability
second because blocking operators are usually implemented endpoint query engines are not able to incrementally produce results
second because blocking operators are usually implemented endpoint query engines may become blocked if data sources stop sending data
sparql endpoints that adapts query execution schedulers to data availability
sparql endpoints that adapts query execution schedulers to runtime conditions
we present an adaptive query engine for sparql endpoints
we present anapsid
anapsid provides physical sparql operators
opportunistically the operators produce results as quickly as data arrives from the sources
physical sparql operators that detect when data traffic is bursty
physical sparql operators that detect when a source becomes blocked
additionally anapsid operators implement main memory replacement policies to move previously computed matches to secondary memory avoiding duplicates
we compared anapsid performance with respect to rdf stores and endpoints and observed that anapsid speeds up execution time in some cases in more than one order of magnitudein the last few years twitter has become a powerful tool for publishing
in the last few years twitter has discussing information
yet content exploration in twitter requires substantial effort
users often have to scan information streams by hand
in this paper twitter approach this problem by means of faceted search
twitter propose strategies for inferring facet values on twitter by enriching the semantics of individual twitter messages including contextadaptive methods for making faceted search on twitter more effective
twitter propose strategies for inferring facet values on twitter by enriching the semantics of individual twitter messages including personalized methods for making faceted search on twitter more effective
twitter propose strategies for inferring facets on twitter by enriching the semantics of individual twitter messages including contextadaptive methods for making faceted search on twitter more effective
twitter propose strategies for inferring facets on twitter by enriching the semantics of present different methods including personalized methods for making faceted search on twitter more effective
twitter propose strategies for inferring facet values on twitter by enriching the semantics of present different methods including contextadaptive methods for making faceted search on twitter more effective
twitter propose strategies for inferring facets on twitter by enriching the semantics of present different methods including contextadaptive methods for making faceted search on twitter more effective
twitter propose strategies for inferring facets on twitter by enriching the semantics of individual twitter messages including personalized methods for making faceted search on twitter more effective
twitter propose strategies for inferring facet values on twitter by enriching the semantics of present different methods including personalized methods for making faceted search on twitter more effective
twitter conduct a largescale evaluation of faceted search strategies show significant improvements over keyword search
twitter reveal significant benefits of faceted search strategies that support users in selecting facet value pairs by adapting the faceted search interface to the specific needs and preferences of a user
links posted in individual twitter messages
twitter reveal significant benefits of faceted search strategies that further enrich the semantics of individual twitter messages by exploiting linksthe mapreduce framework has proved to be very efficient for dataintensive tasks
earlier work has shown promising results
earlier work has tried to use mapreduce for large scale reasoning for pd semantics
in this paper we move a step forward to consider scalable reasoning on top of semantic data under an extension of owl pd semantics with fuzzy vagueness 
in this paper we move a step forward to consider scalable reasoning on top of semantic data under fuzzy pd semantics 
to the best of our knowledge this is the first work to investigate how mapreduce for large scale reasoning for pd semantics can help to solve the scalability issue of fuzzy owl reasoning
the optimizations used by the existing mapreduce framework for pd semantics
while most of the optimizations are also applicable for fuzzy pd semantics unique challenges arise when we handle the fuzzy information
we propose a solution for tackling each of these key challenges
we identify these key challenges
furthermore we implement a prototype system for the evaluation purpose
the experimental results show that the running time of a prototype system for the evaluation purpose is comparable with that of webp the stateoftheart inference engine for scalable reasoning in pd semanticssemantic web data with annotations is becoming available being yago knowledge base a prominent example
annotated semantic web data using standard database technology
in this paper we present an approach to perform the closure of large rdf schema annotated semantic web data
real fuzzy semantic data extracted from yago in the postgresql database management system
in particular we exploit several alternatives to address the problem of computing transitive closure with real fuzzy semantic data
we benchmark the several alternatives in persistent storage
we benchmark compare to classical rdf schema reasoning providing the first implementation of annotated rdf schema in persistent storagewe describe an optimised consequencebased procedure for classification of ontologies
ontologies expressed in a polynomial fragment elh  r of the owl 2 el profile
a distinguishing property of we procedure is that our procedure can take advantage of multiple processorscores
multiple processorscores which increasingly prevail in computer systems
we solution is based on a variant of the  given clause  saturation algorithm for firstorder theorem proving where we assign derived axioms to  contexts within which contexts within can be used
we solution is based on a variant of the  given clause  saturation algorithm for firstorder theorem proving where we assign derived axioms to  contexts within which can be processed independently
we describe an implementation of we within the javabased reasoner elk
we implementation is lightweight in the sense that an overhead of managing concurrent computations is minimal
this is achieved by employing operations such as  compareandswap 
this is achieved by employing lockfree data structures such as  compareandswap 
we report on preliminary experimental results
preliminary experimental results demonstrating a substantial speedup of ontology classification on multicore systems
in particular one of the largest snomed ct can be classified in as little as 5 seconds
in particular one of widelyused medical ontologies snomed ct can be classified in as little as 5 secondsannotation graph datasets are a natural representation of scientific knowledge
the life sciences where genes are annotated with controlled vocabulary terms  from ontologies
the life sciences where proteins are annotated with controlled vocabulary terms  from ontologies
annotation graph datasets are common in the life sciences
the life sciences where proteins are cv terms  from ontologies
the life sciences where genes are cv terms  from ontologies
the w3c linking open data  semantic web technologies are playing a leading role in making such datasets widely available
lod  semantic web technologies are playing a leading role in making such datasets widely available
lod  initiative web technologies are playing a leading role in making such datasets widely available
the w3c linking open data  initiative web technologies are playing a leading role in making such datasets widely available
scientists can mine such datasets to discover patterns of annotation
while ontology alignment and integration across datasets has been explored in the context of the semantic web there is no current approach to mine such patterns in annotation graph datasets
in this paper we propose a novel approach for link prediction this paper is a preliminary task when discovering more complex patterns
we prediction is based on a complementary methodology of dense subgraphs
we prediction is based on a complementary methodology of graph summarization
graph summarization can summarize knowledge
knowledge captured in the annotation patterns
knowledge captured within the ontologies
graph summarization can exploit knowledge
dense subgraphs uses the ontology structure in particular the distance between cv terms to to find promising subgraphs
dense subgraphs uses the ontology structure in particular the distance between cv terms to filter the graph
we develop a scoring function based on multiple heuristics to rank the predictions
we perform an extensive evaluation on arabidopsis thaliana genesexpertdriven business process management is an established means for improving efficiency of organizational knowledge work
implicit procedural knowledge in the organization is made explicit by defining processes
this approach is not applicable to individual knowledge work due to this approach high complexity
this approach is not applicable to individual knowledge work due to variability
however without explicitly described processes there is no analysis communication of best practices of individual knowledge work within the organization
however without explicitly described processes there is no efficient communication of best practices of individual knowledge work within the organization
in addition the activities of the individual knowledge work can not be synchronized with the activities in the organizational knowledge worka highly scalable ontology matching system with  diagnosis capabilities
in this paper we present logmap a highly scalable ontology
a highly scalable ontology matching system with  builtin  reasoning
semantically rich ontologies containing and even hundreds  of thousands of classes
to the best of our knowledge logmaps is the only matching system
semantically rich ontologies containing tens  of thousands of classes
the only matching system that can deal with semantically rich ontologies
in contrast to most existing tools logmaps also implements algorithms for  repair
in contrast to most existing tools logmaps also implements algorithms for  on the fly  unsatisfiability detection
our experiments with the ontologies nci ct confirm that our system can efficiently match even the largest existing biomedical ontologies
our experiments with the ontologies snomed ct confirm that our system can efficiently match even the largest existing biomedical ontologies
our experiments with the ontologies fma ct confirm that our system can efficiently match even the largest existing biomedical ontologies
furthermore logmaps is able to produce a  clean  set of output mappings in many cases in the sense that the ontology does not contain unsatisfiable classes
furthermore logmaps is able to produce a  clean  set of output mappings in many cases in the sense that the ontology is consistent
the ontology obtained by integrating logmaps output mappings with the input ontologiesontology alignment is an important problem for the linked data web as more ontologies get published for specific domains such as government
ontology alignment is an important problem for the linked data web as more ontologies get published for specific domains such as healthcare
ontology alignment is an important problem for the linked data web as more ontologies get published for specific domains such as government
ontology alignment is an important problem for the linked data web as more ontologies get published for specific domains such as healthcare
ontology alignment is an important problem for the linked data web as ontology instances get published for specific domains such as government
ontology alignment is an important problem for the linked data web as ontology instances get published for specific domains such as healthcare
a number of  semi  automated alignment systems have been proposed in recent years
most combine a set of similarity functions on lexical features to align ontologies
most combine a set of similarity functions on structural features to align ontologies
most combine a set of similarity functions on semantic features to align ontologies
although similarity functions on structural features work well in many cases of ontology alignments similarity functions on semantic features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on structural features work well in many cases of ontology alignments similarity functions on lexical features fail to capture alignments when structure varies vastly across ontologies
although similarity functions on lexical features work well in many cases of ontology alignments similarity functions on semantic features fail to capture alignments when structure varies vastly across ontologies
although similarity functions on lexical features work well in many cases of ontology alignments similarity functions on semantic features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on structural features work well in many cases of ontology alignments similarity functions on lexical features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on semantic features work well in many cases of ontology alignments similarity functions on structural features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on structural features work well in many cases of ontology alignments similarity functions on structural features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on lexical features work well in many cases of ontology alignments similarity functions on lexical features fail to capture alignments when structure varies vastly across ontologies
although similarity functions on structural features work well in many cases of ontology alignments similarity functions on structural features fail to capture alignments when structure varies vastly across ontologies
although similarity functions on lexical features work well in many cases of ontology alignments similarity functions on lexical features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on lexical features work well in many cases of ontology alignments similarity functions on structural features fail to capture alignments when structure varies vastly across ontologies
although similarity functions on semantic features work well in many cases of ontology alignments similarity functions on semantic features fail to capture alignments when structure varies vastly across ontologies
although similarity functions on lexical features work well in many cases of ontology alignments similarity functions on structural features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on semantic features work well in many cases of ontology alignments similarity functions on structural features fail to capture alignments when structure varies vastly across ontologies
although similarity functions on semantic features work well in many cases of ontology alignments similarity functions on semantic features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on semantic features work well in many cases of ontology alignments similarity functions on lexical features fail to capture alignments when structure varies vastly across ontologies
although similarity functions on semantic features work well in many cases of ontology alignments similarity functions on lexical features fail to capture alignments when terms varies vastly across ontologies
although similarity functions on structural features work well in many cases of ontology alignments similarity functions on semantic features fail to capture alignments when structure varies vastly across ontologies
in this case one is forced to rely on manual alignment
expert provided ontology alignments for new alignment tasks
in this paper we study whether it is feasible to reuse such expert
we focus in particular on manytoone alignments where the opportunity for reuse is feasible if alignments are stable
specifically we define the notion of a cluster as being made of multiple entities in the source ontology s
the source ontology s that are mapped to the same entity in the target ontology t
we test the stability hypothesis that the formed clusters of source ontology are stable across alignments to different target ontologies
if the stability hypothesis is valid the clusters of an ontology s built from an existing alignment with an ontology t can be effectively exploited to align s with a new ontology t 
evaluation on both manual show remarkable stability of clusters across ontology alignments in the financial domain
evaluation on both manual show remarkable stability of clusters across ontology alignments in life sciences domain
evaluation on automated highquality alignments show remarkable stability of clusters across ontology alignments in life sciences domain
evaluation on automated highquality alignments show remarkable stability of clusters across ontology alignments in the financial domain
evaluation on both manual show remarkable stability of clusters across ontology alignments in the healthcare
evaluation on automated highquality alignments show remarkable stability of clusters across ontology alignments in the healthcare
experimental evaluation also demonstrates the effectiveness of utilizing the stability of clusters in improving the alignment process in terms of recall
experimental evaluation also demonstrates the effectiveness of utilizing the stability of clusters in improving the alignment process in terms of precisionthe evolution proved the evolution importance
modelbased approaches where an evolution  of a kb  results in a set of models
recent studies of the topic mostly focussed on modelbased approaches
for kbs such as dl
for kbs such as lite
it was shown that the evolution suffers from inexpressibility
kbs expressed in tractable dls
the result of evolution can not be expressed in lite
the result of evolution can not be expressed in dl
what is missing in these studies is understanding in which lite fragments evolution can be captured what causes the inexpressibility is sufficient to express evolution whether one can approximate it in lite
what is missing in these studies is understanding in which dl can be captured what causes the inexpressibility is sufficient to express evolution whether one can approximate it in lite
what is missing in these studies is understanding in which lite fragments evolution can be captured what causes the inexpressibility is sufficient to express evolution whether one can approximate it in dl 
what is missing in these studies is understanding in which dl can be captured what causes the inexpressibility is sufficient to express evolution how one can approximate it in lite
what is missing in these studies is understanding in which lite fragments evolution can be captured what causes the inexpressibility is sufficient to express evolution how one can approximate it in lite
the inexpressibility which logics
what is missing in these studies is understanding in which dl can be captured what causes the inexpressibility is sufficient to express evolution how one can approximate it in dl 
what is missing in these studies is understanding in which lite fragments evolution can be captured what causes the inexpressibility is sufficient to express evolution how one can approximate it in dl 
what is missing in these studies is understanding in which dl can be captured what causes the inexpressibility is sufficient to express evolution whether one can approximate it in dl 
this work provides some understanding of these issues for eight of revision
this work provides some understanding of these issues for eight of modelbased approaches
modelbased approaches which cover the case of both update
we found what causes inexpressibility lite
we found what isolated a fragment of dl lite
lite where evolution is expressible
for what isolated a fragment of dl lite we provided polynomialtime algorithms to compute evolution results
for what causes inexpressibility lite we provided polynomialtime algorithms to compute evolution results
lite where evolution is expressible
lite evolution corresponding to a wellknown winsletts approach in a dl shoiq  which is subsumed by owl 2 dl 
for the general case we proposed techniques  based on what we called prototypes  to capture dl lite evolution
we also showed how to approximate this evolution in dl liteanswering queries is an important reasoning task in description logics hence also in highly expressive ontology languages such as owl
an even more expressive language that allows for exceptions
extending such ontology languages with rules such as those expressible in rifcore and further with nonmonotonic rules integrating default negation as described in the riffld yields an even more expressive language
an even more expressive language that allows for modeling defaults
an even more expressive language that allows for integrity constraintsdata quality issues arise in the semantic web because data is created by diverse people andor automated tools
in particular erroneous triples may occur errors in ontology alignment
in particular erroneous triples may occur misuse of ontologies
due to factual errors in the original data source the acquisition tools employed
in particular erroneous triples may occur due to factual errors in the original data source the acquisition tools
we propose that the degree to which a triple deviates from similar triples can be an important heuristic for identifying errors
functional dependency which has shown promise in rdf
inspired by functional dependency we introduce valueclustered graph functional dependency to detect abnormal data in rdf graphs
to better deal with semantic web data this extends the concept of functional dependency on several aspects
first there is the issue of scale since we must consider the whole data schema instead of being restricted to one database relation
second this deals with multivalued properties without explicit value correlations as specified as tuples in databases
third this uses clustering to consider classes of values
focusing on these characteristics we propose a number of heuristics to efficiently discover the extended dependencies
focusing on these characteristics we propose a number of algorithms to efficiently discover the extended dependencies
focusing on these characteristics we propose a number of heuristics to use the extended dependencies to detect abnormal data
focusing on these characteristics we propose a number of algorithms to use the extended dependencies to detect abnormal data
experiments have shown that the system is efficient on multiple data sets
experiments have shown that the system also detects many quality problems in real world datawe present the first large scale investigation into the modular structure of a substantial collection of stateoftheart biomedical ontologies namely those maintained in the ncbo bioportal repository
using the notion of atomic decomposition we partition bioportal ontologies into logically coherent subsets
logically coherent subsets which are related to each other by a notion of dependency
we discuss the resulting structures implications on applications of ontologies
we analyze various aspects of the resulting structures
in particular we investigate the usage of these ontology decompositions to extract modules for instance to facilitate matchmaking of semantic web services in sswap 
in particular we describe the usage of these ontology decompositions to extract modules for instance to facilitate matchmaking of semantic web services in simple semantic web protocol 
in particular we investigate the usage of these ontology decompositions to extract modules for instance to facilitate matchmaking of semantic web services in simple semantic web architecture 
in particular we describe the usage of these ontology decompositions to extract modules for instance to facilitate matchmaking of semantic web services in simple semantic web architecture 
in particular we describe the usage of these ontology decompositions to extract modules for instance to facilitate matchmaking of semantic web services in sswap 
in particular we investigate the usage of these ontology decompositions to extract modules for instance to facilitate matchmaking of semantic web services in simple semantic web protocol 
descriptions of those services use terms from bioportal so service discovery requires reasoning with respect to relevant fragments of ontologies 
descriptions of those services use terms from bioportal so service discovery requires reasoning with respect to relevant fragments of modules 
we present a novel algorithm for extracting modules from decomposed bioportal ontologies to ensure logically complete reasoning
bioportal ontologies which is able to quickly identify logically coherent subsets
coherent subsets that need to be included in a module
compared to existing module extraction algorithms bioportal ontologies has improved performance
coherent subsets that need to be included in a module
compared to existing module extraction algorithms bioportal ontologies has the possibility to avoid loading the entire ontology into memory
bioportal ontologies which is able to quickly identify logically coherent subsets
compared to existing module extraction algorithms bioportal ontologies has a number
the results are presented
the results are discussed
a novel algorithm is also evaluated on bioportal ontologiesobservational studies in the literature have highlighted low levels of user satisfaction in relation to the support for ontology visualization and exploration provided by current ontology engineering tools
nonexpert users who rely on effective tool support to be able to make sense of the contents
nonexpert users who rely on effective tool support to be able to make the structure of ontologies
these issues are particularly problematic for nonexpert users
nonexpert users who rely on effective tool support to abstract from representational details
to address these issues we have developed a novel solution for navigating ontologies kcviz
ontology navigation approach starting from the most informationrich nodes
kcviz which exploits an empiricallyvalidated ontology summarization method both to provide concise views of large ontologies and also to support a  middleout  ontology navigation approach  key concepts 
to address these issues we have developed a novel solution for visualizing ontologies kcviz
in this paper we also discuss the encouraging results
a preliminary empirical evaluation which suggest that the use of kcviz provides performance advantages to users
the encouraging results derived from a preliminary empirical evaluation
users tackling realistic browsing
in this paper we present the main features of kcviz
users tackling visualization tasks
supplementary data gathered through evidence that prior experience in ontology engineering affects not just objective performance in ontology engineering tasks but also subjective views on the usability of ontology engineering tools
supplementary data gathered through questionnaires that prior experience in ontology engineering affects not just objective performance in ontology engineering tasks but also subjective views on the usability of ontology engineering toolslinked data is developing towards a large global repository for structured interlinked descriptions of realworld entities
an emerging problem in many web applications making use of data like linked data is how a lengthy description can be tailored to the task of quickly identifying the underlying entity
as a solution to this novel problem of entity summarization we propose a variant of the random surfer model
the random surfer model that leverages the relatedness and informativeness of description elements for ranking
as a solution to this novel problem of entity summarization we propose relin
we present an implementation of this conceptual model
description elements based on information theory concepts
description elements based on linguistic theory concepts
this conceptual model which captures the semantics of description elements
in experiments involving users our approach outperforms the baselines shown to be useful in a concrete task
in experiments involving realworld data sets our approach outperforms the baselines shown to be useful in a concrete task
the baselines producing summaries that better match handcrafted ones and furtherwhat is the most intuitive way of organizing concepts for describing things
things that people use for describing other things
what are the most relevant types of things
wikipedia data offer knowledge engineering researchers a chance to empirically identifying invariances in conceptual organization of knowledge knowledge patterns
linked data offer knowledge engineering researchers a chance to empirically identifying invariances in conceptual organization of knowledge knowledge patterns
encyclopedic knowledge patterns that have been discovered by analyizing the wikipedia page links discuss why a user study enables a number of research directions
research directions contributing to the realization of a meaningful semantic web
encyclopedic knowledge patterns that have been discovered by analyizing the wikipedia page links dataset describe wikipedia data evaluation with a user study
in this paper we present a resource of encyclopedic knowledge patterns
encyclopedic knowledge patterns that have been discovered by analyizing the wikipedia page links dataset describe linked data evaluation with a user studyin this paper we study watermarking methods to prove the ownership of an ontology
different from existing approaches we propose to watermark not by altering existing statements but by removing existing statements
thereby we approach does not introduce false statements into an ontology
we show how ownership of ontologies can be established with provably tight probability bounds even if only parts of an ontology are being reused
we finally demonstrate the viability of we approach on realworld ontologiesone challenge for linked data is scalably establishing highquality owl sameas links between instances  in different data sources
one challenge for linked data is scalably establishing highquality owl sameas links between people geographical locations publications  in different data sources
traditional approaches to this entity coreference problem do not scale because traditional approaches to this entity coreference problem exhaustively compare every pair of instances
in this paper we propose a candidate selection algorithm for pruning the search space for entity coreference
literal values that are chosen using domainindependent unsupervised learning
we select candidate instance pairs by computing a characterlevel similarity on discriminating literal values
we index the instances on the chosen predicates literal values to efficiently look up similar instances
three structured datasets
we evaluate we approach on three
we evaluate we approach on two rdf
we show that the traditional metrics propose additional metrics
we show that the traditional metrics do not always accurately reflect the relative benefits of candidate selection
we show that we algorithm frequently outperforms alternatives and is able to process 1 million instances in under one hour on a single sun workstation
furthermore on the rdf datasets we show that the entire entity coreference process scales well by applying we technique
surprisingly low precision filtering mechanism frequently leads to higher fscores in the overall system
surprisingly this high recall leads to higher fscores in the overall systeman explicit interpretation phase that maps keywords in a keyword query to structured query constructs
effective techniques for keyword search over rdf databases incorporate an explicit interpretation phase
because of the ambiguity of keyword queries it is often not possible to generate a unique interpretation for a keyword query
consequently heuristics geared toward generating the topk likeliest userintended interpretations have been proposed
however heuristics currently proposed fail to depend on databasedependent properties such as occurrence frequency of subgraph pattern
however heuristics currently proposed fail to capture any userdependent characteristics
databasedependent properties such as occurrence frequency of subgraph pattern connecting keywords
topk interpretations that are not aligned with user intentions
this leads to the problem of generating topk interpretations
keyword query interpretation that personalizes the interpretation process based on a users query context
in this paper we propose a contextaware approach for keyword query interpretation
our approach addresses the novel problem of using a sequence of structured queries in the query history as contextual information for biasing the interpretation of a new query
structured queries corresponding to interpretations of keyword queries
experimental results presented over dbpedia dataset
experimental results show that our approach outperforms the stateoftheart technique on both efficiency and effectiveness particularly for ambiguous querieswe aim at providing a complementary layer for the web semantics being merely asserted by the semantic web
bottomup phenomena that are empirically observable on the semantic web
we aim at providing a complementary layer for the web semantics catering for bottomup phenomena
we focus on meaning that is not associated with particular semantic descriptions
we emerges from the multitude of implicit links on the web of data
we emerges from the multitude of explicit links on the web of data
we claim that the current approaches are mostly topdown
we claim that the current approaches thus lack a proper mechanisms for capturing the emergent aspects of the web meaning
to fill this gap we have proposed a framework based on a successful bottomup approach to meaning representation in computational linguistics 
distributional semantics  that is however still compatible with the topdown semantic web principles due to inherent support of rules
to fill this gap we have proposed a framework based on distributional semantics 
a successful bottomup approach to meaning representation in computational linguistics  that is however still compatible with the topdown semantic web principles due to inherent support of rules
we evaluated we solution in a knowledge consolidation experiment
a knowledge consolidation experiment which confirmed the promising potential of we approachboth materialization and backwardchaining as different modes of performing inference have complementary advantages and disadvantages2 reasoners which in some cases fail to derive all answers to a query
the need for scalable query answering often forces semantic web applications to use incomplete owl 2 reasoners
in some applications may even be unacceptable
this is clearly undesirable
to address this problem we investigate the problem of  repairing  an ontology tthat is computing an ontology r such that a reasoner becomes complete when used with t r
a reasoner that is incomplete for t
we identify conditions on t and present a preliminary evaluation reasonable in size and do not significantly affect reasoner performance
the reasoner that make this possible present a practical algorithm for computing r
we identify conditions on the reasoner and present a preliminary evaluation reasonable in size and do not significantly affect reasoner performance
a preliminary evaluation which shows that in some realistic cases repairs are feasible to computetypical tagging systems merely capture that part of the tagging interactions
the tagging interactions that enrich the semantics of tag assignments according to the systems purposes
the common practice is to build tagbased resource on the basis of statistics about tags disregarding the additional evidence that pertain to the resource
the common practice is to build tagbased resource on the basis of statistics about tags disregarding the additional evidence that pertain to the tag assignment the tag
the common practice is to build tagbased resource on the basis of statistics about tags disregarding the additional evidence that pertain to the user
the common practice is to build user profiles on the basis of statistics about tags disregarding the additional evidence that pertain to the resource
the common practice is to build user profiles on the basis of statistics about tags disregarding the additional evidence that pertain to the tag assignment the tag
the common practice is to build user profiles on the basis of statistics about tags disregarding the additional evidence that pertain to the user
thus the main bulk of this valuable information is ignored when generating user profiles
thus the main bulk of this valuable information is ignored when generating resource profileswe propose a novel application of clustering analysis to identify regularities in the usage of entities in axioms within an ontology
we argue that such regularities will be able to help to identify parts of the schemas and guidelines upon which ontologies are often built especially in the absence of explicit documentation
such analysis can also isolate irregular entities thus highlighting possible deviations from the initial design
generalised axioms that offer a synthetic representation of the detected regularity
the clusters we obtain can be fully described in terms of generalised axioms
we discuss the potential advantages of incorporating our analysis into future authoring tools
in this paper we discuss the results of the application of we analysis to different ontologiesin a semantic p2p network peers use separate ontologies
in a semantic p2p network peers rely on alignments between peers ontologies for translating queries
nonetheless alignments may be limited unsound 
translations leading to unsatisfactory answers
nonetheless alignments may be limited incomplete 
nonetheless alignments generate flawed translations
a trust mechanism that can assist peers to select those in the network
the network that are better suited to answer those in the network
in this paper we present a trust mechanism
the network that are better suited to answer their queries queries
the trust that a peer has towards another peer represents the probability that the latter peer will provide a satisfactory answer
the trust that a peer has towards another peer depends on a specific query
in order to compute trust we exploit both alignments
in order to compute trust we exploit peers direct experience
in order to compute trust we perform bayesian inference
we have implemented we technique
we conducted an evaluation
experimental results showed that trust values converge as more queries are sent
answers received
furthermore the use of trust improves both precision
furthermore the use of trust improves both recallthe entities need to have these labels in order to be exposable to humans in a meaningful way
these labels can then be used for displaying the entities in other frontend applications
these labels can then be used for exploring the data
these labels can then be used for displaying the entities in to support keywordbased based search over the web of data
these labels can then be used for displaying the entities in a linked data browser
these labels can then be used for displaying the entities in to support naturallanguage based search over the web of data
far too many applications fall back to exposing the uris of the entities to the user in the absence of more easily understandable representations such as humanreadable labels
in this work we introduce completeness of the labeling
in this work we introduce a number of labelrelated metrics
in this work we introduce the multilinguality of the labeling
in this work we introduce unambiguity of labeling
in this work we introduce the efficient accessibility of the labels
we report we findings from measuring the web of data
data using these metrics
we also investigate which properties are used for labeling purposes since many vocabularies define further labeling properties beyond the standard property from rdfsexistential variables in the same way that has been used before in mathematical logic
blank nodes are defined in rdf as  existential variables in the same way
however evidence suggests that actual usage of rdf does not follow this definition
in this paper we thoroughly cover the issue of blank nodes from incomplete information in database theory over different treatments of blank nodes across the w3c stack of rdfrelated standards to empirical analysis of rdf data publicly available on the web
we then summarize alternative approaches to the problem weighing up advantages also discussing proposals for skolemization
we then summarize alternative approaches to the problem weighing up disadvantages also discussing proposals for skolemizationcurrent ontology development tools offer debugging support by presenting justifications for entailments of owl ontologies
while these minimal subsets have been shown to support understanding tasks the occurrence of multiple justifications presents a significant cognitive challenge to users
while these minimal subsets have been shown to support debugging tasks the occurrence of multiple justifications presents a significant cognitive challenge to users
in many cases even a single entailment may have many distinct justifications
justifications for distinct entailments may be critically related
however it is currently unknown how prevalent significant numbers of multiple justifications per entailment are in the field
to address this lack we examine the justifications from an independently motivated corpus of actively used biomedical ontologies from the ncbo bioportal
structural features  such as patterns  which can be exploited in order
we find that the majority of ontologies contain multiple justifications while also exhibiting structural features  such as patterns  to reduce user effort in the ontology engineering processan online community owners who have vested interests in an online community longevity
understanding the health of an online community is of great value to an online community owners
an online community owners who have vested interests in success
an online managers who have vested interests in success
forecasting the health of an online community is of great value to an online managers
an online managers who have vested interests in an online community longevity
understanding the health of an online community is of great value to an online managers
forecasting the health of an online community is of great value to an online community owners
nevertheless the association between community evolution is not clearly understood which hinders our ability of making accurate predictions of whether a community is diminishing
nevertheless the association between community evolution is not clearly understood which hinders our ability of making accurate predictions of whether a community is flourishing
nevertheless the behavioural patterns and trends of an online community members is not clearly understood which hinders our ability of making accurate predictions of whether a community is diminishing
nevertheless the behavioural patterns and trends of an online community members is not clearly understood which hinders our ability of making accurate predictions of whether a community is flourishing
in this paper our use statistical analysis combined with rules for representing behaviour in online communities
in this paper our use statistical analysis combined with a semantic model for computing behaviour in online communities
in this paper our use statistical analysis combined with a semantic model for representing behaviour in online communities
in this paper our use statistical analysis combined with rules for computing behaviour in online communities
report on how different behaviour compositions correlate with positive community growth in these forums
our apply this model on a number of forum communities from boards to categorise behaviour of community members over time
report on how different behaviour compositions correlate with negative community growth in these forums
our apply this model on a number of forum communities from reportaccess using expressive formal query languages such as sparql
triple stores have long provided rdf storage access
triple stores have long provided data access
the new end users of the semantic web however are mostly unaware of sparql
the new end users of the semantic web however overwhelmingly prefer imprecise informal keyword queries for searching over data
at the same time the amount of data on the semantic web is approaching the limits of the architectures
the architectures that provide support for the full expressivity of sparql
rdf data using information retrieval methods
these factors have led to an increased interest in semantic search access to rdf data
these factors combined
in this work we propose a method for efficient entity search over rdf data
in this work we propose a method for effective entity search over rdf data
we describe an adaptation of the bm25f ranking function for rdf data
we demonstrate that rdf outperforms other stateoftheart methods in ranking rdf resources
we also propose a set of new index structures for efficient retrieval and ranking of these results
we implement these results
these results using the opensource mg4j frameworkquerying are two wellknown paradigms to search the semantic web
faceted search are two wellknown paradigms to search the semantic web
querying languages such as sparql are difficult to use
querying languages such as sparql offer expressive means for searching rdf datasets
query assistants help users to write wellformed queries
query assistants do not prevent empty results
navigation that returns rich feedbacks to users
faceted search supports exploratory search and guided navigation and prevents rich feedbacks to fall in deadends 
faceted search supports exploratory search and guided navigation and prevents rich feedbacks to fall in empty results 
however faceted search systems do not offer the same expressiveness as query languages
we introduce querybased faceted search  qfs  to reconcile the two paradigms
we introduce querybased faceted search the combination of faceted search to reconcile the two paradigms
we introduce querybased faceted search the combination of an expressive query language to reconcile the two paradigms
in this paper the lisql query language generalizes existing semantic faceted search systems
in this paper the lisql query language covers most features of sparql
a prototype sewelis  aka
camelis 2  a usability evaluation demonstrated that qfs enables users to build complex queries with little training
camelis 2  has been implemented
camelis 2  a usability evaluation demonstrated that qfs retains the easeofuse of faceted searchrecently much attention has been directed to extending logic programming with description logic  dl  expressions so that logic programs have access to dl knowledge bases
recently much attention has been directed to extending logic programming with description logic  dl  expressions so that logic programs thus are able to reason with ontologies in the semantic web
dl expressions called normal dl logic programs
in this paper we propose a new extension of logic programs with dl expressions
in a normal dl logic program arbitrary dl expressions are allowed to appear in rule bodies
atomic concepts  allowed in rule heads
in a normal dl logic program arbitrary dl expressions are allowed to appear in atomic concepts 
atomic atomic roles  allowed in rule heads
in a normal dl logic program arbitrary dl expressions are allowed to appear in atomic dl expressions 
in a normal dl logic program arbitrary dl expressions are allowed to appear in atomic atomic roles 
atomic dl expressions  allowed in rule heads
we extend the key condition of wellsupportedness for normal logic programs under the standard answer set semantics to normal dl logic programs
an answer set semantics for dl logic programs
dl logic programs which satisfies the extended wellsupportedness condition
we extend the key condition of wellsupportedness for normal logic programs under the standard answer define an answer
the answer set semantics for normal dl logic programs
we show that the answer is decidable if the underlying description logic is decidable  egmotivated by the growing amount of semantic data sources available on the web new challenges to query processing are emerging
motivated by the ongoing success of linked data new challenges to query processing are emerging
distributed settings that require joining data provided by multiple sources
especially in distributed settings sophisticated optimization techniques are necessary for efficient query processing
we propose novel join processing to minimize the number of remote requests
we propose novel join grouping techniques to minimize the number of remote requests
we propose novel develop an effective solution for source selection in the absence of preprocessed metadata
we present fedx a practical framework
a practical framework that enables efficient sparql query processing on heterogeneous virtually integrated linked data sources
in experiments we demonstrate the practicability and efficiency of we framework on a set of data sources from the linked open data cloud
in experiments we demonstrate the practicability and efficiency of we framework on a set of realworld queries from the linked open data cloud
with fedx we achieve a significant improvement in query performance over stateoftheart federated query enginesconcept diagrams were introduced for precisely specifying ontologies in a manner more readily accessible to other stakeholders than symbolic notations
concept diagrams were introduced for precisely specifying ontologies in a manner more readily accessible to developers than symbolic notations
in this paper we present a case study on the use of concept diagrams in visually specifying the semantic sensor networks ontology
visually specifying the semantic sensor networks ontology was originally developed by an incubator group of the w3c
visually specifying the semantic sensor networks ontology was originally developed by an incubator group of the w3c
a physical object that implements sensing
an observation is observed by a single sensor
in visually specifying the semantic sensor networks ontology
in visually specifying the semantic sensor networks ontology
a sensor is a physical object
concepts are captured visually by concept diagrams
concepts are captured precisely by concept diagrams
roles are captured precisely by concept diagrams
other are captured precisely by concept diagrams
roles are captured visually by concept diagrams
these
other are captured visually by concept diagrams
we consider the lessons show how to convert description logic axioms into concept diagrams
we consider the lessons learnt from developing this visual model
we also demonstrate how to merge simple concept diagram axioms into more complex axioms whilst ensuring that diagrams remain relatively unclutteredreferential qualities are qualities of an entity
an entity taken with reference to another entity
for example the vulnerability of a coast to sea level rise
an entity x taken with reference to another entity r
in contrast to most nonrelational qualities which only depend on most nonrelational qualities host referential qualities require a referent additional to their host a quality q of an entity x
referential qualities occur frequently in ecological systems
ecological systems which make concepts from these systems
these systems challenging to model in formal ontology
in this paper we discuss exemplary resilience vulnerability and affordance as qualities of an entity
an entity taken with reference to an external factor
we suggest an ontology design pattern for referential qualities
the design pattern is anchored in the foundational ontology dolce and evaluated using implementations for the notions vulnerability
the design pattern is anchored in the foundational ontology dolce and evaluated using implementations for the notions resilience
the design pattern is anchored in the foundational ontology dolce and evaluated using implementations for the notions affordanceontology verification is concerned with the relationship between the models of the axiomatization of an ontology
ontology verification is concerned with the relationship between the intended structures for an ontology
the verification of a particular ontology requires characterization of the models of an ontology up to isomorphism that the models of the ontology are equivalent to the intended structures for an ontology
the verification of a particular ontology requires characterization of the models of an ontology up to a proof that the models of the ontology are equivalent to the intended structures for an ontology
time introduced by hobbs
time introduced by pan
in this paper we provide the verification of the ontology of time which is a firstorder axiomatization of owltime
we present a complete account of the metatheoretic relationships between other time ontologies for points
we present a complete account of the metatheoretic relationships among five modules
we identify five modules within an ontology
we present a complete account of the metatheoretic relationships between other time ontologies for intervalsa task that has attracted considerable attention in recent years
ontology matching is a task
with very few exceptions however research in ontology matching has focused primarily on the development of monolingual matching algorithms
as more resources become available in more than one language novel algorithms are required which are capable of matching ontologies
as more resources become available in more than one language novel algorithms are required which are capable of matching ontologies
as more resources become available in more than one language novel algorithms are required which are capable of matching ontologies
ontologies which do not share any languages
ontologies which share more than one language
ontologies which are multilingual
as more resources become available in more than one language novel algorithms are required which are capable of matching ontologies
two ontologies using a small set of manually aligned concepts
in this paper we discuss several approaches to learning a matching function between two ontologies and evaluate two ontologies in crosslingual scenarios
two ontologies using a small set of manually aligned concepts on different pairs of financial accounting standards showing that multilingual information can indeed improve the matching quality even
in addition to this as current research on ontology matching does not make a satisfactory distinction between crosslingual ontology matching we quantify these terms in relation effects on different matching algorithms
in addition to this as current research on ontology matching does not make a satisfactory distinction between multilingual we quantify these terms in relation effects on different matching algorithms
in addition to this as current research on ontology matching does not make a satisfactory distinction between crosslingual ontology matching we provide precise definitions of these terms in relation to monolingual ontology matching
in addition to this as current research on ontology matching does not make a satisfactory distinction between multilingual we provide precise definitions of these terms in relation to monolingual ontology matchingwhen thousands of vocabularies having been published on the semantic web by various authorities a question arises as to how thousands of vocabularies are related to each other
existing work has mainly analyzed existing work similarity
in this paper we inspect the more general notion of relatedness
in this paper we characterize closeness in expressivity
in this paper we characterize the more general notion of relatedness from four angles welldefined semantic relatedness lexical similarity in contents
in this paper we characterize distributional relatedness
a large real data set containing 2996 vocabularies
we present an empirical study of these measures on a large real data
we present 15 million rdf documents that use 2996 vocabularies
then we propose to apply vocabulary relatedness to the problem of postselection vocabulary recommendation
we implement such a recommender service as part of a vocabulary search engine
we test a recommender service as part of a vocabulary search engine effectiveness against a handcrafted gold standardin this paper we present an approach to determining the cognitive complexity of justifications for entailments of owl ontologies
we present the results of validating that model via experiments
experiments involving owl users
we introduce a simple cognitive complexity model
the validation is based on test data
test data derived from a large corpus of naturally occurring justifications
test data derived from a diverse corpus of naturally occurring justifications
we contributions include new insights into justification complexity
we contributions include a significant corpus with novel analyses of justifications suitable for experimentation
we contributions include an experimental protocol suitable for model validation and refinement
we contributions include validation for the cognitive complexity modelwe present a new approach to adding closed world reasoning to the web ontology language owl
circumscriptive description logics which had the drawback of yielding an undecidable logic unless severe restrictions were imposed
a new approach to adding closed world reasoning to the web ontology language owl transcends previous work on circumscriptive description logics
in particular it was not possible in general to apply local closure to rolesas sparql endpoints are increasingly used to serve linked data sparql endpoints ability to scale becomes crucial
although much work has been done to improve query evaluation little has been done to take advantage of caching
effective solutions for caching query results can improve scalability by reducing latency
effective solutions for caching query results can improve scalability by reducing cpu overhead
effective solutions for caching query results can improve scalability by reducing network io
the database indexes found in common sparql implementations
we show that simple augmentation of the database indexes can directly lead to effective caching at the http protocol level
using tests from the berlin sparql benchmark we evaluate the potential of such caching to improve overall efficiency of sparql query evaluationin this paper we present fedbench
analyzing the performance of federated query processing strategies on semantic data
in this paper we present a comprehensive benchmark suite for testing
the major challenge lies in the heterogeneity of semantic data use cases at both the data such as varying data incomplete knowledge about data sources
the major challenge lies in the heterogeneity of semantic data use cases at both the data such as varying data varying degrees of query expressiveness
cases where applications may face different settings
the major challenge lies in the heterogeneity of semantic data use cases at both the data such as varying data access interfaces
the major challenge lies in the heterogeneity of semantic data use cases at both the data such as varying data availability of different statistics
the major challenge lies in the heterogeneity of semantic data use cases at both query level such as varying data access interfaces
the major challenge lies in the heterogeneity of semantic data use cases at both query level such as varying data incomplete knowledge about data sources
the major challenge lies in the heterogeneity of semantic data use cases at both query level such as varying data availability of different statistics
the major challenge lies in the heterogeneity of semantic data use cases at both query level such as varying data varying degrees of query expressiveness
a highly flexible benchmark suite which can be customized to accommodate a variety of use cases
accounting for this heterogeneity we present a highly flexible benchmark suite
a highly flexible benchmark suite which can be customized to compare competing approaches
we discuss design decisions highlight the flexibility in customization
we elaborate on the choice of query sets
we elaborate on the choice of data
various application scenarios where we indicate both the benefits
the practicability of we benchmark is demonstrated by a rigorous evaluation of various application scenarios
limitations of the stateoftheart federated query processing strategies for semantic datanowadays communication exchanges are time consuming part of peoples job especially for the so called knowledge workers
nowadays communication exchanges are an integral
an organisation which is only shared with those immediately involved in the particular communication act
contents discussed during meetings instant messaging exchanges informal communication exchanges exchanges therefore constitute a potential source of knowledge within an organisation
contents become
this work poses a knowledge management issue as this kind of contents  buried knowledge 
knowledge enabling topic trends spotting
this work uses semantic technologies to extract buried knowledge
knowledge enabling expertise finding spotting
specifically we claim it is possible to automatically model peoples expertise by monitoring informal communication exchanges
specifically we claim it is possible to automatically model peoples expertise by annotating peoples content to derive dynamic user profiles
profiles are then used to calculate similarity between people
profiles are then used to calculate plot semantic knowledgebased networks
a semantic network which reflects people expertise rather than capturing social interactions
semantic concepts captured from informal content to build a semantic network
the major contribution and novelty of this work is the exploitation of semantic concepts
we validate the approach
the approach using contents from a research group internal mailing list using informal communication exchanges exchanges within the group
the group collected over a ten months periodwhen ontological knowledge is acquired automatically quality control is essential
we consider an exhaustive manual inspection of the acquired data
we consider the tightest possible approach 
axioms that are entailed by the already approved statements
axioms that would lead to an inconsistency
by using automated reasoning we partially automate the process after each expert decision axioms are automatically approved whereas axioms are declined
adequate axiom ranking strategies are essential in this
this setting to minimize the amount of expert decisionstriple stores are the backbone of increasingly many data web applications
it is thus evident that the performance of triple stores is mission critical for data integration on the data web in general
it is thus evident that the performance of triple stores is mission critical for individual projects on the data web in general
consequently it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations
in this paper we propose a generic sparql benchmark creation procedure
a generic sparql benchmark creation procedure which we apply to the dbpedia knowledge base
previous approaches often compared relational stores and thus settled on measuring performance against a relational database
previous approaches often compared triple stores and thus settled on measuring performance against a relational database
a relational database which had been converted to rdf by using sqllike queries
queries that were actually issued by humans against existing rdf data not resembling a relational schema
queries that were actually issued by applications against existing rdf data not resembling a relational schema
in contrast to previous approaches often compared triple stores and thus settled on measuring performance against a relational database we benchmark is based on queries
a relational database which had been converted to rdf by using sqllike queries
in contrast to previous approaches often compared relational stores and thus settled on measuring performance against a relational database we benchmark is based on queries
we generic procedure for benchmark creation is based on querylog mining
we generic procedure for benchmark creation is based on clustering
we generic procedure for benchmark creation is based on sparql feature analysis
we argue that a pure sparql benchmark is more useful to provide results for the popular triple store implementations bigowlim
we argue that a pure sparql benchmark is more useful to compare existing triple stores
we argue that a pure sparql benchmark is more useful to provide results for the popular triple store implementations virtuoso sesame jenatdb
the subsequent comparison of our results with other benchmark results indicates that the performance of triple stores is by far less homogeneous than suggested by previous benchmarksnumerous forms of licensing terms are associated with web data and services
numerous forms of policies are associated with web data and services
numerous forms of related conditions are associated with web data and services
a natural goal for facilitating the reuse and recombination of such content is to model usage policies as part of the data so as to enable automated processing
a natural goal for facilitating the reuse and recombination of such content is to model usage policies as part of the data so as to enable the data exchange
this paper thus proposes a concrete policy modelling language
sharealike that mandate that derived content is published under some license with the same permissions and requirements
a particular difficulty are selfreferential policies such as creative commons sharealike
we present a general semantic framework for evaluating such recursive statements show that a general semantic framework for evaluating such recursive statements explain how it can be evaluated using existing tools
we present a general semantic framework for evaluating such recursive statements show that a general semantic framework for evaluating such recursive statements has desirable formal properties
we then show that we approach is compatible with both owl dl
we then show that we approach illustrate how one can concretely model selfreferential policies in these languages to obtain desired conclusions
we then show that we approach is compatible with both datalogthe purpose of data browsers is to help users identify
the purpose of data browsers is to help users query data effectively without being overwhelmed by large complex graphs of data
a proposed solution to identify data in graphbased datasets is or setoriented browsing 
a manytomany graph browsing technique
a proposed solution to identify data in graphbased datasets is pivoting 
instances followed by navigation through common links
a proposed solution to query data in graphbased datasets is pivoting 
a proposed solution to query data in graphbased datasets is or setoriented browsing 
a proposed solution to identify data in graphbased datasets is a manytomany graph
technique that allows users to navigate the graph by starting from a set of instances
a proposed solution to query data in graphbased datasets is a manytomany graph
relying solely on navigation however makes it difficult for users to even see if the element of interest is in the graph
relying solely on navigation however makes it difficult for users to find paths
the graph when the points of interest may be many vertices apart
paths which require combinations of forward links in order to make the necessary connections
further challenges include finding paths
paths which require combinations of backward links in order to make the necessary connections
the necessary connections which further adds to the complexity of pivoting
in order to enhance the strengths of pivoting we present a multipivot approach which we embodied in tool
in order to mitigate the effects of these problems present a multipivot approach which we embodied in tool
tool called visor
visor allows users to explore from multiple points in the graph helping users connect key points of interest in the graph on the conceptual level visually occluding the remainder parts of the graph thus helping create a roadmap for navigation
we carried out an user study to demonstrate the viability of we approachin this paper we address the problem of scalable query processing over linked stream data
in this paper we address the problem of native query processing over linked stream data
stream data integrated with linked data
in this paper we address the problem of adaptive query processing over linked stream data
sensors enriched with semantic descriptions following the standards
linked stream data consists of data
linked stream data consists of sensors
data generated by stream sources
the standards proposed for linked data
this enables the integration of stream data with linked data collections
this facilitates a wide range of novel applications
currently available systems use a  black box  approach which delegates the processing to other engines such as streamevent processing engines by translating to other engines provided languages
currently available systems use a  black box  approach which delegates the processing to other engines such as sparql query processors by translating to other engines provided languages
as the experimental results the need for the lack of full control over the query execution pose major drawbacks in terms of efficiency
the experimental results described in this paper show
as the experimental results the need for query translation pose major drawbacks in terms of efficiency
as the experimental results the need for data transformation pose major drawbacks in terms of efficiency
to remedy major drawbacks we present continuous query evaluation over linked streams  for unified query processing over linked linked data
to remedy major drawbacks we present a native query processor for unified query processing over linked stream data
to remedy major drawbacks we present a native query processor for unified query processing over linked linked data
to remedy major drawbacks we present a adaptive query processor for unified query processing over linked stream data
to remedy major drawbacks we present cqels  for unified query processing over linked linked data
to remedy major drawbacks we present a adaptive query processor for unified query processing over linked linked data
to remedy major drawbacks we present cqels  for unified query processing over linked stream data
to remedy major drawbacks we present continuous query evaluation over linked streams  for unified query processing over linked stream data
in contrast to the existing systems cqels uses a  white box  implements the required query operators natively to avoid the overhead and limitations of closed system regimes
in contrast to the existing systems cqels uses a  white box  approach to avoid the overhead and limitations of closed system regimes
cqels provides a flexible query execution framework with the query processor dynamically adapting to the changes in the input data
during query execution cqels continuously reorders operators according to some heuristics to achieve improved query execution in terms of delay
during query execution cqels continuously reorders operators according to some heuristics to achieve improved query execution in terms of complexity
moreover external disk access on large linked data collections is reduced with the use of data caching of intermediate query results
moreover external disk access on large linked data collections is reduced with the use of data encoding of intermediate query results
to demonstrate the efficiency of our approach our present extensive experimental performance evaluations in terms of query execution time under dataset sizes
to demonstrate the efficiency of our approach our present extensive experimental performance evaluations in terms of query execution time under number of parallel queries
to demonstrate the efficiency of our approach our present extensive experimental performance evaluations in terms of query execution time under varied query types
these results show that cqels outperforms related approaches by orders of magnitudethe increasing availability of large rdf datasets offers an exciting opportunity to use such data to build predictive models using machine learning algorithms
however the massive size and distributed nature of rdf data calls for approaches to learning from rdf data in the sparql endpoint of the rdf store
a setting where the data can be accessed only through a query interface
however the massive size and distributed nature of rdf data calls for approaches to learning from rdf data in a setting
algorithms that allow the predictive model to be incrementally updated in response to changes in the data
applications where the data are subject to frequent updates
in applications there is a need for algorithms
the attributes that are relevant for specific prediction tasks
furthermore in some applications the attributes are not known a priori
furthermore in some applications the attributes hence need to be discovered by the algorithm
we present an approach to learning relational bayesian classifiers from rdf data that addresses such scenarios
specifically we show how to build relational bayesian classifiers from rdf data using statistical queries through the sparql endpoint of the rdf store
one that requires direct centralized access to the data
we compare the communication complexity of we algorithm with one
one that has to retrieve the entire rdf dataset from the remote location for processing
the conditions under which the relational bayesian classifier models can be incrementally updated in response to addition of rdf data
we establish the conditions
the conditions under which the relational bayesian classifier models can be incrementally updated in response to deletion of rdf data
we show how we approach can be extended to the setting where the attributes are not known a priori by selectively crawling the rdf data for attributes of interest
the attributes that are relevant for prediction
we provide open source implementation
we evaluate the proposed approach on several large rdf datasetsthe proliferation of semantic data on the web requires rdf database systems to constantly improve transactional efficiency
the proliferation of semantic data on the web requires rdf database systems to constantly improve rdf database systems scalability
at the same time users are increasingly interested in visualizing large collections of online data by performing complex analytic queries
at the same time users are increasingly interested in investigating
diplodocus  inyrdf which supports both transactional and analytical queries efficiently
this paper introduces a novel database system for rdf data management called diplodocus  inyrdf
diplodocus  inyrdf takes advantage of a new hybrid storage model for rdf data based on recurring graph patterns
in this paper we describe the general architecture of we system
in this paper we compare our system performance to stateoftheart solutions for both analytic workloads
in this paper we compare our system performance to stateoftheart solutions for both transactional workloads
