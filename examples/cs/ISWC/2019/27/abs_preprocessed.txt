In this paper we conduct an empirical investigation of neural query graph ranking approaches for the task of complex question answering over knowledge graphs
We propose a novel selfattention based slot matching model
slot matching model which exploits the inherent structure of query graphs our logical form of choice
We proposed model generally outperforms other ranking models on two QA datasets over the DBpedia knowledge graph evaluated in different settings
We also show that domain adaption based transfer
transfer learning yield improvements effectively offsetting the general lack of training data
We also show that pretrained language model based transfer