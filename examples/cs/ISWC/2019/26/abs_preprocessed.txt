Answering simple questions over knowledge graphs is a wellstudied problem in question answering
Previous approaches for this task built on convolutional neural network
recurrent neural network based architectures
Previous approaches for this task built on recurrent neural network
architectures that use pretrained word embeddings
convolutional neural network based architectures
It was recently shown that finetuning eg  can outperform previous approaches on various natural language processing tasks
It was recently shown that finetuning pretrained transformer networks  can outperform previous approaches on various natural language processing tasks
It was recently shown that finetuning BERT  can outperform previous approaches on various natural language processing tasks
In this work we investigate how well both BERT provide BiLSTMbased models in limiteddata scenarios
In this work we investigate how well both BERT performs on SimpleQuestions
In this work we investigate how well both BERT provide an evaluation of both BERT