several centralised rdf systems support datalog reasoning by precomputing all logically implied triples
all logically implied triples using the wellknown seminaive algorithm
several centralised rdf systems support datalog reasoning by storing all logically implied triples
a common solution is to distribute large rdf datasets in a cluster of sharednothing servers
large rdf datasets often exceed the capacity of centralised rdf systems
numerous distributed query answering techniques
while numerous are known distributed seminaive evaluation of arbitrary datalog rules is less understood
in fact most distributed support no reasoning
in fact most distributed rdf stores
in fact most can handle only limited datalog fragments
in this paper we extend the dynamic data exchange approach for distributed query answering by potter et al
to a reasoning algorithm that can handle arbitrary rules while preserving important properties such as nonrepetition of inferences
we also show empirically that we algorithm scales well to very large rdf datasetsdespite the prospect of a vast web of interlinked data the semantic web today mostly fails to meet semantic web potential
semantic web current architecture which totally relies on the availability of the servers
the availability of the servers providing access to the data
one of the main problems semantic web faces is rooted in semantic web current architecture
the servers are subject to failures
situations where some data is unavailable
failures which often results in situations
decentralized peertopeer based architectures to alleviate this problem
recent advances have proposed decentralized peertopeer
however for query processing these approaches mostly rely on flooding a standard technique for peertopeer systems and hence because high query response times
peertopeer systems which can easily result in very high network traffic
to still enable efficient query processing in such networks this paper proposes two indexing schemes locational indexes
to still enable efficient query processing in such networks this paper proposes two indexing schemes prefixpartitioned bloom filters
two indexing schemes which in a decentralized fashion aim at efficiently finding nodes with relevant data for a given query
our experiments show that such indexing schemes are able to considerably speed up query processing times compared to existing approachescollaborative knowledge graph platforms allow automated scripts to collaborate in creating updating entities
collaborative knowledge graph platforms allow humans to collaborate in creating interlinking entities
collaborative knowledge graph platforms allow automated scripts to collaborate in creating interlinking entities
collaborative knowledge graph platforms allow humans to collaborate in creating interlinking facts
collaborative knowledge graph platforms allow humans to collaborate in creating updating entities
collaborative knowledge graph platforms allow automated scripts to collaborate in creating interlinking facts
collaborative knowledge graph platforms allow humans to collaborate in creating updating facts
collaborative knowledge graph platforms allow automated scripts to collaborate in creating updating facts
to ensure both a uniform coverage of the different topics it is crucial to identify underrepresented classes in the knowledge graph
to ensure both the completeness of the data it is crucial to identify underrepresented classes in the knowledge graph
in this paper we tackle this problem by developing statistical techniques for class cardinality estimation in collaborative knowledge graph platforms
we method is able to estimate the completeness of a class as defined by a schema or ontology hence can be used to answer questions such as  does the knowledge base have a complete list of all beer brands  volcanos  video game consoles 
wikidata which poses unique challenges in terms of the size of its ontology
wikidata which poses unique challenges in terms of the size of its ontology
wikidata which poses unique challenges in terms of the size of its ontology
wikidata which poses unique challenges in terms of the size of wikidata
wikidata which poses its extremely dynamic nature extremely dynamic nature
wikidata which poses the number of users actively populating its graph
as a usecase we focus on wikidata
wikidata which poses its extremely dynamic nature ontology
wikidata which poses its extremely dynamic nature graph
wikidata which poses the number of users actively populating wikidata
wikidata which poses the number of users actively populating its graph
wikidata which poses the number of users actively populating its graph
wikidata which poses wikidata
we techniques are applied to the case of collaborative editing
we techniques are derived from datamanagement methodologies
we techniques are applied to the case of graphs
we techniques are derived from species estimation
we observe that the number and frequency of unique class instances drastically influence the performance of an estimator because some estimators to overestimate the true size of the class if some estimators are not properly handled
in we empirical evaluation
we observe that the number and frequency of unique class instances drastically influence the performance of bursts of inserts because some estimators to overestimate the true size of the class if some estimators are not properly handled
one can effectively measure the convergence of a class towards its true size by considering the stability of an estimator against the number of available instancesknowledge graphs are composed of different elements
knowledge graphs are composed of relation edges
knowledge graphs are composed of literal nodes
knowledge graphs are composed of entity nodes
information which in general can not be represented by relations between entities alone
each literal node thereby encodes information
each literal node contains the height of an entity of type person 
each literal node contains an entitys attribute value 
however most of the existing embedding only thus do not take the information
however latentfeaturebased methods for knowledge graph analysis only consider relation edges
however most of the existing embedding only consider entity nodes
the information provided by literals into account
however latentfeaturebased methods for knowledge graph analysis only consider entity nodes
however latentfeaturebased methods for knowledge graph analysis only thus do not take the information
however most of the existing embedding only consider relation edges
in this paper we extend existing latent feature methods for link prediction by a simple portable module for incorporating literals
literals which we name literale
concurrent methods where literals are affect the entity embeddings
unlike in concurrent methods literale directly enriches these embeddings with information from literals via a learnable parametrized function
concurrent methods where literals are incorporated by adding a literaldependent term to the output of the scoring function
a learnable parametrized function learned along with the entity embeddings in an endtoend manner
a learnable parametrized function can be easily integrated into the scoring function of existing methods
in an extensive empirical study over three datasets we evaluate literaleextended versions of various stateoftheart latent feature methods for link prediction and demonstrate that literale presents an effective way to improve three datasets performance
three datasets literals which we publicly provide as testbeds for further research
for these experiments we augmented standard datasets with three datasets literals
moreover we show that it can be easily extended to handle literals from different modalities
moreover we show that literale leads to an qualitative improvement of the embeddingsany data publisher can make rdf knowledge graphs available for consumption on the web
the data web which has led to more than 150 billion facts on more than 3 billion things
the decentralized publishing paradigm underlying the data web
more than 3 billion things being published on the web in more than 10000 rdf knowledge graphs over the last decade
this is a direct consequence of the decentralized publishing paradigm
however the success of this publishing paradigm also means that the validation of the facts contained in rdf knowledge graphs has become more important than ever before
several families of fact validation algorithms have been developed over the last years to address several settings of the fact validation problems
in this paper we consider the following fact validation setting given an rdf knowledge graph compute the likelihood that a given  novel  fact is true
none of the current solutions to this problem exploits rdfs semantics especially domain range subsumption information
none of the current solutions to this problem exploits rdfs semantics especially domain class subsumption information
we address this research gap by presenting an unsupervised approach dubbed copaal that extracts paths from knowledge graphs to corroborate  novel  input facts
the rdfs semantics underlying the knowledge graph into consideration
a mutual information measure that takes the rdfs semantics
we approach relies on a mutual information measure
in particular we use the information
the information shared by paths within the knowledge graph to compute the likelihood of a fact
a fact being corroborated by the knowledge graph
the information shared by predicates within the knowledge graph to compute the likelihood of a fact
we evaluate we approach extensively using 17 publicly available datasets
we results indicate that we approach outperforms the state of the art unsupervised approaches significantly by up to 015 aucroc
we even outperform supervised approaches by up to 007 aucroc
the source code of copaal is opensource
the source code of copaal is available at githubcomdicegroupcopaal
the source code of copaal is available at httpsontology design patterns have been proposed to facilitate ontology engineering
the often claimed benefits provided by odps
despite numerous conceptual contributions for over more than a decade there is little empirical work to support the often claimed benefits
determining ontology design patterns use from ontologies alone  without other supporting documentation  is challenging as there is or required  mechanism for stipulating the intended use of an ontology design patterns
determining ontology design patterns use from ontologies alone  without interviews  is challenging as there is no standard  mechanism for stipulating the intended use of an ontology design patterns
determining ontology design patterns use from ontologies alone  without interviews  is challenging as there is or required  mechanism for stipulating the intended use of an ontology design patterns
determining ontology design patterns use from ontologies alone  without other supporting documentation  is challenging as there is no standard  mechanism for stipulating the intended use of an ontology design patterns
modelling features which are suggestive of a given ontology design
instead we must rely on modelling features patternss influence
for the purpose of determining the prevalence of odps in ontologies we developed a variety of these techniques to detect these features with varying degrees of liberality
using these techniques we survey bioportal with respect to wellknown repositories for odps
using these techniques we survey bioportal with respect to publicly available repositories for odps
we findings are predominantly negative
for the vast majority of odps we can not find empirical evidence for the vast majority of odps use in biomedical ontologiesthe rapid progress of question answering  qa  systems over the rapid progress of question answering qa systems over knowledge bases  enables end users to acquire knowledge with natural language questions
the rapid progress of question answering  qa  systems over knowledge bases  enables end users to acquire knowledge with natural language questions
while mapping relational phrases to semantic constructs in the rapid progress of question answering qa systems over knowledge bases has been extensively studied little attention has been devoted to adjectives
while mapping proper nouns to semantic constructs in the rapid progress of question answering qa systems over knowledge bases has been extensively studied little attention has been devoted to adjectives
adjectives most of which play the role of factoid constraints on the modified nouns
in this paper we study the problem of finding appropriate representations for adjectives over the rapid progress of question answering qa systems over knowledge bases
a novel approach called adj2er
we propose a novel approach to automatically map an adjective to several existential restrictions negation forms
we propose a novel approach to automatically map an adjective to several existential restrictions negation forms
the candidates which largely reduce the search space
specifically we leverage statistic measures for generating supervised learning for filtering the candidates
specifically we leverage statistic measures for generating candidate existential restrictions for filtering the candidates
the candidates which largely overcome the lexical gap
we create two question sets with adjectives from yahoo answers
we create two question sets with conduct experiments over dbpedia
we create two question sets with adjectives from qald
we experimental results show that adj2er can generate highquality mappings for significantly outperform several alternative approaches
we experimental results show that adj2er can generate highquality mappings for most adjectives
furthermore current qa systems can gain a promising improvement when integrating our adjective mapping approachwe propose a new endtoend method for extending a knowledge graph from tables
existing techniques tend to tend to extract many redundant facts
information that is already in the a
existing techniques tend to interpret tables by focusing on information knowledge graph
we method aims to find more novel facts
a scalable graphical model using entity similarities
we introduce a new technique for table interpretation based on a scalable graphical model
our method further disambiguates cell values using a knowledge graph embeddings as additional ranking method
other distinctive features are the lack of assumptions about the underlying a knowledge graph
other distinctive features are the lack of the enabling of a finegrained tuning of the precisionrecall tradeoff of extracted facts
we experiments show that we approach has a higher recall during the interpretation process than the stateoftheart
we experiments show that we approach is more resistant against the bias
the bias observed in extracting mostly redundant facts since the stateoftheart produces more novel extractionsknowledge graphs have become ubiquitous data knowledge graphs utility has been amplified by the research on ability to answer carefully crafted questions over knowledge graphs
knowledge graphs have become ubiquitous data sources utility has been amplified by the research on ability to answer carefully crafted questions over knowledge graphs
we investigate the problem of question generation over knowledge graphs wherein the level of difficulty of the question can be controlled
we present an endtoend neural networkbased method for automatic generation of complex multihop questions over knowledge graphs
taking an answer as input we transformerbased model generates a natural language question
taking a subgraph as input we transformerbased model generates a natural language question
our transformerbased model makes use of this estimation to generate difficultycontrollable questions
our transformerbased model incorporates difficulty estimation based on named entity popularity
we evaluate our transformerbased model on two recent multihop qa datasets
we evaluation shows that our transformerbased model is able to generate highquality questions
we evaluation shows that our transformerbased model is able to generate fluent questions
we evaluation shows that our transformerbased model is able to generate relevant questions
we have released we curated question generation dataset at githubcomliyuanfangmhqg
we have released we curated question generation dataset at https 
we have released we curated code at https 
we have released we curated code at githubcomliyuanfangmhqgthe language is a specification for validating rdf graphs
the language is a specification for describing rdf graphs
rdf graphs that has recently become a w3c recommendation
while the language is gaining traction in the industry algorithms for shacl constraint validation are still at an early stage
a first challenge comes from the fact that rdf graphs are often therefore only accessible via queries
a first challenge comes from the fact that rdf graphs are often exposed as sparql endpoints
another difficulty is the absence of guidelines about the way recursive constraints should be handled
a shacl schema which can be executed over a sparql endpoint
in this paper we provide algorithms for validating a graph against a shacl schema
we first investigate the possibility of validating a graph through a single query for nonrecursive constraints
a strategy that consists in evaluating a small number of sparql queries over the endpoint
propositional formulas that are passed to a sat solver
a strategy that consists in using the answers to build a set of propositional formulas
then for the recursive case since the problem has been shown to be nphard we propose a strategy
finally we show that the process can be optimized when dealing with recursive fragments of shacl without the need for an external solver
finally we show that the process can be optimized when dealing with tractable fragments of shacl without the need for an external solver
we also present a proofofconcept evaluation of this last approachrelation linking
relation is an important problem for knowledge graphbased question answering
given a natural language question the task is to identify relevant relations from the given knowledge graph
given a knowledge graph the task is to identify relevant relations from the given knowledge graph
since existing techniques for entity extraction are more stable compared to relation our idea is to exploit entities
entities extracted from the question to support relation
since linking are more stable compared to relation our idea is to exploit entities
relation linking
relation linking
in this paper we propose a novel approach based on dbpedia entities for computing relation candidates
we have empirically evaluated we approach on different standard benchmarks
we evaluation shows that we approach significantly outperforms existing baseline systems in both recall precision and runtimean ontology which is usually expressed in a description logic of dllite family
ontologybased data access is a popular approach for querying multiple data sources by means of an ontology
ontologybased data access is a popular approach for integrating multiple data sources by means of an ontology
the conventional semantics of ontologybased data access and dls is setbased that is duplicates are disregarded
this disagrees with the standard database bag  semantics
the standard database bag  semantics which is especially important for the correct evaluation of aggregate queries
multiset  semantics which is especially important for the correct evaluation of aggregate queries
this disagrees with multiset  semantics
in this article we study two variants of bag semantics for query answering over  textitdllite  mathcalf  extending basic  with functional roles
in this article we study two variants of bag semantics for query answering over  textitdllite  mathcalf  extending textitdllite  textitcore  with functional roles
the semantics of primary keys in sql conjunctive query answering is in tc  that tc  for the restricted class of rooted cqs such cqs are also rewritable to the bag relational algebra
our first semantics which follows the semantics of primary keys in sql
the semantics of primary keys in sql conjunctive query answering is in tc  that tc  for the restricted class of rooted cqs such cqs are also rewritable to the bag relational algebra
for our first semantics conjunctive query answering is conphard in data complexity in general such cqs are also rewritable to the bag relational algebra
for our second semantics
rewritability hold only for the restricted class of ontologies
the results are the same except that tc membership
ontologies identified by a new notion of functional weak acyclicityanalytical queries are queries with numerical aggregators computing the average number of objects per property identifying the most frequent subjects
such queries are essential to monitor the content of the linked open data cloud
such queries are essential to monitor the quality
many analytical queries can not be executed directly on the sparql endpoints because the fair use policy cuts off expensive queries
in this paper we show how to rewrite such queries into a set of queries that each satisfy the fair use policy
we then show how to execute these queries in such a way that the result provably converges to the exact query answer
we algorithm is an anytime algorithm
an anytime algorithm meaning that our algorithm can give intermediate approximate results at any time point
we experiments show that the approach converges rapidly towards the exact solution
we experiments show that the approach can compute even complex indicators at the scale of the linked open data cloudmarkup languages such as rdfa have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as microdata potentially increases markup languages such as rdfa sales
markup languages such as rdfa have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as rdfa potentially increases markup languages such as microdata sales
markup languages such as rdfa have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as rdfa improve clickthrough rates for eshops
markup languages such as rdfa have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as rdfa potentially increases markup languages such as rdfa sales
markup languages such as microdata have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as microdata potentially increases markup languages such as microdata sales
markup languages such as rdfa have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as microdata improve clickthrough rates for eshops
markup languages such as microdata have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as rdfa potentially increases markup languages such as rdfa sales
markup languages such as microdata have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as microdata improve clickthrough rates for eshops
markup languages such as rdfa have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as microdata potentially increases markup languages such as microdata sales
markup languages such as microdata have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as rdfa improve clickthrough rates for eshops
markup languages such as microdata have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as rdfa potentially increases markup languages such as microdata sales
markup languages such as microdata have been widely used by eshops to embed structured product data as evidence has shown that markup languages such as microdata potentially increases markup languages such as rdfa sales
while eshops often embed certain categorisation information in markup languages such as rdfa product data in order to improve markup languages such as rdfa products visibility to product search such sitespecific product category labels are highly inconsistent and unusable across websites
while eshops often embed certain categorisation information in markup languages such as microdata product data in order to improve markup languages such as rdfa products visibility to aggregator services such sitespecific product category labels are highly inconsistent and unusable across websites
while eshops often embed certain categorisation information in markup languages such as rdfa product data in order to improve markup languages such as microdata products visibility to product search such sitespecific product category labels are highly inconsistent and unusable across websites
while eshops often embed certain categorisation information in markup languages such as rdfa product data in order to improve markup languages such as rdfa products visibility to aggregator services such sitespecific product category labels are highly inconsistent and unusable across websites
while eshops often embed certain categorisation information in markup languages such as microdata product data in order to improve markup languages such as microdata products visibility to product search such sitespecific product category labels are highly inconsistent and unusable across websites
while eshops often embed certain categorisation information in markup languages such as microdata product data in order to improve markup languages such as rdfa products visibility to product search such sitespecific product category labels are highly inconsistent and unusable across websites
while eshops often embed certain categorisation information in markup languages such as rdfa product data in order to improve markup languages such as microdata products visibility to aggregator services such sitespecific product category labels are highly inconsistent and unusable across websites
while eshops often embed certain categorisation information in markup languages such as microdata product data in order to improve markup languages such as microdata products visibility to aggregator services such sitespecific product category labels are highly inconsistent and unusable across websites
this work studies the task of automatically classifying products into a universal categorisation taxonomy using products markup data
markup data published on the web
we best performing model can significantly improve state of the art on this task by up to 96 percent points in macroaverage f1
despite the highly heterogeneous nature of the sitespecific categories the sitespecific categories can be used as very effective features even only by the sitespecific categories for the classification task
using three new neural network models adapted based on previous work we show that
using three new neural network models adapted based on previous work we analyse the effect of different kinds of product markup data on this taskthe wikipedias category graph has been used extensively for tasks like semantic similarity estimation
the wikipedias category graph serves as the taxonomic backbone for largescale knowledge graphs like yago
the wikipedias category graph serves as the taxonomic backbone for largescale knowledge graphs like probase
the wikipedias category graph has been used extensively for tasks like entity disambiguation
wikipedias categories are a rich source of nontaxonomic information
wikipedias categories are a rich source of taxonomic information
the category their nationality science fiction writers for example encodes genre  science fiction 
the category their nationality science fiction writers for example encodes the type of wikipedias resources  science fiction 
the category their nationality science fiction writers for example encodes their nationality  science fiction 
several approaches in the literature make use of fractions of this
this encoded information without exploiting wikipedias full potential
category axioms that uses information from category instances
category axioms that uses information from the category network
in this paper we introduce an approach for the discovery of category axioms
category axioms that uses information from category instances lexicalisations
with dbpedia as background knowledge
we discover 703k axioms covering 502k of wikipedias categories
33 m type assertions at more than 87 percent respectively
we populate the dbpedia knowledge graph with additional 44 m relation assertions
33 m type assertions at 90 percent precision respectivelytools that can effectively support users in analysis tasks
tools that can effectively support users in exploration tasks
largescale knowledge graphs are increasingly being used in applications
there is a growing need for tools
one such important task is entity comparison to describe in an informative way the similarities between two given entities as described in a knowledge graph
a query asking for all telecom companies based in europe
in our previous work the result of entity comparison is modelled as a similarity query that is a sparql query having the input entities as part of the answer over the input graph for instance one can describe the similarity between two companies such as telenor in the yago graph as a query
in our previous work the result of entity comparison is modelled as a similarity query that is a sparql query having the input entities as part of the answer over the input graph for instance one can describe the similarity between two companies such as vodafone in the yago graph as a query
in this paper our extend the results of our prior work in different ways
first our expand the language of similarity queries to consider a richer fragment of sparql this enables our to express that vodafone are also similar in that telenor both have at least 30000 employees
first our expand the language of similarity queries to consider a richer fragment of sparql this enables our to express that telenor are also similar in that vodafone both have at least 30000 employees
first our expand the language of similarity queries to consider a richer fragment of sparql this enables our to express that vodafone are also similar in that vodafone both have at least 30000 employees
sparql allowing for numeric filter expressions
first our expand the language of similarity queries to consider a richer fragment of sparql this enables our to express that telenor are also similar in that telenor both have at least 30000 employees
computing similarity queries satisfying certain additional desirable properties such as being as specific as possible
we then propose algorithms for computing similarity queries
such algorithms are however impractical hence we also implement necessarily a most specific one
a scalable algorithm that is guaranteed to compute a similarity query
such algorithms are however impractical hence we also propose necessarily a most specific one
such algorithms are however impractical hence we also implement a scalable algorithm
such algorithms are however impractical hence we also propose a scalable algorithmefficient ontology reuse is a key factor in the semantic web to enhance the interoperability of computing systems
efficient ontology reuse is a key factor in the semantic web to enable the interoperability of computing systems
one important aspect of ontology reuse is concerned with ranking most relevant ontologies based on a keyword query
apart from the semantic match of query the stateoftheart often relies on ontologies occurrences in the linked open data cloud to determine relevance
apart from the semantic match of ontology the stateoftheart often relies on ontologies occurrences in the linked open data cloud to determine relevance
we observe that ontologies of some application domains in particular those related to web of things often do not appear in the underlying linked open data datasets used to define ontologies popularity
ontologies popularity resulting in ineffective ranking scores
this motivated us to investigate based on the problematic things case whether the scope of ranking models can be extended by relying on qualitative attributes instead of an explicit popularity feature
a ranking model that uses ontologies popularity as prediction target for the relevance degree
we propose a novel approach to ontology ranking by selecting a range of relevant qualitative features
we propose a novel approach to ontology ranking by proposing a popularity measure for ontologies
testing ontologies on independent datasets derived from the stateoftheart
we propose a novel approach to ontology ranking by confirming ontologies validity by testing ontologies on independent datasets
we propose a novel approach to ontology ranking by training a ranking model
ontologies based on scholarly data
we find that qualitative features help to improve the prediction of the relevance degree in terms of popularity
we further discuss the influence of these features on the ranking modelin this paper we conduct an empirical investigation of neural query graph ranking approaches for the task of complex question answering over knowledge graphs
slot matching model which exploits the inherent structure of query graphs our logical form of choice
we propose a novel selfattention based slot matching model
we proposed model generally outperforms other ranking models on two qa datasets over the dbpedia knowledge graph evaluated in different settings
transfer learning yield improvements effectively offsetting the general lack of training data
we also show that domain adaption based transfer
we also show that pretrained language model based transferan empirical study aiming at understanding the modeling style
this paper presents an empirical study
an empirical study aiming at understanding the overall semantic structure of linked open data
we observe how individuals are used in practice
we observe how classes are used in practice
we observe how properties are used in practice
we also investigate how hierarchies of concepts are structured
we also investigate how much hierarchies of concepts are linked
an open source implementation that facilitates this paper application to other linked data knowledge graphs
metrics which generalises over an open source implementation
in addition to discussing the results this paper contributes a set of metrics
metrics which generalises over the observable constructs
in addition to discussing the results this paper contributes a conceptual frameworkhuge knowledge bases whose content has been generated from collaborative platforms
huge knowledge bases whose content has been generated by integration of heterogeneous databases
semantic web connects huge knowledge bases
huge knowledge bases whose content has been generated from collaborative platforms
naturally huge knowledge bases are incomplete
huge knowledge bases whose content has been generated by integration of heterogeneous databases
naturally huge knowledge bases contain erroneous data
huge knowledge bases whose content has been generated from collaborative platforms and by integration of heterogeneous databases returns reliable results
huge knowledge bases whose content has been generated from collaborative platforms and by integration of heterogeneous databases data quality
that querying huge knowledge bases
knowing huge knowledge bases is an essential longterm goal to guarantee that
completely insufficiently informed
having cardinality constraints for roles would be an important advance to distinguish correctly
those having data either incorrect
completely described individuals from those
each concept when the knowledge bases exists
in this paper we propose a method for automatically discovering from the knowledge bases content the maximum cardinality of roles for each concept
a method is robust thanks to the use of hoeffdings inequality
pruning properties that drastically reduce the search space
a knowledge base benefiting from pruning properties
we also design an algorithm for an exhaustive search of such constraints in a knowledge base
an algorithm named c3m
experiments conducted on dbpedia
also highlight the robustness of we method with a precision higher than 95 percent
experiments demonstrate the scaling up of c3manswering simple questions over knowledge graphs is a wellstudied problem in question answering
recurrent neural network based architectures
convolutional neural network based architectures
previous approaches for this task built on recurrent neural network
architectures that use pretrained word embeddings
previous approaches for this task built on convolutional neural network
it was recently shown that finetuning bert  can outperform previous approaches on various natural language processing tasks
it was recently shown that finetuning pretrained transformer networks  can outperform previous approaches on various natural language processing tasks
it was recently shown that finetuning can outperform previous approaches on various natural language processing tasks
in this work we investigate how well both bert provide bilstmbased models in limiteddata scenarios
in this work we investigate how well both bert performs on simplequestions
in this work we investigate how well both bert provide an evaluation of both bertthe ontologybased data access  paradigm can ease access to incomplete data sources in many application domains
obda  paradigm can ease access to incomplete data sources in many application domains
the ontologybased data access  paradigm can ease access to heterogeneous data sources in many application domains
obda  paradigm can ease access to heterogeneous data sources in many application domains
however stateoftheart tools are still based on the dllite family of description logics
owl 2 ql which despite its usefulness is not sufficiently expressive for many domains
description logics that underlies owl 2 ql
the consensus is that horn description logics like horn  mathcal shiq  are particularly promising
accommodating more expressive ontology languages remains an open challenge
many ontologies can not be handled
a prerequisite for obda is supported in existing reasoners
query answering in horn   is supported in existing reasoners
mathcal shiq   is supported in existing reasoners
this is largely because algorithms build on an aboxindependent approach to ontological reasoning
ontological reasoning that easily incurs in an exponential behaviour
as an alternative to full aboxindependence in this paper we advocate taking into account general information about the structure of the aboxes of interest
this have a predictable structure
this is especially natural in the setting of obda where aboxes are generated via mappings
the abox which can be obtained from the mappings of an obda specification
guides ontological reasoning using the possible combinations of concepts
concepts that may occur in the abox
we present a simple yet effective approach that guides ontological reasoning
we tested we optimization in the clipper reasoner with encouraging results
we implemented we optimization in the clipper reasoner with encouraging resultsmodelbased approaches to recommendation can recommend items with a very high level of accuracy
unfortunately even when the model embeds contentbased information if we move to a latent space we miss references to the actual semantics of recommended items
consequently this makes nontrivial the interpretation of a recommendation process
semantic features coming from a knowledge graph in order to train an interpretable model
in this paper we show how to initialize latent factors in factorization machines by using semantic features
with we semantic features are injected into the learning process to retain the original informativeness of the items available in the dataset
the accuracy and effectiveness of the trained model have been tested using two wellknown recommender systems datasets
by relying on the information we have also evaluated the semantic accuracy and robustness for the knowledgeaware interpretability of the final model
the information encoded in the original knowledge graphnongoal oriented generative dialogue systems lack the ability to generate answers with grounded facts
the real world consisting of wellgrounded facts
a knowledge graph can be considered an abstraction of the real world
this paper addresses the problem of generating wellgrounded responses by integrating knowledge graphs into the dialogue systems response generation process in an endtoend manner
a dataset for nongoal oriented dialogues is proposed in this paper in the domain of soccer conversing on different clubs along with a knowledge graph for each of national teams
a dataset for nongoal oriented dialogues is proposed in this paper in the domain of soccer conversing on national teams along with a knowledge graph for each of national teams
a dataset for nongoal oriented dialogues is proposed in this paper in the domain of soccer conversing on different clubs along with a knowledge graph for each of different clubs
a dataset for nongoal oriented dialogues is proposed in this paper in the domain of soccer conversing on national teams along with a knowledge graph for each of different clubs
a novel neural network architecture is also proposed as a baseline on this dataset knowledge grounded responses
this dataset which can integrate knowledge graphs into the response generation process producing well articulated
knowledge graph integrated dialogue systems
empirical evidence suggests that the proposed model performs better than other stateoftheart models for knowledge graphevent series such as the wimbledon championships represent important happenings in
culture
key societal areas
event series such as the us presidential elections represent important happenings in key societal areas
sports
politics
event series such as the wimbledon championships represent important happenings in key societal areas
event series such as the us presidential elections represent important happenings in
however semantic reference sources such as dbpedia knowledge graphs provide only an incomplete event series representation
however semantic reference sources such as eventkg knowledge graphs provide only an incomplete event series representation
however semantic reference sources such as wikidata knowledge graphs provide only an incomplete event series representation
in this paper we target the problem of event series completion in a knowledge graph
realworld events that happened as a part of event series
we address two tasks inference of realworld events
realworld events that are missing in the knowledge graph
we address two tasks prediction of subevent relations
to address these problems we proposed supervised happening approach leverages structural features of event series
happening does not require any external knowledge the characteristics
the characteristics making it unique in the context of event inference
we experimental evaluation demonstrates that happening outperforms the baselines by 44
we experimental evaluation demonstrates that happening outperforms the baselines 52 percent points in terms of precision for the subevent prediction
we experimental evaluation demonstrates that happening outperforms the baselines 52 percent points in terms of the inference tasks correspondinglythe launch of the new google news in 2018 introduced the frequently asked questions feature to structurally summarize the news story in the launch of the new google news in 2018 full coverage page
while news summarization has been a research topic for decades this new feature is poised to usher in a new line of news summarization techniques
data associated with the news story
there are two fundamental approaches learning the questions from the content of the news story directly
there are two fundamental approaches mining the questions from data
a learning based approach to generate a structured summary of news articles with answer pairs to capture interesting aspects of the news story
a learning based approach to generate a structured summary of news articles with answer pairs to capture salient aspects of the news story
a learning based approach to generate a structured summary of news articles with question pairs to capture salient aspects of the news story
a learning based approach to generate a structured summary of news articles with question pairs to capture interesting aspects of the news story
this paper provides the first study to the best of our knowledge of a learning
specifically this learningbased approach predicts a news article attention map
multiple natural language questions corresponding to each snippet
specifically this learningbased approach reads a news article
specifically this learningbased approach generates multiple natural language questions
furthermore we describe a miningbased approach as the mechanism to generate weak supervision data for training the learning
the learning based approach
we evaluate we approach on the existing squad dataset we constructed
we evaluate we approach on a large dataset with 91k news articles we constructed
we show that we proposed system can achieve an auc of a bleu4 score of 1246 for natural question generation for question summarization beating stateofart baselines
we show that we proposed system can achieve an auc of 0734 for document attention map prediction for question summarization beating stateofart baselines
we show that we proposed system can achieve an auc of a bleu4 score of 244 for question summarization beating stateofart baselineslink prediction has recently been a major focus of knowledge graphs
a major focus of a major focus of knowledge graphs aims at predicting missing links between entities to complement a major focus of knowledge graphs
a major focus of knowledge graphs aims at predicting missing links between entities to complement a major focus of knowledge graphs
the triples provide less information than the paths
most previous works only consider the triples
although some works consider similar entities get similar representations  of the paths some works ignore the order of entities  of the paths
although some works consider similar entities get similar representations  of the paths some works ignore the order of relations  of the paths
although some works consider the semantic information  of the paths some works ignore the order of entities  of the paths
although some works consider the semantic information  of the paths some works ignore the syntactic information  of the paths
although some works consider similar entities get similar representations  of the paths some works ignore the syntactic information  of the paths
although some works consider the semantic information  of the paths some works ignore the order of relations  of the paths
the paths using the word2vec models
in this paper we propose rwlmlm 
in this paper we propose a novel approach for link prediction
rwlmlm consists of a random walk algorithm for a language modelbased link prediction model
rwlmlm consists of a random walk algorithm for a major focus of knowledge graph 
rwlmlm consists of a random walk algorithm for a major focus of kg 
the paths generated by kg
the paths are viewed as pseudosentences for modelbased link prediction model training
rwlmlm can capture the syntactic information in a major focus of knowledge graphs by considering order information of the paths
rwlmlm can capture the semantic information in a major focus of knowledge graphs by considering entities
rwlmlm can capture the semantic information in a major focus of knowledge graphs by considering relations
rwlmlm can capture the syntactic information in a major focus of knowledge graphs by considering relations
rwlmlm can capture the semantic information in a major focus of knowledge graphs by considering order information of the paths
rwlmlm can capture the syntactic information in a major focus of knowledge graphs by considering entities
experimental results show that our method outperforms several stateoftheart models on benchmark datasets
further analysis shows that our model is highly parameter efficientdata offered in the form of knowledge graphs
with the continuously growing amount of data users are often overwhelmed by the amount of potentially relevant information and entities
a problem that becomes more
hence helping users find relevant data is a problem
a problem that becomes more important
skyline queries are typically used in multicriteria decision
multicriteria decision making applications to find a set of objects
objects that are of interest to a user
this type of queries has been extensively studied over relational data in the database community
but only little attention has yet been paid to investigating if the skyline principle can help identifying sets of interesting entities in knowledge graphs
but only little attention has yet been paid to investigating how the skyline principle can help identifying sets of interesting entities in knowledge graphs
in this paper we therefore show how the skyline principle can be applied to rdf knowledge graphs
in this paper we therefore show how the skyline principle can help the user find interesting entities
in particular we present algorithms to process skyline queries
in particular we present a lightweight extension of existing interfaces to process skyline queries
algorithms using commonly used standard interfaces for accessing rdf data
we experiments show that algorithms enable scalable skyline query processing over knowledge graphs
we experiments show that algorithms enable efficient skyline query processing over knowledge graphs
we experiments show that a lightweight extension of a lightweight extension of existing interfaces enable efficient skyline query processing over knowledge graphs
algorithms using commonly used standard interfaces for accessing rdf data
we experiments show that a lightweight extension of a lightweight extension of existing interfaces enable scalable skyline query processing over knowledge graphs
we experiments show that a lightweight extension of existing interfaces enable efficient skyline query processing over knowledge graphs
we experiments show that a lightweight extension of existing interfaces enable scalable skyline query processing over knowledge graphsconjunctive query answering is an important reasoning task for logicbased knowledge representation formalisms such as description logics to query for instance data
instance data that is related in certain ways
full conjunctive query answering for more expressive description logics
any systems that support full conjunctive query
although many knowledge bases use language features of more expressive description logics there are hardly any systems
in fact existing systems compute incomplete results
in fact existing systems usually impose restrictions on the queriesworstcase optimal multiway join these algorithms have recently gained a lot of attention in the database literature
have also been empirically demonstrated to significantly improve query runtimes for relational databases
these algorithms not only offer strong theoretical guarantees of efficiency
have also been empirically demonstrated to significantly improve query runtimes for graph databases
despite these promising theoretical however the semantic web community has yet to adopt such techniques to the best of our knowledge no native rdf database currently supports such join algorithms
despite practical results however the semantic web community has yet to adopt such techniques to the best of our knowledge no native rdf database currently supports such join algorithms
algorithms where in this paper our demonstrate that this should change
sparql queries based on an existing worstcase join this algorithm
we propose a novel procedure for evaluating sparql queries
we propose an adaptation of this algorithm for evaluating sparql queries
we implement this algorithm in apache jena
we then present experiments over the berlin and a novel benchmark that we propose based on wikidata
wikidata that is designed to provide insights into join performance for a more diverse set of basic graph patterns
we then present experiments over watdiv sparql benchmarks and a novel benchmark that we propose based on wikidata
we results show that with this new join algorithm apache jena often runs orders of magnitude faster than the base version blazegraph
we results show that with this new join algorithm apache jena often runs orders of magnitude faster than two other sparql engines virtuoso
we results show that with this new join algorithm apache jena often runs orders of magnitude faster than the base version virtuoso
we results show that with this new join algorithm apache jena often runs orders of magnitude faster than two other sparql engines blazegraphcorrespondences that map between two agents respective ontologies
in the distributed ontology alignment construction problem two agents agree upon a meaningful subset of correspondences
however an agent may be tempted to manipulate the negotiation in favour of a preferred alignment by misrepresenting the weight or confidence of the exchanged correspondences
therefore such an agreement can only be meaningful if the agents can be incentivised to be honest when revealing information
valuations associated with this problem
we examine this problem and model this problem on an edgeweighted bipartite graph where each agent maintains a private set of valuations
we examine this problem and model this problem on an edgeweighted bipartite graph where each side of the graph represents each agents private entities
we examine model as a novel mechanism design problem on an edgeweighted bipartite graph where each agent maintains a private set of valuations
we examine model as a novel mechanism design problem on an edgeweighted bipartite graph where each side of the graph represents each agents private entities
valuations associated with model candidate correspondences
the objective is to find a matching
a matching that maximises the agents social welfare
we study implementations in dominant strategies
we show that implementations in dominant strategies should be solved optimally if truthful mechanisms are required
a decentralised version of the greedy allocation algorithm is then studied with a firstprice payment rule proving tight bounds on the price of stability
a decentralised version of the greedy allocation algorithm is then studied with a firstprice payment rule proving tight bounds on the price of anarchyontologybased knowledge bases like dbpedia usefulness and usability are limited by various quality issues
ontologybased knowledge bases like dbpedia usefulness and usability are limited by various quality issues
ontologybased knowledge bases like dbpedia are very valuable resources
one such issue is the use of string literals instead of semantically typed entities
replacing the literal with an existing entity from the ontologybased knowledge base
a new entity that is typed using classes from the ontologybased knowledge base
in this paper we study the automated canonicalization of such literals
replacing the literal with a new entity
we evaluate this framework against stateoftheart baselines for both semantic typing
we propose a framework
a framework that combines both reasoning learning in order to predict the relevant entities and types
we evaluate this framework against stateoftheart baselines for both entity matching
a framework that combines both machine learning in order to predict the relevant entities and typesentitycentric information resources in the form of huge rdf knowledge graphs have become an important part of todays information systems
but while the integration of independent sources promises rich information independent sources inherent heterogeneity also poses threats to the overall usefulness
to some degree challenges of heterogeneity have been addressed by creating underlying ontological structures
yet our analysis shows that synonymous relationships are still prevalent in current knowledge graphs
in this paper we compare stateoftheart relational learning techniques to analyze the semantics of relationships for unifying synonymous relationships
by embedding relationships into latent feature models we are able to identify relationships
relationships showing the same semantics in a datadriven fashion
the resulting relationship synonyms can be used for knowledge graph consolidation
we evaluate we technique on wikidata we identify hundreds of existing relationship duplicates with very high precision outperforming the current stateoftheart method
we evaluate we technique on dbpedia we identify hundreds of existing relationship duplicates with very high precision outperforming the current stateoftheart method
we evaluate we technique on freebase we identify hundreds of existing relationship duplicates with very high precision outperforming the current stateoftheart methodreusing existing datasets is of considerable significance to developers
reusing existing datasets is of considerable significance to researchers
dataset search engines help a user find relevant datasets for reuse
dataset search engines can present a snippet for each retrieved dataset to explain dataset search engines relevance to the users data needs
this emerging problem of snippet generation for dataset search has not received much research attention
to provide a basis for future research we introduce a framework for quantitatively evaluating the quality of a dataset snippet
the proposed metrics assess the extent to which a snippet matches the query intent
the proposed metrics assess the extent to which a snippet covers the main content of each retrieved dataset to explain its relevance to the users data needs
to establish a baseline we perform an empirical evaluation based on realworld datasets and queries
to establish a baseline we adapt four stateoftheart methods from related fields to we problem
we also conduct a user study to verify we findings
the results demonstrate the effectiveness of we evaluation framework
the results suggest directions for future researchusers who wish to pose expressive queries against such graphs
despite the growing popularity of knowledge graphs for managing diverse data at large scale users are often expected to know how entities of interest are described in the graph
despite the growing popularity of knowledge graphs for managing diverse data at large scale users are often expected to know how to formulate queries in a language such as sparql
an interactive graphbased exploration that allows nonexpert users to simultaneously query knowledge graphs
a language that relaxes these expectations
an interactive graphbased exploration that allows nonexpert users to simultaneously navigate knowledge graphs
in this paper we propose a language the languages operators are based on an interactive graphbased exploration we compare the expressivity of this language with sparql
we then discuss an implementation of this language with sparql that we call rdf explorer such as avoiding interactions
interactions that lead to empty results
we then discuss an implementation of this language with sparql that we discuss various desirable properties explorer has such as avoiding interactions
through a user study over the wikidata knowledgegraph we show that users successfully complete more tasks with rdf explorer than with the existing wikidata query helper while a usability questionnaire demonstrates that users generally prefer our tool and selfreport lower levels of mental effort
through a user study over the wikidata knowledgegraph we show that users successfully complete more tasks with rdf explorer than with the existing wikidata query helper while a usability questionnaire demonstrates that users generally prefer our tool and selfreport lower levels of frustration effortquantities appear in companies with annual revenue of at least 50 mio usd athletes
athletes who ran 200 m faster than 195 s electric cars with range above 400 miles and so on
quantities appear in search queries in numerous forms  athletes
processing such queries requires the understanding of numbers present in the query to capture the contextual information about the queried entities
queries that involve entities
qa systems can handle queries
candidate answers when the specifics of the context of the quantity matter
modern search engines often fail on properly interpreting quantities in queries  quarterly revenue 
candidate answers when the specifics of the search condition  matter
modern search engines can handle queries
queries when the specifics of the search condition  matter
queries when the specifics of the context of the quantity matter
qa systems often fail on properly interpreting quantities in queries  annual 
queries when the specifics of less than above  matter
modern search engines often fail on properly interpreting quantities in candidate answers  annual 
candidate answers when the specifics of the units of interest  seconds miles meters  matter
modern search engines often fail on properly interpreting quantities in candidate answers  quarterly revenue 
qa systems often fail on properly interpreting quantities in candidate answers  quarterly revenue 
qa systems often fail on properly interpreting quantities in candidate answers  annual 
queries that involve types
modern search engines often fail on properly interpreting quantities in queries  annual 
candidate answers when the specifics of less than above  matter
queries when the specifics of the units of interest  seconds miles meters  matter
qa systems often fail on properly interpreting quantities in queries  quarterly revenue 
qsearch
in this paper we present a search system
a qa system called qsearch
in this paper we present a qa system
a search system called qsearch
that can effectively answer advanced queries with quantity conditions
we solution is based on a deep neural network for extracting quantitycentric tuples from text sources
we solution is based on a novel matching model to retrieve answers from other web pages
we solution is based on a novel matching model to rank answers from other web pages
we solution is based on a novel matching model to retrieve answers from news articles
we solution is based on a novel matching model to rank answers from news articles
experiments demonstrate the effectiveness of qsearch on benchmark queries
benchmark queries collected by crowdsourcingrdf has been recently introduced as a w3c recommendation to define constraints
constraints that can be validated against rdf graphs
interactions of shacl with other semantic web technologies such as ontologies is a matter of ongoing research
interactions of shacl with other semantic web technologies such as reasoners is a matter of ongoing research
inference rules expressed in datalog
in this paper we study the interaction of a subset of shacl with inference rules
on the one hand shacl constraints can be used to define a  schema  for graph datasets
on the other hand inference rules can lead to the discovery of new facts
new facts that do not match the original schema
given a set of datalog rules we present a method to detect which constraints could be violated by the application of the inference rules on some graph instance of the original schema and update the original schema the set of shacl constraints in order to capture the new facts
given a set of shacl constraints we present a method to detect which constraints could be violated by the application of the inference rules on some graph instance of the original schema and update the original schema the set of shacl constraints in order to capture the new facts
the new facts that can be inferred
we provide experimental results of the various components of we approach
we provide theoretical results of the various components of we approachknowledge graphs are used in an increasing number of applications
although considerable human effort has been invested into making knowledge graphs available in multiple languages most knowledge graphs are in english
additionally regional facts are often only available in the language of the corresponding region
this lack of multilingual knowledge availability clearly limits the porting of machine learning models to different languages
in this paper we aim to alleviate this drawback by proposing thoth 
in this paper we aim to alleviate this drawback by proposing an approach for translating knowledge graphs
in this paper we aim to alleviate this drawback by proposing an approach for enriching knowledge graphs
thoth extracts bilingual alignments between target knowledge graph
thoth extracts bilingual alignments between a source
thoth extracts learns how to translate from one to the other by relying on two different recurrent neural network models along with knowledge graph embeddings
we evaluated thoth extrinsically by comparing the german dbpedia with the german translation of the english dbpedia on two tasks fact checking and entity
fact checking and entity linking
in addition we ran a manual intrinsic evaluation of the german translation of the english dbpedia
a promising approach which achieves a translation accuracy of 8856 percent
we results show that thoth is a promising approach
moreover we enrichment improves the quality of the german dbpedia significantly as we report  1  for entity
entity linking
moreover we enrichment improves the quality of the german dbpedia significantly as we report 19 percent f  for entity
moreover we enrichment improves the quality of the german dbpedia significantly as we report 184 percent accuracy for fact validationit is a strength of graphbased data formats like rdf that rdf that they are very flexible with representing data are very flexible with representing data
to avoid runtime errors program code might be setvalued
program code that processes highlyflexible data representations exhibits the difficulty that it must always include the most general case in which attributes
to avoid runtime errors program code might be possibly not available
the shapes constraint language has been devised to enforce constraints on otherwise random data structures
type checking using the shapes constraint language 
type checking using shacl 
we present we approach type checking for type checking code
type checking code that queries rdf data graphs validated by a the shapes constraint language shape graph
to this end we derive the shapes constraint language integrate calculus
to this end we derive the shapes constraint language integrate query shapes as types into a  lambda 
to this end we derive the shapes constraint language integrate data shapes as types into a  lambda 
to this end we derive the shapes constraint language shapes from queries
we provide the formal underpinnings
we provide a proof of type safety for shacl
code that will not encounter runtime errors  with usual exceptions as type checking can not prevent accessing empty lists 
a programmer can use we method in order to process rdf data with simplified type checked codelearning knowledge graph embeddings has received increasing attention in recent years
most embedding models in literature interpret relations as bilinear mapping functions to operate on entity embeddings
most embedding models in literature interpret relations as linear mapping functions to operate on entity embeddings
however we find that such relationlevel modeling can not capture the diverse relational structures of kgs well
a novel edgecentric embedding model transedge which contextualizes relation representations in terms of specific headtail entity pairs
in this paper we propose a novel edgecentric embedding model transedge
we refer to such interpret representations of a relation as translations between entity embeddings
we refer to such contextualized representations of a relation as edge embeddings
a novel edgecentric embedding model transedge achieves promising performance on different prediction tasks
transedge which contextualizes relation representations in terms of specific headtail entity pairs
we experiments on benchmark datasets indicate that a novel edgecentric embedding model transedge obtains the stateoftheart results on embeddingbased entity alignment
transedge which contextualizes relation representations in terms of specific headtail entity pairs
transedge which contextualizes relation representations in terms of specific headtail entity pairs
we also show that a novel edgecentric embedding model transedge is complementary with conventional entity alignment methods
moreover a novel edgecentric embedding model transedge shows very competitive performance on link prediction
transedge which contextualizes relation representations in terms of specific headtail entity pairslink keys are recently introduced to formalize data
data interlinking between data sources
link keys are considered as a new kind of correspondences
correspondences included in ontology alignments
we propose a procedure for reasoning in a decentralized manner on a network of ontologies with alignments
alignments containing link keys
in this paper the ontologies are expressed in mathcal alc  while the alignments can contain link key correspondences
link key correspondences equipped with a loose semantics
in this paper the ontologies are expressed in the logic  while the alignments can contain concept
in this paper the ontologies are expressed in the logic  while the alignments can contain link key correspondences
in this paper the ontologies are expressed in mathcal alc  while the alignments can contain individual
in this paper the ontologies are expressed in mathcal alc  while the alignments can contain concept
in this paper the ontologies are expressed in the logic  while the alignments can contain individual
the ontologies involved in such a network
the decentralized aspect of our procedure is based on a process of knowledge propagation through the network via correspondences
a process of knowledge propagation allows to reduce polynomially global reasoning to local reasoningthe vast majority of many methods focus on finding plausible missing facts graph triples in particular
many methods have been proposed to automatically extend knowledge bases
the vast majority of many methods focus on finding plausible knowledge graph triples in particular
in this paper we instead focus on automatically extending ontologies
ontologies that are encoded as a set of existential rules
rules that are plausible
in particular we aim which can not be deduced from the given ontology
in particular we aim is to find rules
to this end we propose a graphbased representation of rule bases
nodes of the correspond to predicates
nodes of the are annotated with the vectors
the considered graphs
the considered graphs
the vectors may be obtained from external resources such as word embeddings could be estimated from the rule base the rule base itself
the vectors may be obtained from external resources such as the vectors could be estimated from the rule base the rule base itself
edges connect predicates that cooccur in the same rule reflect the types of rules in which the predicates cooccur
edges connect predicates that cooccur in the vectors annotations reflect the types of rules in which the predicates cooccur
a representation which is predictive
we then use a neural network model
a neural network model based on graph convolutional networks to refine the initial vector representation of the predicates to obtain a representation
predictive of which rules are plausible
experimental results that demonstrate the strong performance of this method
we present experimental results
