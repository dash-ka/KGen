ontologies are becoming so large in ontologies coverage that a small group of people can develop ontologies effectively
ontologies are becoming so large in ontologies coverage that no single person can develop ontologies effectively
ontologies are becoming so large in ontologies coverage that ontology development becomes a communitybased enterprise
in this paper we discuss annotations of changes
in this paper we discuss requirements for supporting present collaborative protege
in this paper we discuss annotations of ontology components
in this paper we discuss a tool
in this paper we discuss requirements for supporting collaborative ontology development
discussions integrated with ontologyediting process
a tool that supports many of these requirements such as discussions chats
an ongoing largescale biomedical project that actively uses ontologies at the va palo alto healthcare system
we have evaluated collaborative protege in the context of ontology development in an ongoing largescale biomedical project
users have found the new tool effective as an environment for carrying out discussions
users have found the new tool effective as an environment for recording references for design rationale
users have found the new tool effective as an environment for recording references for the information sourcesdomain experts who should lead the process of providing the relevant conceptual knowledge
the process of authoring ontologies requires the active involvement of domain experts
domain experts who should lead the process of authoring ontologies
however most domain experts lack knowledge modelling skills
however most domain experts find it hard to follow logical notations in owl
this paper presents a tool
a controlled natural language called rabbit
a tool that facilitates domain experts definition of ontologies in owl by allowing domain experts to author the ontology in a controlled natural language
this paper presents roo
roo guides users through the ontology construction process by following a methodology geared towards domain experts involvement in ontology authoring
exploiting intelligent user interfaces techniques
an evaluation study has been conducted comparing roo against another popular ontology authoring tool
hydrology modelling scenarios related to real tasks at the mapping agency of great britain
environment modelling scenarios related to real tasks at the mapping agency of great britain
participants were asked to create ontologies based on environment modelling scenarios
participants were asked to create ontologies based on hydrology modelling scenarios
an evaluation study is discussed focusing on the quality of the resultant ontologies
an evaluation study is discussed focusing on the usability and usefulness of the toolnovel techniques which allow for efficient querying of large expressive knowledge bases in secondary storage
we present novel techniques
grounded conjunctive query answering over owldl ontologies is intractable in the worst case
in particular we show that we can effectively answer grounded conjunctive queries without building a full completion forest for a large abox  unlike state of the art tableau reasoners 
instead we rely on the completion forest of a dramatically reduced summary of a large abox
we demonstrate the effectiveness of this approach in aboxes with up to 45 million assertionswe discuss certain nontrivial consequences of the datatype system of owl such as the extensibility of the set of supported datatypes and complexity of reasoning
we analyze the datatype system of owl
we analyze the datatype system of owl 2
we discuss certain nontrivial consequences of the datatype system of owl 2 definition such as the extensibility of the set of supported datatypes and complexity of reasoning
we also argue that certain datatypes from the list of normative datatypes in the current owl 2 are inappropriate
the current owl 2 working draft
we also argue that certain datatypes from the list of normative datatypes in the current owl 2 should be replaced with different ones
finally we present an algorithm for datatype reasoning
an algorithm for datatype reasoning is modular in the sense that an algorithm for datatype reasoning can handle any datatype
any datatype that supports certain basic operations
we show how to implement certain basic operations for string datatypes
we show how to implement certain basic operations for number datatypesintegration of heterogeneous services is often hardwired in workflow implementations
integration of heterogeneous services is often hardwired in service implementations
in this paper we define an execution model operating on semantic descriptions of services
services allowing flexible integration of services with solving process conflicts where necessary
services allowing flexible integration of services with solving data where necessary
we implement the model
the model using we wsmo a case scenario from the b2b domain of the sws challenge
the model using we wsmo technology from the b2b domain of the sws challengedle  owl reasoning defining a framework
the mixed dl defining a framework
entailmentbased  owl reasoning defining a framework
a framework inspired from homogeneous paradigms for integration of ontologies
we introduce the notion of dle  owl reasoning
a framework inspired from the hybrid for integration of ontologies
we introduce the notion of entailmentbased  owl reasoning
we introduce the notion of the mixed dl
a framework inspired from homogeneous paradigms for integration of rules
a framework inspired from the hybrid for integration of rules
the idea is to combine the tbox inferencing capabilities of the dl algorithms over large aboxes
the idea is to combine the scalability of the rule paradigm over large aboxes
towards this end we define the dle framework
the dle framework enhances the entailmentbased owl reasoning paradigm in two directions
firstly the dle framework disengages the manipulation of the tbox semantics from any incomplete entailmentbased approach using the efficient dl algorithms
secondly the dle framework achieves faster application of the aboxrelated entailments comparing the dle framework to the conventional entailmentbased approaches due to the domainspecific nature of the entailments
secondly the dle framework achieves faster application of efficient memory usage comparing the dle framework to the conventional entailmentbased approaches due to the low complexity
secondly the dle framework achieves faster application of the aboxrelated entailments comparing the dle framework to the conventional entailmentbased approaches due to the low complexity
secondly the dle framework achieves faster application of efficient memory usage comparing the dle framework to the conventional entailmentbased approaches due to the domainspecific nature of the entailmentsthe profile based service signature matching
various semantic web service discovery techniques have been proposed many of which perform the profile
however the service service signature concepts are not sufficient to discover web services accurately
this paper presents a new method to enhance the semantic description of semantic web service by using the semantic constraints of service service signature concepts in specific context
the semantic constraints are extracted automatically from the parsing results of the service description text by a set of heuristic rules
the semantic constraints described in a constraint graph
a constraint graph based matchmaking algorithm
the corresponding semantic web service matchmaker performs not only the profiles semantic matching with the help of a constraint graph
the corresponding semantic web service matchmaker performs the matching of the corresponding semantic web service matchmaker semantic constraints with the help of a constraint graph
the experiment results are encouraging when applying the semantic constraint to discover semantic web services on the service retrieval test collection owlstc v2the ontology that is sufficient for an entailment in an owl ontology to hold
a justification for an entailment in an owl ontology is a minimal subset of the ontology
since justifications respect the syntactic form of axioms in an ontology syntactically nor semantically minimal
since justifications respect the syntactic form of axioms in they are usually neither syntactically nor semantically minimal
this paper presents two new subclasses of justifications laconic justifications
this paper presents two new subclasses of justifications precise justifications
axioms that do not contain any superfluous  parts
laconic justifications only consist of axioms 
precise justifications can be derived from laconic justifications
precise justifications are characterised by the fact that precise justifications consist of flat small axioms
flat small axioms which facilitate the generation of semantically minimal repairs
formal definitions for both types of justification are presented
in contrast to previous work in this area formal definitions for both types of justification make it clear as to what exactly  parts of axioms  are
in order to demonstrate the practicability of computing laconic justifications an algorithm results from an empirical evaluation carried out on several published ontologies are presented
in order to demonstrate the practicability of computing hence precise justifications an algorithm results from an empirical evaluation carried out on several published ontologies are presented
in order to demonstrate the practicability of computing laconic justifications an algorithm is provided carried out on several published ontologies are presented
in order to demonstrate the practicability of computing hence precise justifications an algorithm is provided carried out on several published ontologies are presented
ontologies that vary in complexity
an empirical evaluation showed that laconicprecise justifications can be computed in a reasonable time for entailments in a range of ontologies
ontologies that vary in size
entailments that had more laconicprecise justifications than regular justifications
it was found that in half of the ontologies sampled there were entailments
more surprisingly it was observed that for some ontologies there were fewer laconic justifications than regular justificationsfinding mappings between compatible ontologies is an important open problem
finding mappings between compatible ontologies is an difficult open problem
instancebased methods for solving this problem reflect concept semantics as the most active parts of the ontologies are actually being used
instancebased methods for solving this problem have the advantage of focusing on the most active parts of the ontologies
however such methods have not at present been widely investigated in ontology mapping compared to structural techniques
however such methods have not at present been widely investigated in ontology mapping compared to linguistic techniques
furthermore previous instancebased mapping techniques were only applicable to cases vocabularies
furthermore previous instancebased mapping techniques were only applicable to cases with
cases where a substantial set of instances was available that was doubly annotated
in this paper we approach the mapping problem as a classification problem based on the similarity between instances of concepts
this has the advantage that no doubly annotated instances are required so that the method can be applied to any two corpora annotated with two own vocabularies
we evaluate the resulting classifiers on two realworld use cases
we evaluate the resulting classifiers on one with homogeneous
we evaluate the resulting classifiers on one with heterogeneous instances
the results illustrate the efficiency and generality of this methodthose found in health care
a modelling paradigm that is especially well suited for developing models of large structurally complex domains such as those
the web ontology language provides a modelling paradigm
those found in the life sciences
the web ontology languages declarative nature combined with powerful reasoning tools has effectively supported the development of disease
the web ontology languages declarative nature combined with powerful reasoning tools has effectively supported the development of clinical ontologies
the web ontology languages declarative nature combined with powerful reasoning tools has effectively supported the development of very large and complex anatomy
the web ontology language however is not a programming language so using these models in applications necessitates both a technical means of integrating the web ontology language models with programs in knowing how to integrate the web ontology language models
the web ontology language however is not a programming language so using these models in applications necessitates both a technical means of integrating the web ontology language models with considerable methodological sophistication in knowing how to integrate the web ontology language models
in this paper we present an analytical framework for evaluating various owljava combination approaches
hybrid modelling building models in which part of the model exists
hybrid modelling that is
hybrid modelling building models in which part of the model is developed directly in the web ontology language
we have developed a software framework for what we call hybrid modelling
hybrid modelling building models in which part of the model is developed directly in java
hybrid modelling building models in which part of the model exists
we analyse the advantages and disadvantages of hybrid modelling both by means of a case study of a large medical records system
we analyse the advantages and disadvantages of hybrid modelling both in comparison to other approachesthe semantic desktop is a means to support users in personal information management
using the open source software prototype gnowsis we evaluated the approach in a two month case study in 2006 with eight participants
two participants continued using the open source software prototype
two participants were interviewed after two years in 2008 to show two participants longterm usage patterns
this allows us to analyse how the system was used for personal information management
contextual interviews gave insights on behaviour while questionnaires did not
contextual interviews gave insights on behaviour while event logging did not
we discovered that the personal semantic wiki was used creatively to note information
we discovered that in the personal environment simple haspart are sufficient for users to file
we discovered that in the personal environment simple haspart are sufficient for refind information
we discovered that in the personal environment isrelated relations are sufficient for users to file
we discovered that in the personal environment isrelated relations are sufficient for refind informationthis paper studies the expressive power of sparql
the main result is that sparql have equivalent expressive power
hence by classical results sparql is equivalent from an expressiveness point of view to relational algebra
the main result is that nonrecursive safe datalog with negation have equivalent expressive power
we present explicit generic rules of the transformations in both directions
among other findings of this paper are the proof that negation can be simulated in sparql that nonsafe filters are superfluous
among other findings of this paper are the proof that current sparql w3c semantics can be simplified to a standard compositional onein this paper we present the proposed framework
ontology modules that can access the knowledge bases of the others through the knowledge bases of the others
within the context of the proposed framework an ontology can be defined and developed as a set of ontology modules welldefined interfaces
an important implication of the proposed framework is that ontology modules can be developed completely independent of each others signature and language
such modules are free to only utilize the required knowledge segments of the others
the interfacebased modular ontology formalism which theoretically supports the proposed framework
we describe the interfacebased modular ontology formalism distinctive features compared to the exiting modular ontology formalisms
the interfacebased modular ontology formalism which theoretically present the proposed framework
we also describe the realworld design and implementation of the proposed framework for modifying the swoop interfaces and reasoners
we also describe the realworld design and implementation of the proposed framework for creating modular ontologies by extending owldlwe introduce extelp as a decidable fragment of the semantic web rule language
the semantic web rule language that admits reasoning in polynomial time
extelp encompasses an extended notion of the recently proposed dl rules for that logic
extelp is based on the tractable description logic el
features introduced by the forthcoming owl 2 such as local reflexivity
features introduced by the forthcoming owl 2 such as certain range restrictions
thus extelp extends el with a number of features
features introduced by the forthcoming owl 2 such as disjoint roles
features introduced by the forthcoming owl 2 such as the universal role
we present a reasoning algorithm based on a translation of extelp to datalog
a translation of extelp to datalog also enables the seamless integration of dlsafe rules into extelp
the description logic programming fragment of owl 2
while reasoning with dlsafe rules as such is already highly intractable we show that dlsafe rules can be admitted in extelp without losing tractability
dlsafe rules based on the description logicparametric languageindependent kernel functions defined for individuals within ontologies
a novel family of parametric languageindependent kernel functions is presented
linear classifiers that offer an alternative way to perform classification wrt deductive reasoning
individuals within ontologies are easily integrated with efficient statistical learning methods for inducing linear classifiers
a method for adapting the parameters of the kernel to the knowledge base through stochastic optimization is also proposed
tasks where an inductive approach may bridge the gaps of the standard methods due the inherent incompleteness of the knowledge bases
this enables the exploitation of statistical learning in a variety of tasks
approximate query answering with real ontologies
in this work a system has been tested in experiments on approximate query
real ontologies collected from standard repositories
a system integrating the kernelswe study the problem of distributed rdfs reasoning on top of distributed hash tables
we study the problem of query answering on top of distributed hash tables
scalable distributed rdfs reasoning
scalable is an essential functionality for providing the scalability and performance that largescale semantic web applications require
we goal in this paper is to compare two wellknown approaches to rdfs reasoning forward chaining on top of distributed hash tables
we goal in this paper is to evaluate two wellknown approaches to rdfs reasoning forward chaining on top of distributed hash tables
we goal in this paper is to evaluate two wellknown approaches to rdfs reasoning namely backward on top of distributed hash tables
we goal in this paper is to compare two wellknown approaches to rdfs reasoning namely backward on top of distributed hash tables
we show how to prove bamboo correctness
we show how to implement both the algorithms
we also study the timespace tradeoff experimentally by evaluating we algorithms on planetlab
we also study the timespace tradeoff exhibited by the algorithms analyticallyontologies which considers especially the object properties between the concepts
this paper presents a new semantic relatedness measure on ontologies
our approach relies on two hypotheses
firstly using only concept hierarchy only a few paths can be considered as  semantically corrects  and these paths obey to a given set of rules
firstly using only object properties only a few paths can be considered as  semantically corrects  and these paths obey to a given set of rules
a cost  represented as a weight  which depends on a given edge in isa partof 
a cost  represented as a weight  which depends on a given edge in a path position in a path
a cost  represented as a weight  which depends on a given edge in a path type 
a cost  represented as a weight  which depends on a given edge in a path context in the ontology
secondly following a given edge in a path has a cost  represented as a weight 
the lexical base wordnet using partof relation with two different benchmarks
we propose an evaluation of we measure on the lexical base wordnet
we show that in this context our measure outperforms the classical semantic measuresan approach to improve an rccderived geospatial approximation is presented which makes use of concept inclusion axioms in owl
consistency checking provided by a knowledge representation system based on description logics
the algorithm combines hypothesis testing with consistency checking
the algorithm used to control the approximation
the associated tbox when compared to baseline abox and tbox
propositions about the consistency of the refined abox wrt the associated tbox are made
formal proves of the divergent consistency results when checking either of both are provided
a roughly tenfold improved approximation when using the refined abox and tbox
the application of the approach to a geospatial setting results in a roughly tenfold improved approximation
ways to automate the detection of falsely calculated relations are discussed
ways to further improve the approximation are discussedwe propose a novel method for reasoning in the description logic shiq
an ordered binary decision diagram which represents a canonical model for t
after a satisfiability is converted into an ordered binary decision diagram
a satisfiability preserving transformation from shiq to the description logic alcib the obtained alcib tbox t
a disjunctive datalog program that can be used for abox reasoning
this an is turned into a disjunctive datalog program
this an ordered binary decision diagram
the algorithm admits easy extensions with dlsafe rules
the algorithm is worstcase optimal wrt data complexity
the algorithm admits easy extensions with ground conjunctive queriescontrolled language for ontology editing tools offer an attractive alternative for naive users
time learning the correct syntactic structures and vocabulary in order to use the controlled language properly
ontology editing tools are still required to spend time
naive users wishing to create ontologies
this paper extends previous work
previous work which uses standard nlp tools to process the controlled language
previous work which uses standard nlp tools to manipulate an ontology
here we also generate text in the language from an existing ontology  nlg 
an existing ontology using or shallow  natural language generation
an existing ontology using templatebased  natural language generation
the text generator combine to form a roundtrip ontology authoring environment one can start with an existing imported ontology modify the text as then turn the text back into the ontology in the previous work environment
the previous work authoring process combine to form a roundtrip ontology authoring environment one can start with an existing imported ontology modify the text as required
the text generator combine to form a roundtrip ontology authoring environment one can start with an existing imported ontology edit the text as then turn the text back into the ontology in the previous work environment
the previous work authoring process combine to form a roundtrip ontology authoring environment one can start with one originally produced using previous work  re  produce the modify the text as then turn the text back into the ontology in the previous work environment
the previous work authoring process combine to form a roundtrip ontology authoring environment one can start with one originally produced using previous work  re  produce the edit the text as then turn the text back into the ontology in the previous work environment
the previous work authoring process combine to form a roundtrip ontology authoring environment one can start with one originally produced using previous work  re  produce the modify the text as required
the text generator combine to form a roundtrip ontology authoring environment one can start with an existing imported ontology modify the text as required
the previous work authoring process combine to form a roundtrip ontology authoring environment one can start with one originally produced using previous work  re  produce the edit the text as required
the controlled language
the text generator combine to form a roundtrip ontology authoring environment one can start with an existing imported ontology edit the text as required
the text generator combine to form a roundtrip ontology authoring environment one can start with one originally produced using previous work  re  produce the edit the text as required
the text generator combine to form a roundtrip ontology authoring environment one can start with one originally produced using previous work  re  produce the modify the text as required
the text generator combine to form a roundtrip ontology authoring environment one can start with one originally produced using previous work  re  produce the modify the text as then turn the text back into the ontology in the previous work environment
the text generator combine to form a roundtrip ontology authoring environment one can start with one originally produced using previous work  re  produce the edit the text as then turn the text back into the ontology in the previous work environment
the previous work authoring process combine to form a roundtrip ontology authoring environment one can start with an existing imported ontology edit the text as then turn the text back into the ontology in the previous work environment
the previous work authoring process combine to form a roundtrip ontology authoring environment one can start with an existing imported ontology modify the text as then turn the text back into the ontology in the previous work environment
the previous work authoring process combine to form a roundtrip ontology authoring environment one can start with an existing imported ontology edit the text as required
an evaluation comparing the roundtrip ontology authoring process with a wellknown ontology editor
this learning curve for users
building on previous methodology we undertook an evaluation where previous work required a language reference manual with several examples in order to use the controlled language the use of nlg improves on existing results for basic ontology editing tasks
building on previous methodology we undertook an evaluation where previous work required a language reference manual with several examples in order to use the controlled language the use of nlg reduces thiscurrently proposed semantic web services technologies allow the creation of ontologybased semantic annotations of web services so that software agents are able to discover invoke compose web services with a high degree of automation
currently proposed semantic web services technologies allow the creation of ontologybased semantic annotations of web services so that software agents are able to discover invoke monitor web services with a high degree of automation
the owl services ontology is an upper ontology in owl language providing essential vocabularies to semantically describe web services
currently owl services services can only be developed independently if one service is unavailable then finding a suitable alternative would require an difficult global searchmatch
currently owl services services can only be developed independently if one service is unavailable then finding a suitable alternative would require an expensive global searchmatch
substitution tracing as well as incremental development
substitution tracing as well as reuse of services
a new owl services construct that can systematically support substitution
it is desirable to have a new owl services construct
introducing inheritance relationship into owl services is a natural solution
however most of the other currently discussed formalisms for semantic web services such as wsmo has yet to define human organization of services into a taxonomylike structure
however most of the other currently discussed formalisms for semantic web services such as sawsdl has yet to define a selfcontained mechanism of establishing inheritance relationships among services
however owl services has yet to define a selfcontained mechanism of establishing inheritance relationships among services
however owl services has yet to define a concrete mechanism of establishing inheritance relationships among services
services which we believe is very important for discovery of web services
however owl services has yet to define human organization of services into a taxonomylike structure
however most of the other currently discussed formalisms for semantic web services such as sawsdl has yet to define human organization of services into a taxonomylike structure
however most of the other currently discussed formalisms for semantic web services such as wsmo has yet to define a selfcontained mechanism of establishing inheritance relationships among services
however most of the other currently discussed formalisms for semantic web services such as wsmo has yet to define a concrete mechanism of establishing inheritance relationships among services
services which we believe is very important for the automated annotation
however most of the other currently discussed formalisms for semantic web services such as sawsdl has yet to define a concrete mechanism of establishing inheritance relationships among services
in this paper we extend owl services with the ability to define inheritance relationships between services
in this paper we extend owl services with the ability to maintain inheritance relationships between services
through the definition of an additional  inheritance profile  inheritance relationships can be stated about
through the definition of an additional  inheritance profile  inheritance relationships can be reasoned about
two types of irs are allowed to grant service developers the choice to respect the  contract  between not
two types of irs are allowed to grant service developers the choice to respect the  contract  between services
the prototype will be briefly evaluated as well
the proposed inheritance framework has also been implementedrevision of a description logicbased ontology deals with the problem of incorporating newly received information consistently
in this paper we propose a general operator for revising terminologies in description logicbased ontologies
a general operator relies on a reformulation of the kernel contraction operator in belief revision
we first define we revision operator for terminologies
we first show that our revision operator satisfies some desirable logical properties
second these two algorithms are developed to instantiate a general operator
since in general these two algorithms are computationally too hard we propose a third algorithm as a more efficient alternative
we provide evaluation results on these two algorithms efficiency in the context of two application scenarios incremental ontology learning
we provide evaluation results on these two algorithms effectiveness in the context of two application scenarios incremental ontology learning
we provide evaluation results on these two algorithms effectiveness in the context of two application scenarios incremental mapping revision
we provide evaluation results on these two algorithms efficiency in the context of two application scenarios incremental mapping revision
we implemented these two algorithms
we provide evaluation results on these two algorithms meaningfulness in the context of two application scenarios incremental mapping revision
we provide evaluation results on these two algorithms meaningfulness in the context of two application scenarios incremental ontology learningnavigational features have been largely recognized as fundamental for graph database query languages
this fact has motivated several authors to propose rdf query languages with navigational capabilities
in particular
we have argued in a previous paper that nested regular expressions are appropriate to navigate rdf data
we have proposed the nsparql query language for rdf
rdf that uses nested regular expressions as building blocks
in a previous paper that nested regular expressions are appropriate to navigate rdf data we study some of the fundamental properties of nsparql concerning complexity of evaluation
in a previous paper that nested regular expressions are appropriate to navigate rdf data we study some of the fundamental properties of nsparql concerning expressiveness of evaluation
regarding expressiveness we show that nsparql is expressive enough to answer queries
queries considering the semantics of the rdfs vocabulary by directly traversing the input graph
we study the expressiveness of the combination of nested regular expressions
we study the expressiveness of the combination of sparql operators
we also show that nesting is necessary to obtain this last result
regarding complexity of evaluation we prove that the evaluation of a nested regular expression e over an rdf graph g can be computed in time omore ontologies have been published widely on the web
more ontologies have been published widely on the web
more ontologies have been used widely on the web
more ontologies have been used widely on the web
in order to make good use of an ontology especially a complex ontology we need methods to help understand methods first
in order to make good use of an ontology especially a new ontology we need methods to help understand methods first
identifying potentially important concepts and relations in an ontology is an challenging method
identifying potentially important concepts and relations in an ontology is an intuitive method
in this paper we first define four features for potentially important concepts and relation from the ontological structural point of view
then a simple yet effective conceptandrelationranking algorithm is proposed to simultaneously rank the importance of concepts
then a simple yet effective conceptandrelationranking algorithm is proposed to simultaneously rank the importance of relations
different from the importance of concepts reinforce one another in simple yet effective conceptandrelationranking in an iterative manner
different from the weights of relations reinforce one another in simple yet effective conceptandrelationranking in an iterative manner
different from the traditional ranking methods reinforce one another in simple yet effective conceptandrelationranking in an iterative manner
such an iterative process is proved to be convergent both by experiments
such an iterative process is proved to be convergent both in principle
we experimental results show that simple yet effective conceptandrelationranking has a similar convergent speed as a more reasonable ranking result
we experimental results show that simple yet effective conceptandrelationranking has a similar convergent speed as the pageranklike algorithmsefficient rdf data management is one of the cornerstones in realizing the semantic web vision
in the past different rdf storage strategies have been proposed ranging from simple triple stores to more advanced techniques like clustering
in the past different rdf storage strategies have been proposed ranging from simple triple stores to vertical partitioning on the predicates
we present an experimental comparison of existing storage strategies on top of the sp2bench sparql performance benchmark suite
we put the results into context by comparing the results to a purely relational model of the benchmark scenario
we observe that in terms of scalability a simple triple store is competitive to the vertically partitioned approach scenario with realworld queries none of the approaches scales to documents containing tens of millions of rdf triples
we observe that in terms of performance a simple triple store is competitive to the vertically partitioned approach scenario with realworld queries none of the approaches scales to documents containing tens of millions of rdf triples
we observe that none of the approaches can compete with a purely relational model
a simple triple store built on top of a columnstore dbms
approach when choosing a physical  sort order in we
approach when choosing a predicate subject object  sort order in we
we conclude that future research is necessary to further bring forward rdf data managementcombining multiple ontologies on the web is bound to lead to inconsistencies between the combined vocabularies
reusing multiple ontologies on the web is bound to lead to inconsistencies between the combined vocabularies
the ontologies that are in use today implicit knowledge
the ontologies that are in use today
even many of the ontologies turn out to be inconsistent once some of even many of the ontologies is made explicit
current semantic web reasoning systems which are typically based on classical logic
however robust methods to deal with inconsistencies are lacking from current semantic web reasoning systems
however efficient methods to deal with inconsistencies are lacking from current semantic web reasoning systems
in earlier papers we have proposed the use of syntactic relevance functions as a method for reasoning with inconsistent ontologies
in this paper we extend that work to the use of semantic distances
we show how google distances can be used to develop semantic relevance functions to reason with inconsistent ontologies
the implicit knowledge hidden in the web for explicit reasoning purposes
in essence we are using the implicit knowledge
we have implemented this approach as part of the pion reasoning system
we report on experiments with several realistic ontologies
the test results show that a mixed syntacticsemantic approach can significantly improve reasoning performance over the purely syntactic approach
furthermore we methods allow to tradeoff computational cost for inferential completeness
we experiment shows that we only have to give up a little quality to obtain a high performance gaina more dynamic and interactive web where individuals can share resources
a more dynamic and interactive web where individuals can organise resources
the continued increase in web usage in particular participation in folksonomies reveals a trend towards a more dynamic and interactive web
tagging has emerged as the defacto standard for the organisation of such resources providing a reactive knowledge management mechanism that users find easy to use
tagging has emerged as the defacto standard for the organisation of such resources providing a versatile knowledge management mechanism that users find easy to understand
tagging has emerged as the defacto standard for the organisation of such resources providing a reactive knowledge management mechanism that users find easy to understand
tagging has emerged as the defacto standard for the organisation of such resources providing a versatile knowledge management mechanism that users find easy to use
it is common nowadays for users to have multiple profiles in various folksonomies thus distributing users tagging activities
in this paper
subsequent semantic modelling of two popular social networking sites interests utilising wikipedia as a multidomain model
we present a method for the automatic consolidation of user profiles across two popular social networking sites
subsequent semantic modelling of two popular social networking sites interests
domains the knowledge acquired
we evaluate how much can be learned from such sites
we evaluate in which domains the knowledge is focussed
users when multiple tagclouds are combined
results show that far richer interest profiles can be generated for userswe present redgraph 
we present the first generic virtual reality visualization program for semantic web data
the first generic virtual reality visualization program for semantic web data is capable of handling large datasets as we demonstrate on social network data from the yous
redgraph is capable of handling large datasets as we demonstrate on social network data from the yous
patent trade office
we develop a semantic web vocabulary of virtual reality terms compatible with graphxml to map graph visualization into the semantic web the semantic web
we approach to visualizing semantic web data takes advantage of userinteraction in an immersive environment to bypass a number of difficult issues in 3dimensional graph visualization layout by relying on users users themselves to interactively extrude the nodes and links of a 2dimensional graph into the third dimension
data formatted according to social network data schema or ontology
when users touch nodes in the virtual reality environment users retrieve data
social network data constructed from inventors from trademark office in order to explore networks of innovation in computing
social network data constructed from institutions from trademark office in order to explore networks of innovation in computing
we applied redgraph to social network data
social network data constructed from institutions from the united states patent in order to explore networks of innovation in computing
social network data constructed from patents from trademark office in order to explore networks of innovation in computing
social network data constructed from inventors from the united states patent in order to explore networks of innovation in computing
social network data constructed from patents from the united states patent in order to explore networks of innovation in computing
we applied the first generic virtual reality visualization program for semantic web data to social network data
a user study comparing extrusion vs noextrusion
using this dataset results of a user study are presented
a user study comparing extrusion vs noextrusion
a user study comparing extrusion vs noextrusion
a user study comparing extrusion vs noextrusion
no significant difference was found for broad questions about the overall structure of social network data
a user study comparing extrusion vs noextrusion
a user study showed the use of a extrusion interface by subjects led to significant improvement on answering of finegrained questions about this dataset
furthermore inference can be used to improve the visualization as demonstrated with a dataset of biotechnology patents and researchersthe growing popularity of social tagging systems promises to alleviate the knowledge bottleneck since social tagging systems allow ordinary users to create knowledge in a simple cheap representation usually known as folksonomy
the growing popularity of social tagging systems promises to alleviate the knowledge bottleneck since social tagging systems allow ordinary users to share knowledge in a simple cheap representation usually known as folksonomy
the growing popularity of social tagging systems promises to alleviate the knowledge bottleneck since social tagging systems allow ordinary users to share knowledge in a simple scalable representation usually known as folksonomy
the growing popularity of social tagging systems promises to alleviate the knowledge bottleneck since social tagging systems allow ordinary users to create knowledge in a simple scalable representation usually known as folksonomy
the knowledge bottleneck that slows down the full materialization of the semantic web
however for the sake of knowledge workflow one needs to find a compromise between the controlled vocabulary of domain experts
however for the sake of knowledge workflow one needs to find a compromise between the uncontrolled nature of folksonomies
however for the sake of knowledge workflow one needs to find a compromise between the more systematic vocabulary of domain experts
in this paper we propose to address this concern by introducing a novel algorithm based on frequent itemset mining techniques to efficiently learn an ontology over the enriched folksonomy
in this paper we propose to address this concern by devising a method to efficiently learn an ontology over the enriched folksonomy
a method that automatically enriches a folksonomy with domain expert knowledge
in order to quantitatively assess our method we propose a new benchmark for taskbased ontology evaluation where the quality of the ontologies is measured based on how helpful the ontologies are for the task of personalized information finding
we conduct experiments on real data
we show the effectiveness of we approachthe semantic web is a distributed environment for knowledge representation and reasoning
the distributed nature brings with the semantic web sources between autonomous knowledge bases
the distributed nature brings with the semantic web inconsistencies between autonomous knowledge bases
the semantic web failing data
problems resulting from unavailable sources
to reduce problems caching can be used
to improve performance caching can be used
caches however raise new problems of outdated information
caches however raise new problems of imprecise information
we propose to distinguish between certain information when reasoning on the semantic web by extending the well known four bilattice of knowledge orders to fourc taking into account cached information
we propose to distinguish between cached information when reasoning on the semantic web by extending the well known four bilattice of knowledge orders to fourc taking into account cached information
we propose to distinguish between certain information when reasoning on the semantic web by extending the well known four bilattice of truth orders to fourc taking into account cached information
we propose to distinguish between cached information when reasoning on the semantic web by extending the well known four bilattice of truth orders to fourc taking into account cached information
we discuss how users can be offered additional information about the reliability of inferred information based on the availability of the corresponding information sources
the framework towards fourt allowing for multiple levels of trust on data sources
we then extend the framework towards fourt
in this extended setting knowledge about trust in information sources can be used to compute how well an inferred statement can be trusted
inconsistencies arising from connecting multiple data sources
in this extended setting knowledge about trust in information sources can be used to resolve inconsistencies
we redefine the stable model
trust based reasoning
we founded semantics on the basis of fourt
reformalize the web ontology language owl2 based on logical bilattices to augment owl knowledge bases with trusthumandefined concepts are fundamental buildingblocks in constructing knowledge bases such as ontologies
concept definition driven by data
statistical learning techniques provide an alternative automated approach to concept definition
concept definition driven by prior knowledge
in this paper we propose a probabilistic modeling framework
a probabilistic modeling framework that combines both datadriven topics in a principled manner
a probabilistic modeling framework that combines both humandefined concepts in a principled manner
the methodology we propose is based on applications of statistical topic models  also known as latent dirichlet allocation models 
we demonstrate the utility of this general framework in two ways
we first illustrate how the methodology we propose can be used to automatically tag web pages with concepts from a known set of concepts without any need for labeled documents
we then perform a series of experiments
experiments that quantify how combining humandefined semantic knowledge with datadriven techniques leads to better language models than can be obtained with either alonea large amount of terms  have been published on the semantic web by various parties to be shared for describing resources
a large amount of properties  have been published on the semantic web by various parties to be shared for describing resources
a large amount of classes  have been published on the semantic web by various parties to be shared for describing resources
a directed dependence relation is formed
terms are defined based on other terms
the study of term dependence is important for many other tasks such as integration
the study of term dependence is important for many other tasks such as distributed reasoning on the web scale
the study of term dependence is important for many other tasks such as ontology maintenance
the study of term dependence is a foundation work
in this paper we analyze the complex network characteristics of the term dependence graph
in this paper we analyze the complex network characteristics of the induced vocabulary dependence graph
the graphs analyzed in the experiments
the graphs are constructed from a large data set that contains 1278233 terms in 3039 vocabularies
the results characterize reachability
the results characterize connectivity
the results characterize degree distributions
the results characterize the current statuswe present a technique for answering queries over rdf data through an evolutionary search algorithm using bloom filters for rapid approximate evaluation of generated solutions
we present a technique for answering queries over rdf data through an evolutionary search algorithm using fingerprinting for rapid approximate evaluation of generated solutions
we evolutionary approach has several advantages compared to traditional databasestyle query answering
first the result quality converges with offering  anytime  behaviour with arbitrary tradeoff between computation time in addition the level of approximation can be tuned by varying the size of the bloom filters
first the result quality converges with offering  anytime  behaviour with arbitrary tradeoff between query results in addition the level of approximation can be tuned by varying the size of the bloom filters
first the result quality converges with each evolution with arbitrary tradeoff between query results in addition the level of approximation can be tuned by varying the size of the bloom filters
first the result quality increases monotonically in addition the level of approximation can be tuned by varying the size of the bloom filters
first the result quality converges with each evolution with arbitrary tradeoff between computation time in addition the level of approximation can be tuned by varying the size of the bloom filters
secondly through bloom filter compression we can fit large graphs in main memory reducing the need for disk io during query evaluation
finally since the individuals evolve independently parallel execution is straightforward
prototype that show initial results over large datasets
we present we prototype
prototype that evaluates basic sparql queries over arbitrary rdf graphsthere are many reasons for measuring a distance between ontologies
in particular it is useful to know quickly if two ontologies are close or remote before deciding to match two ontologies
to that extent a distance between ontologies must be quickly computable
constraints applying to several possible ontology distances
constraints applying to such measures
we present constraints
then we evaluate experimentally some of several possible ontology distances in order to assess some of speed
then we evaluate experimentally some of such measures in order to assess some of them accuracy
then we evaluate experimentally some of such measures in order to assess some of speed
then we evaluate experimentally some of several possible ontology distances in order to assess some of them accuracycollaborative tagging systems have nowadays become important data sources for populating semantic web applications
for tasks like synonym detection and discovery of concept hierarchies many researchers introduced these measures
even though most of these measures appear very natural most of these measures design often seems to be rather ad hoc
the underlying assumptions on the notion of similarity are not made explicit
validation of tag similarity in terms of formal representations of knowledge is still lacking
a more systematic characterization is still lacking
here we address this issue each measure is computed on data from the social bookmarking system delicious
here we analyze several measures of tag similarity each measure is computed on data from the social bookmarking system delicious
the mapped tags
here we address this issue a semantic grounding is provided by mapping pairs of similar tags in the folksonomy to pairs of synsets in wordnet where we use validated measures of semantic distance to characterize the semantic relation between the
here we analyze several measures of tag similarity a semantic grounding is provided by mapping pairs of similar tags in the folksonomy to pairs of synsets in wordnet where we use validated measures of semantic distance to characterize the semantic relation between the
this exposes important features of the investigated similarity measures
this indicates which ones are better suited in the context of a given semantic applicationefficient discovery mechanisms are critical for enabling serviceoriented architectures on the semantic web
scalable discovery mechanisms are critical for enabling serviceoriented architectures on the semantic web
the majority of currently existing approaches focuses on centralized architectures typically by precomputing concepts
the majority of currently existing approaches focuses on deals with efficiency typically by precomputing concepts
the majority of currently existing approaches focuses on centralized architectures typically by storing the results of the semantic matcher for all possible query concepts
the majority of currently existing approaches focuses on deals with efficiency typically by storing the results of the semantic matcher for all possible query concepts
such approaches however fail to scale with respect to the number of service advertisements
such approaches however fail to scale with respect to the size of the ontologies
the ontologies involved
on the other hand this paper presents an efficient indexbased method for the semantic web service discovery
the semantic web service discovery that is suitable for both p2p environments
the semantic web service discovery that is suitable for both centralized environments
on the other hand this paper presents an scalable indexbased method for the semantic web service discovery
the semantic web service discovery that allows for fast selection of services at query time
we employ a novel encoding of the service descriptions allowing the match between a request and an advertisement to be evaluated in constant time and we index these representations to prune the search space reducing the number of comparisons
comparisons required
given a desired ranking function the search algorithm can retrieve the topk matches progressively
further reducing the search engines response time
better matches are computed first thereby further
better matches are returned first thereby further
we also show how the search engines can be performed efficiently in a suitable structured p2p overlay network
the benefits of the proposed method are demonstrated through experimental evaluation on both synthetic data
the benefits of the proposed method are demonstrated through experimental evaluation on both real datathis paper describes the first steps towards developing a methodology for testing
this paper describes the first steps towards evaluating the performance of reasoners for the probabilistic description logic pshiq
since this paper is a new formalism for handling uncertainty in dl ontologies no such methodology has been proposed
there are no sufficiently large probabilistic ontologies to be used as test suites
in addition since the reasoning services in pshiq are mostly query oriented there is no single problem  like classification in classical dl 
no single problem  like realization in classical dl  that could be an obvious candidate for benchmarking
in addition since the reasoning services in pshiq are mostly query oriented there is no single problem  like realization in classical dl 
no single problem  like classification in classical dl  that could be an obvious candidate for benchmarking
all these issues make it hard to evaluate the performance of reasoners
all these issues make it hard to reveal the complexity bottlenecks
all these issues make it hard to assess the value of optimization strategies
this paper addresses these important problems by making the following contributions this paper addresses describes a probabilistic ontology
breast cancer which poses significant challenges for the stateofart pshiq reasoners
this paper addresses these important problems by making the following contributions first describes a probabilistic ontology
a probabilistic ontology that has been developed for the reallife domain of breast cancer
she would light on what makes reasoning in pshiq hard in practice
second
this paper addresses explains a systematic approach to generating a series of probabilistic reasoning problems
probabilistic reasoning problems that enable evaluation of the reasoning performance
finally this paper presents an
an optimized algorithm for the nonmonotonic entailment
this paper positive impact on performance is demonstrated using our evaluation methodologyan xmlbased model that is specialized into a number of languages such as eventsml g2
for easing the exchange of news the international press telecommunication council has developed the newsml architecture an xmlbased model
an xmlbased model that is specialized into a number of languages such as newsml g2
as part of the newsml architecture an xmlbased model specific controlled vocabularies such as the the international press telecommunication council news codes are used to categorize news items together with other industrystandard thesauri
as part of the newsml architecture an xmlbased model specific controlled vocabularies such as the the international press telecommunication council news codes are used to categorize news items together with other industrystandard thesauri
an xmlbased model that is specialized into a number of languages such as newsml g2
an xmlbased model that is specialized into a number of languages such as eventsml g2
while news is still mainly in the form of textbased stories these are often illustrated with images
while news is still mainly in the form of textbased stories these are often illustrated with videos
while news is still mainly in the form of textbased stories these are often illustrated with graphics
mediaspecific metadata formats such as dig35 are used to describe the media
mediaspecific metadata formats such as exif are used to describe the media
mediaspecific metadata formats such as xmp are used to describe the media
the use of different metadata formats in a single production process leads to interoperability problems within the news production chain the news production chain
the news production chain also impedes the construction of uniform enduser interfaces for searching news content
the news production chain also impedes the construction of uniform enduser interfaces for browsing news content
the news production chain also excludes linking to existing web knowledge resourceswe describe rdf123 for translating spreadsheet data to rdf
we describe a highly flexible opensource tool for translating spreadsheet data to rdf
existing spreadsheettordf tools typically map only to starshaped rdf graphs each spreadsheet row is an instance with each column
each column representing a property
rdf123 on the other hand allows users to define mappings to arbitrary graphs thus allowing much richer spreadsheet semantics to be expressed
further each row in the spreadsheet can be mapped with a fairly different rdf scheme
two interfaces are available
the first is a graphical application
a graphical application that allows users to create users
users mapping in an intuitive manner
a web service that takes as input a url to a google spreadsheet map
a web service that provides rdf as output
a web service that takes as input a url to csv file map
a web service that takes as input a url to an rdf123 map
the second is a web servicevocabularies originating from often called folksonomies
systems in which users can annotate items
vocabularies originating from keywords
the emergence of web raises the question of the semantic interoperability between vocabularies
vocabularies originating from collaborative annotation processes
keywords assigned in a more traditional way
web based systems
if collections are annotated according to two systems with keywords the annotated data can be used for instance
if collections are annotated according to two systems with tags the annotated data can be used for instance
instance based mapping between the vocabularies
concepts based on this kind of matching distribution as annotations
the basis for this kind of matching is an appropriate similarity measure between concepts
in this paper we propose a new similarity measure generated metadata
a new similarity measure that can take advantage of some special properties of user
we have evaluated a new similarity measure
wikipedia which are both annotated by users of the bookmarking service delicious
wikipedia which are both classified according to the topic structure of wikipedia
a new similarity measure that can take advantage of some special properties of user with a set of articles from wikipedia
a new similarity measure that can take advantage of some special properties of user
standard similarity measures proposed for this task in the literature
the results using a new similarity measure
this task in the literature correlates better with human judgments
the results are significantly better than those
those obtained using standard similarity measures
we argue that a new similarity measure also has benefits for instance
a new similarity measure that can take advantage of some special properties of user
instance based mapping of more traditionally developed vocabulariesthe emerging paradigm of serviceoriented computing requires novel techniques for various servicerelated tasks
along with automated support for service discovery negotiation support for automated service contracting and enactment is crucial for any large scale service environment
any large scale service environment where large numbers of clients interact
along with automated support for service discovery selection support for automated service contracting and enactment is crucial for any large scale service environment
any large scale service environment where large numbers of service providers interact
along with automated support for service discovery composition support for automated service contracting and enactment is crucial for any large scale service environment
many problems in this area involve reasoning
a number of logicbased methods to handle many problems in this area have emerged in the field of semantic web services
in this paper we build upon we previous work
previous work where we used concurrent transaction logic to model about service contracts
previous work where we used concurrent transaction logic to reason about service contracts
we extend the proof theory of concurrent transaction logic to enable reasoning about such contracts
we significantly extend the modeling power of the previous work by allowing iterative processes in the specification of service contracts
with this extension we logicbased approach is capable of modeling general services
modeling general services represented using languages such as wsbpelseveral ontology repositories provide access to the growing collection of ontologies on the semantic web
some repositories collect ontologies automatically by crawling the semantic web in other repositories users submit ontologies users
in addition to providing search across multiple ontologies the added value of ontology repositories lies in the ontologymapping metadata that users may contain
information provided by ontology authors such as intended use
the ontologymapping metadata may include mapping metadata
the ontologymapping metadata may include feedback
mapping metadata that relates concepts from different ontologies
information provided by ontology authors such as ontologies scope
feedback provided by users such as ontologies experiences in using the ontologies or reviews of the content
the ontologymapping metadata may include information
in this paper we focus on the ontologymapping metadata to collect ontology mappings
in this paper we focus on communitybased method to collect ontology mappings
more specifically we develop a model for representing the metadata
more specifically we develop a model for representing mappings
mappings collected from the user community
the metadata associated with the mapping
we use a model to bring together more than 30000 mappings from 7 sources
upload mappings created with other tools
we also validate a model by extending bioportala repository of biomedical ontologies that we have developed to enable users to create single concepttoconcept mappings in a model graphical user interface to upload mappings to comment on the mappings and to discuss other tools and to visualize the corresponding metadata
download mappings created with other tools
we also validate a model by extending bioportala repository of biomedical ontologies that we have developed to enable users to create single concepttoconcept mappings in a model graphical user interface to download mappings to comment on the mappings and to discuss other tools and to visualize the mappings
we also validate a model by extending bioportala repository of biomedical ontologies that we have developed to enable users to create single concepttoconcept mappings in a model graphical user interface to upload mappings to comment on the mappings and to discuss other tools and to visualize the mappings
we also validate a model by extending bioportala repository of biomedical ontologies that we have developed to enable users to create single concepttoconcept mappings in a model graphical user interface to download mappings to comment on the mappings and to discuss other tools and to visualize the corresponding metadatacorrespondences in ontology alignments relate two ontology entities with a relation
typical relations are equivalence
typical relations are subsumption
however different systems may need different kinds of relations
we propose to use the concepts of algebra of relations in order to express the relations between ontology entities in a general way
we show the benefits in doing so in expressing disjunctive relations
we show the benefits in amalgamating alignments with relations of different granularity
we show the benefits in composing alignments
we show the benefits in merging alignments in different ways
