A common problem is the imprecision in the use of vocabularies annotators can misunderstand the semantics of a class or property
A common problem is the imprecision in the use of vocabularies annotators may not be able to find the right objects to annotate with
a CQE algorithm that can be implemented by relying on stateoftheart triple stores
a CQE algorithm that has the same computational complexity as standard query answering
Addressing this task is especially challenging when dealing with geospatial datasets due to the potential complexity of single geospatial objects
Addressing this task is especially challenging when dealing with geospatial datasets due to this task sheer size of single geospatial objects
a deontic logic semantics which allows us to define the deontic components of obligations
a deontic logic semantics which allows us to define the deontic components of permissions
a deontic logic semantics which allows us to define the deontic components of prohibitions
a deontic logic semantics which allows us to define the deontic components of the licenses
a deontic logic semantics which allows us to generate a composite license compliant with the licensing items of the composed different licenses
a feature needed in several application domains
Afterwards those patterns are converted to SPARQL based pattern detection algorithms
a general authorisation framework that can be used to cater for the secure manipulation of linked data
a general authorisation framework that can be used to deliver dynamic query results based on user credentials
a graphbased approach to hypothesize a rich semantic description of a new target source from a set of known sources takes into account user corrections to learn more accurate semantic descriptions of future data sources
a graph that represents the space of plausible source descriptions
A huge challenge the massive extraction of RDF facts from unstructured data has remained open so far
A huge challenge the timely extraction of RDF facts from unstructured data has remained open so far
a joint inference module that uses knowledge from the linked open data cloud to jointly infer eg numbers 
a joint inference module that uses knowledge from the linked open data cloud to jointly infer eg strings 
a joint inference module that uses knowledge from the linked open data cloud to jointly infer relations between columns
a joint inference module that uses knowledge from the linked open data cloud to jointly infer table cell values 
a joint inference module that uses knowledge from the linked open data cloud to jointly infer the semantics of column headers
A key facet is instrumental for many applications is the entity type
A key facet that is often displayed on the Engine Result Pages and that
a knowledge base that reflects the content of the input streams
A learning model is used to obtain the weights of different features
A learning model predict the destination entity for each attribute value
algorithms are implemented in we CQELS Cloud system and we present extensive evaluations of Our approach and algorithms superior performance on Amazon EC2 demonstrating excellent elasticity in a real deployment
algorithms are implemented in we CQELS Cloud system and we present extensive evaluations of Our approach and algorithms superior performance on Amazon EC2 demonstrating high scalability in a real deploymentIn the domain of Linked Open Data a need is emerging for developing automated frameworks able to generate the licensing terms
algorithms on multiple realworld graph data sets showing that We algorithms are efficient even on networks with up to 15M edges than popular triple stores
algorithms on multiple realworld graph data sets showing that We algorithms are efficient even on networks with up to 6M vertices than popular triple stores
algorithms on multiple realworld graph data sets showing that We algorithms are far more efficient than popular triple stores
All those types are correct
Although an increasing number of RDF knowledge bases are published many of those consist primarily of instance data
Among all the infobox attributes those attributes identify semantic relations between entities
an approach called Topical Relational Model
an approach that allows extracting RDF triples from unstructured data streams
an easytouse webbased tool that covers most of the frequently used OWL constructs
an evaluation framework that enables developers to analyze the performance of different ontologyranking methods
an evaluation framework that enables developers to compare the performance of different ontologyranking methods
a new blocking condition for characterizing modelsunfortunately transforming these candidate facts into useful knowledge is a formidable challenge
an experimental evaluation that demonstrates the scalability of this approach
An extensive experimental evaluation over several document collections at different levels of different type hierarchies  including DBPedia  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
An extensive experimental evaluation over several document collections at different levels of different type hierarchies  including Freebase  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
An extensive experimental evaluation over several document collections at different levels of different type hierarchies  including schemaorg  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
An extensive experimental evaluation over several document collections at different levels of eg sentences paragraphs   including DBPedia  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalableSemantic models of services provide support to automate many tasks such as data integration
An extensive experimental evaluation over several document collections at different levels of eg sentences paragraphs   including Freebase  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
An extensive experimental evaluation over several document collections at different levels of eg sentences paragraphs   including schemaorg  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
An extensive experimental evaluation over several document collections at different levels of granularity   including DBPedia  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
An extensive experimental evaluation over several document collections at different levels of granularity   including Freebase  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
An extensive experimental evaluation over several document collections at different levels of granularity   including schemaorg  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an iterative approach that builds on human feedbackMuch of Web search is today centered around entities
an ontology that alone is consistenta largescale knowledge base that exploits Wikipedia as primary data source
a novel application that helps users explore a large scale RDF dataset
a novel duplicateaware approach to federated querying over the Web of Data is based on a combination of compact data summaries
a novel duplicateaware approach to federated querying over the Web of Data is based on a combination of minwise independent permutations
a novel hybrid query engine based on the SPARQL federation framework FedX
a novel Semantic Message Passing algorithm which uses the linked open data knowledge to improve existing message
applications
approaches which are able to infer a partial order of services
appropriate tools that could analyze the underlying semantics in mathematical papers and effectively build mathematical papers consolidated representation
a reductionratiooptimal link discovery approach designed especially for geospatial data
As a result a set of relationspecific relevant concepts is obtained
a semiautomatic schemata construction approach addressing this problem First in existing knowledge bases
a semiautomatic schemata construction approach addressing this problem the frequency of axiom patterns in existing knowledge bases
a source model that includes the relationships between the attributes in addition to the attributes in addition semantic types
a source selection mechanism that maximises the query recallLinked Stream Data extends the Linked Data paradigm to dynamic data sources
a stateoftheart approach that requires manuallytuned regularization
As the group we are keenly aware of how difficult the users perceive this task to be
a synthetic Linked Data corpus derived from a realworld set of extractions from the NELL project
a synthetic Linked Data corpus derived from the MusicBrainz music community from the NELL project
a taskcentric empirical evaluation which shows that Rexplore is highly effective at providing support for the aforementioned sensemaking tasks
a task that is notorious for Ontology engineering difficulty
a technique is freely available as a part of the opensource EL reasoner ELK efficiency is demonstrated on naturally occurringvarious entities which have been explored by DBpedias to generate large scale Linked Data
a technique is freely available as a part of the opensource EL reasoner ELK efficiency is demonstrated on synthetic data
a technique is freely available as a part of the opensource EL reasoner ELK is demonstrated on naturally occurring
a technique is freely available as a part of the opensource EL reasoner ELK is demonstrated on synthetic data
a technique which avoids this extra cost while being very efficient for small incremental changes in ontologies
a technique which avoids this extra cost while being very efficient for small incremental changes in ontologies
a tool that will be easy to use while still accounting for commonly used OWL constructs social interaction around distributed ontology editing as part of the core tool design
a tool that will be easy to use while still accounting for commonly used OWL constructs support collaboration around distributed ontology editing as part of the core tool design
At the same time researchers have paid less attention to investigating the usability of common visualization techniques that many practitioners regularly use to visualize ontological data
At the same time We system is able to deal with an ontology
At the same time We system is able to deal with possible inconsistencies between the nonmonotonic rules
Based on a corpus of 500 papers we analyse the importance
Based on a corpus of 500 papers we analyse the quality of experimental research compare the importance to general Computer Science
Based on a corpus of 500 papers we analyse the quality of experimental research conducted
Both pattern extraction are completely automated
Both pattern usage are completely automated
business intelligence
But such additional data can consume memory
But such additional data place an extra burden on the reasoner during application of inferences
By performing intelligent grouping of query clauses we are able to reduce significantly the communication costs making we approach suitable for topk hybrid search across multiple data sources
By performing onthefly adaptation of the query execution plan we are able to reduce significantly the communication costs making we approach suitable for topk hybrid search across multiple data sources
candidate facts associated extraction confidences
candidate facts associated extraction confidences
Combining structured queries with fulltext search provides a powerful means to access
completeness statements expressed in RDF
Consequently the same procedure has been applied to other languages to create the localized versions of DBpedia
content exposed
CQE algorithms that ensure confidentiality of sensitive information are efficiently implementable by means of RDF triple store technologies
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of facets to random sampling
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of ontology versions to random sampling
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of variants to random sampling
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of facets to random sampling
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of ontology versions to random sampling
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of variants to random sampling
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of facets to random sampling
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of ontology versions to random sampling
Current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of variants to random sampling
Currently the Linking Open Data cloud lacks detailed information on professional level mathematics
Currently the Linking Open Data cloud lacks uptodate information on professional level mathematics
data coming from heterogeneous distributed sources
DAW is based on a combination of compact data summaries
DAW is based on a combination of minwise independent permutations
DBpedia is a largescale knowledge base
Despite approaches services data flow remains implicit and difficult to be automatically generated
Despite the large number and variety of services available today for exploring scholarly data current support focus instead on performing finegrained academic expert search along multiple dimensions
Despite the large number and variety of services available today for exploring scholarly data current support focus instead on understanding the dynamics of research areas
Despite the large number and variety of services available today for exploring scholarly data current support is still very limited in the context of sensemaking tasks
Despite the large number and variety of tools available today for exploring scholarly data current support focus instead on performing finegrained academic expert search along multiple dimensions
Despite the large number and variety of tools available today for exploring scholarly data current support focus instead on relating authors semantically
Despite the large number and variety of tools available today for exploring scholarly data current support focus instead on understanding the dynamics of research areas
Despite the large number and variety of tools available today for exploring scholarly data current support is still very limited in the context of sensemaking tasks
different sources which may provide only limited information about These entities content
diverse biomedical terminology contained in biomedical ontologies
Due to the decentralised architecture of this compendium several of the Web of Data contain duplicated data
each of relationspecific relevant concepts is then used to represent the structured semantics of the corresponding relation
each query provides application context not available in the ontologies alone thereby the system is able to disambiguate mappings for different queries
effective semantic rule filtering
Empirical activities often resort to  somewhat arbitrarily  hand curated corpora available on the web
Empirical activities often resort to  somewhat arbitrarily  hand curated corpora manually selected sets of wellknown ontologies
Empirical activities often resort to  somewhat arbitrarily  hand curated corpora such as the NCBO BioPortal
Empirical activities often resort to  somewhat arbitrarily  hand curated corpora such as the TONES Repository
empirical experimentation in OWL ontology engineering require a wide variety of suitable ontologies as detailed characterisations of real ontologies
empirical experimentation in OWL ontology engineering require a wide variety of suitable ontologies as input for testing purposes
encyclopedic knowledge describing entities
English Wikipedia infoboxes contain rich structured information of various entities sets
Enhanced with formal representations the semantic links between input parameters of services can be then exploited to infer services data flow
Enhanced with formal representations the semantic links between output parameters of services can be then exploited to infer services data flow
entities which are important for creating RDF links between DBpedias instances
entity consolidation performed at query time
entity consolidation that is based on using language models hence more suitable for this onthefly uncooperative setting than stateoftheart methods
entity consolidation that is completely unsupervised hence more suitable for this onthefly uncooperative setting than stateoftheart methods
entity search where the units of retrieval are structured These entities
existing message passing schemes
existing topic modeling approaches which are highly tailored toward document collections
Experimentation is an important way to validate results of Computer Science research in general
experiments using data sources
experiments using real Web queries
extending large repositories of formalized knowledge
federated entity search where redundant information about entities is reduced onthefly through entity consolidation
Finally Our results indicate that Orchid scales to large datasets while outperforming the state of the art significantly
Finally we present an extension of the completeness framework for federated data sourcesAlthough an increasing number of RDF knowledge bases are published many of those lack sophisticated schemata
Findings from a controlled usability study with an emphasis on the effectiveness efficiency workload and satisfaction of these visualization techniques have revealed both strengths and weaknesses of each visualization technique
Findings of results of benchmarking activities may be biased even heavily towards these datasets
Findings of surveys of benchmarking activities may be biased even heavily towards these datasets
For analyzing this type of data existing topic modeling approaches require manuallydefined regularization terms to exploit
For analyzing this type of data existing topic modeling approaches require manuallydefined regularization terms to to bias the topic
For example one can find on the Linked Open Data cloud the fact that Tom Hanks is an actor
For example one can find on the Linked Open Data cloud the fact that Tom Hanks is a person
For example one can find on the Linked Open Data cloud the fact that Tom Hanks is a person from Concord California
For ontology reuse and integration a number of approaches have been devised that aim at identifying modules of  relevant  axioms from ontologies
For the identified fragment We design a CQE algorithmNowadays search on the Web goes beyond the retrieval of textual Web sites
For the seven semantic relations the semantic filter consistently yields a higher precision at any relative recall value in the highrecall rangeThe vision behind the Web of Data is to extend the current documentoriented Web with structured data thus creating a representation of general knowledge
For this reason Search Engine Result Pages increasingly contain information about the searched entities such as factual information
For this reason Search Engine Result Pages increasingly contain information about the searched entities such as pictures
For this reason Search Engine Result Pages increasingly contain information about the searched entities such as related entities
For this reason Search Engine Result Pages increasingly contain information about the searched entities such as short summaries
For this we have developed novel highly efficient for continuous query operators
For this we have developed scalable parallel algorithms for continuous query operators
Further different sources which may provide only limited information about their content capture complementary but also redundant information about entities
Furthermore mappings need maintenance due to the constant changes of Wikipedia articles
Furthermore mappings need maintenance due to the quick changes of Wikipedia articles
Further we apply the same language model technique to deal with the federated search problem of ranking results returned from different sources
Given a request with a specific context The contextual tag cloud system needs to quickly find what how many instances in the context use each tag
Given a request with a specific context The contextual tag cloud system needs to quickly find what other tags the instances in the context use instances in the context use each tag
Having such schemata allows consistency checking
Having such schemata allows debugging as well as improved inference
Having such schemata allows more powerful querying
Here
Here we consider modules polynomial in the size of the ontologyreadytouse linked open data sources currently available on The Semantic Web
Here we consider three logically sound notions of modules MEX modules only applicable to inexpressive ontologies these modules a sound approximation of the first polynomial in the size of the ontology
How does her know which ontology is the most relevant to her search
how do Our measure how well Our ranking works
However an entity is usually not associated to a set of more specific types or not given the document context
However an entity is usually not associated to a single generic type in the background knowledge bases or not given the document context
However constructing a source model remains a largely unsolved problem
However executing hybrid search queries in a federation of multiple data sources presents a number of challenges due to data source heterogeneity and lack of statistical data about keyword selectivity
However most large open knowledge bases are incomplete with respect to type information
However most large open knowledge bases contain incorrect data
However most large open knowledge bases contain noisy data
However most of the Web of Data is limited to being a large compendium of encyclopedic knowledge
However quite a few hyperlinks have not been anotated by editors in infoboxes
However such a decentralised architecture brings with such a decentralised architecture a number of additional challenges with respect to both data security and integrity
However the number of accomplished mappings is still limited to most frequent infoboxes
However the number of accomplished mappings is still small to most frequent infoboxes
However when these bounds do not coincide there still remain a number of possible answer tuples
In addition the results are robust both also with respect to whether the aforementioned sensemaking tasks are selected by the evaluators or proposed by the users the aforementioned sensemaking tasks
In addition the results are robust both with respect to the background of the users themselves to whether the aforementioned sensemaking tasks are selected by the evaluators or proposed by the users the aforementioned sensemaking tasksWe describe work on automatically inferring the intended meaning of tables making the intended meaning of tables available for improving interoperability
In experiments we demonstrate that we optimization techniques can lead to a substantial performance improvement reducing the execution time of hybrid queries by more than an order of magnitudeWith thousands of RDF data sources available on the Web and possibly overlapping knowledge domains the problem of providing highlevel descriptions  in the form of metadata  of thousands of RDF data content becomes crucial
infinitely nontreeshaped models which are different from those of SHOIQ ontologies
infoboxes which causes lots of relations between entities
In order to perform the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification we must identify coreferent entities
In order to perform the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification we must incorporate ontological constraints
In order to perform the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification we must reason jointly about candidate facts
In order to perform the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification we must reason jointly about candidate facts
In order to perform the task of inferring missing information we must identify coreferent entities
In order to perform the task of inferring missing information we must incorporate ontological constraints
In order to perform the task of inferring missing information we must reason jointly about candidate facts
In order to perform the task of inferring missing information we must reason jointly about candidate facts
In order to perform the task of removing noise we must identify coreferent entities
In order to perform the task of removing noise we must incorporate ontological constraints
In order to perform the task of removing noise we must reason jointly about candidate facts
In order to perform the task of removing noise we must reason jointly about candidate facts
In our previous work our showed how a scalable OWL 2 RL reasoner can be used to compute both lower bound query answers over arbitrary OWL 2 ontologies
In our previous work our showed how a scalable OWL 2 RL reasoner can be used to compute both upper bound query answers over arbitrary OWL 2 ontologies
In our previous work our showed how a scalable OWL 2 RL reasoner can be used to compute both upper bound query answers over very large datasets
In particular while the indented tree visualization is more organized and familiar to novice users subjects found the graph visualization to be more controllable and intuitive without visual redundancy particularly for ontologies with multiple inheritanceTo bring the Life Sciences domain closer to a Semantic Web realization it is fundamental to establish meaningful relations between biomedical ontologies
instances specified by the context a user constructs
instances that use each tag
instances that use particular ontological terms
Instead of leveraging Tbox information from the schema SDType is also robust to misused schema elements
Instead of leveraging Tbox information from the schema SDType takes the actual use of a schema into accountthe contextual tag cloud system which can execute large volumes of queries about the number of instances
In this article we propose a semiautomatic schemata construction approach is discovered
In this context materializing every possible implicit derivation from a given input can be computationally expensive especially when considering large data volumesOntology engineering is a task
In this environment of uncooperative data sources we study the problem of federated entity search
In this paper
In this paper Our develop an evaluation framework
In this paper our show computed by a scalable OWL 2 RL reasoner to efficiently identify a subset of the data and ontology
In this paper we address the problem of the actuality of the Web of Data by presenting an approach
In this paper we address this gap by presenting a reductionratiooptimal link discovery approach
In this paper we address this gap by presenting Orchid
In this paper we define the new task of ranking entity types given an entity
In this paper we define the new task of ranking entity types given an entity context
In this paper we develop a standard setbased  extensional  semantics for the RDF Schema vocabulary while preserving the computational complexity of deduction of the intensional version
In this paper we develop a standard setbased  extensional  semantics for the RDF Schema vocabulary while preserving the simplicity complexity of deduction of the intensional version
In this paper we focus on the problem of automatically mapping infobox attributes to properties into the DBpedia ontology for extending the coverage of building from scratch versions for languages not covered in the current version
In this paper we focus on the problem of automatically mapping infobox attributes to properties into the DBpedia ontology for extending the coverage of the existing localized versions
In this paper we focus on two popular ontology visualization techniques graph
In this paper we focus on two popular ontology visualization techniques indented tree
In this paper we inspect the feasibility of propertybased typing for accessing data from the linked open data cloud
In this paper we introduce a theoretical framework for describing data sources in terms of data sources completeness
In this paper we investigate how different approaches for the lexical components of several biomedical ontologies use can impact the performance of ontology matching techniques
In this paper we investigate the current status of experimental work on the Semantic Web
In this paper we investigate the development
In this paper we present a graphbased approach to hypothesize a rich semantic description of a new target source from a set of known sources
In this paper we present an overview of the lexical components of several biomedical ontologies
In this paper we present a technique
In this paper we present scalable algorithms to find the topK answers to a practically important subset of SPARQLqueries via a suite of pruning techniques
In this paper we present the infrastructure of the contextual tag cloud system
In this paper we present the new version of WebProtege that we designed with two main goals in mind create a tool
In this paper we propose a decision procedure for this logic
In this paper we propose a general authorisation framework
In this paper we propose an approach for automatically discovering the missing entity links in English Wikipedia infoboxes so that the missing semantic relations between entities can be established
In this paper we propose the heuristic linkbased type inference mechanism SDType
In this paper we show how to replace the query rewriting with a filtering technique
In this paper we show how uncertain extractions about entities relations can be transformed into a knowledge graph
In this paper we show how uncertain extractions about entities relations can be transformed into a knowledge graph
In this paper we survey ontologies as large numbers of facets describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling various forms of deduplications
In this paper we survey ontologies as large numbers of facets describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling various forms of manual cleaningWebscale relation extraction is a means for building
In this paper we survey ontologies as large numbers of facets exist on the web
In this paper we survey ontologies as large numbers of ontology versions describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling various forms of deduplications
In this paper we survey ontologies as large numbers of ontology versions describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling various forms of manual cleaning
In this paper we survey ontologies as large numbers of ontology versions exist on the web
In this paper we survey ontologies as large numbers of variants describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling various forms of deduplications
In this paper we survey ontologies as large numbers of variants describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling various forms of manual cleaning
In this paper we survey ontologies as large numbers of variants exist on the web
It can be directly combined with existing federated query engines in order to achieve the same query recall values while querying fewer data sources
Just like Google does these answers can be shown to the user in accordance with an importance ranking
Known algorithms address this problem
known sources that have been modeled over the same domain ontology
known sources that have been modeled over the same domain ontology
Largescale information processing systems are able to extract massive collections of these candidate facts
Linked Stream Data enables the integration processing of heterogeneous stream data with quasistatic data from the Linked Data Cloud in nearrealtime
Linked Stream Data enables the joint processing of heterogeneous stream data with quasistatic data from the Linked Data Cloud in nearrealtime
logical consequences that are no longer valid
lots of relations between entities being missing in English Wikipedia
models that are twice as accurate than the models
models which may have an infinite nontreeshaped part
modules based on syntactic locality a sound approximation of and thus the first  widely used since these modules can be extracted from OWL DL ontologies in time
modules based on syntactic locality a sound approximation of the second  widely used since these modules can be extracted from OWL DL ontologies in time
Moreover a novel duplicateaware approach to federated querying over the Web of Data provides a source selection mechanism
Moreover DAW provides a source selection mechanism
Moreover our approaches to the computation of Hausdorff distances require two orders of magnitude less time than a naive approach to achieve this goal
more specific types which may be relevant
Most of the related work focuses on automatic annotation with classes of input
Most of the related work focuses on automatic annotation with classes of output parameters
Most of the related work focuses on automatic annotation with classes of source attributes
Most of the related work focuses on automatic annotation with properties of input
Most of the related work focuses on automatic annotation with properties of output parameters
Most of the related work focuses on automatic annotation with properties of source attributes
Much of browsing activity is today centered around entities
new paradigms in which data as the main driver of applications is promoted to first class status
news retrieval
Nowadays search on the Web increasingly takes advantage of the growing amount of structured data
Of particular interest is entity search
One of the reasons is the effort
ontologies expressed in the EL family of Description Logics after some axioms have been added
ontologies expressed in the EL family of Description Logics after some axioms have been deleted
ontologies that are formulated in the basic version of the description logic DLLite
ontologies that contain her terms of interest
ontologies which we define through an acyclicity constraint over a reference relation between equivalence classes of concepts
ontology based extraction conversion of the article body into RDF integration with some existing Linking Open Data data sets search
ontology based extraction conversion of the article body into RDF integration with some semantic search
ontology based the article body metadata into RDF integration with some existing Linking Open Data data sets search
ontology based the article body metadata into RDF integration with some semantic search
Orchid relies on a combination of orthodromic metrics to compute the distance between geospatial objects
Orchid relies on a combination of the Hausdorff to compute the distance between geospatial objects
Our analysis however shows that papers are more often cited than other papers
Our collected log data from more than 4800 BioPortal searches
Our evaluate Our framework by analyzing the data on BioPortal searches
Our evaluation shows that Our method produces models
Our explore several different ranking algorithms
Our framework is based on processing search logs
Our framework is determining how often users select the top link that the search engine offers
Our further analysis demonstrates that ranking ontologies based on page view data significantly improves the user experience with an approximately 26We study confidentiality enforcement in ontologybased information systems where ontologies are expressed in a profile of OWL 2
Our measure the effectiveness of each ranking by measuring how often users click on the highest ranked ontology
Our research group hosts a public repository of more than 330 ontologies in the biomedical domain
Our research group hosts BioPortal 
Our results show that regardless of the ranking in more than half the searches users select the first link
our show how in the case of Horn ontologies one can exploit the lower
our show how in the case of Horn ontologies one can exploit upper bounds
OWL 2 that is becoming increasingly popular in Semantic Web applications
papers comparing Our analysis to other systemsthe Protege plugin NoHR that allows the user to query the combined knowledge base
Particular novel are the mechanisms we propose to incorporate consolidation results into this ranking
pattern detection algorithms which allow to enrich knowledge base schemata
possible answer tuples whose status is not determined
precision which is hard to achieve with automatically acquired rule sets
QODI is distinguished in that the ontology mapping algorithm dynamically determines a partial mapping specific to the reformulation of each query
queries sent to the endpoints
query rewriting when DLLite is extended with the popular role hierarchies
Relying on properties of resources as an indicator for the type propertybased typing is such a paradigm
Rexplore which integrates semantic technologies to provide effective support for exploring sense of scholarly data
Rexplore which integrates semantic technologies to provide effective support for exploring sense of scholarly data
Rexplore which integrates semantic technologies to provide effective support for making sense of scholarly data
Rexplore which integrates semantic technologies to provide effective support for making sense of scholarly data
Rexplore which integrates statistical analysis to provide effective support for exploring sense of scholarly data
Rexplore which integrates statistical analysis to provide effective support for exploring sense of scholarly data
Rexplore which integrates statistical analysis to provide effective support for making sense of scholarly data
Rexplore which integrates statistical analysis to provide effective support for making sense of scholarly data
Rexplore which integrates visual analytics to provide effective support for exploring sense of scholarly data
Rexplore which integrates visual analytics to provide effective support for exploring sense of scholarly data
Rexplore which integrates visual analytics to provide effective support for making sense of scholarly data
Rexplore which integrates visual analytics to provide effective support for making sense of scholarly data
rules whose result together with the nonmonotonic rules serve as input for the topdown querying engine XSB Prolog
Sampling from a large corpus of ontologies on the other hand may lead to more representative results
Semantic models of data sources provide support to automate many tasks such as data integration
Semantic models of data sources provide support to automate many tasks such as service composition
Semantic models of data sources provide support to automate many tasks such as source discovery
Semantic models of services provide support to automate many tasks such as service composition
Semantic models of services provide support to automate many tasks such as source discovery
sensemaking tasks which go beyond standard search and ranking of authors
sensemaking tasks which go beyond standard search and ranking of publications
sentiment analysis
services based on This work representations
several biomedical ontologies based both on internal synonym derivation and on external ontologies
Several Linked Stream Data processing engines
Several Linked Stream Data processing engines
Several Linked Stream Data processing engines exist but Several scalability still
Several needs to be in improved in terms of dynamic data sizes number of concurrent queries
Several needs to be in improved in terms of static data sizes number of concurrent queries
Since the beta release of this new WebProtege interface in January 2013 we users from around the world have created 519 ontologies on we server
Since the beta release of this new WebProtege interface in January 2013 we users from around the world have uploaded 519 ontologies on we server
SKPs are automatically generated based on statistical data analysis
SKPs can be effectively used to automatically normalise data
SKPs can be effectively used to increase recall in querying
SKPs encapsulate key information about ontology classes including synonymous properties in datasets
So far none of Several supports parallel processing in the Linked Data Cloud in nearrealtime
Some heuristics are proposed to support the data publisher in choosing the licenses composition strategy which better suits her needs wrt the data her is publishingempirical experimentation in OWL ontology engineering require a wide variety of suitable ontologies as input for evaluation purposes
some may be too general to be eg person  while other may be already known to eg actor 
some may be too general to be eg person  while other may be already known to the user 
some may be too general to be eg person  while other may be interesting
some may be too general to be eg person  while other may be irrelevant given eg person from Concord California Concord California 
some may be too general to be eg person  while other may be irrelevant given the current browsing context 
some may be too general to be interesting  while other may be already known to eg actor 
some may be too general to be interesting  while other may be already known to the user 
some may be too general to be interesting  while other may be interesting
some may be too general to be interesting  while other may be irrelevant given eg person from Concord California Concord California 
some may be too general to be interesting  while other may be irrelevant given the current browsing context 
SPARQLqueries denoted as importance queries
Specifically we describe how conflict resolution policies can together be used to enforce consistent access control policies
Specifically we describe how conflict resolution policies can together be used to specify consistent access control policies
Specifically we describe how graph patterns can together be used to enforce consistent access control policies
Specifically we describe how graph patterns can together be used to specify consistent access control policies
Specifically we describe how integrity constraints can together be used to enforce consistent access control policies
Specifically we describe how integrity constraints can together be used to specify consistent access control policiesQODI is an automatic ontologybased data integration system
Specifically we describe how propagation rules can together be used to enforce consistent access control policies
Specifically we describe how propagation rules can together be used to specify consistent access control policies
Specifically we mapped 45978 Wikipedia infobox attributes to DBpedia properties in 14 different languages
standard ontologies queries and data stored in relational databases
stateoftheart methods that require training data
Still with respect to application development the Web community is just starting to develop new paradigms
stream update frequencies
synonyms encoded by several biomedical ontologies
tags that defines a subset of instances
TBoxes expressed in the lightweight description logic EL
Textrich structured data become more ubiquitous on the Web databases by encoding heterogeneous structural information between entities such as locations
Textrich structured data become more ubiquitous on the Web databases by encoding heterogeneous structural information between entities such as organizations
Textrich structured data become more ubiquitous on the Web databases by encoding heterogeneous structural information between entities such as organizations
Textrich structured data become more ubiquitous on the Web databases by encoding heterogeneous structural information between entities such as people
Textrich structured data become more ubiquitous on the Web databases by encoding heterogeneous structural information between entities such as people
Textrich structured data become more ubiquitous on the Web databases by encoding heterogeneous structural information between the associated textual information
Textrich structured data become more ubiquitous on the Web databases by encoding heterogeneous structural information between the associated textual information
Textrich structured data become on the enterprise databases by encoding heterogeneous structural information between entities such as locations
Textrich structured data become on the enterprise databases by encoding heterogeneous structural information between entities such as organizations
Textrich structured data become on the enterprise databases by encoding heterogeneous structural information between entities such as people
Textrich structured data become on the enterprise databases by encoding heterogeneous structural information between the associated textual information
Thanks to a large number of infoboxes has been mapped in the English DBpedia
Thanks to crowdsourcing has been mapped in the English DBpedia
That makes classic type inference by reasoning difficult
the amount of experimental work reported
the art system that does not learn from prior modelsThe Semantic Web makes an extensive use of the OWL DL ontology language underlied by the SHOIQ description logic to formalize The Semantic Web resources
The availability of such knowledge on the Web of Data would provide significant benefits to manifold
The availability of such knowledge on the Web of Data would provide significant benefits to manifold applications
the background knowledge exploited in several novel attacks
the background knowledge exploited in the attacksTextrich structured data become more ubiquitous on the Web databases by encoding heterogeneous structural information between entities such as locations
The comparison shows that a novel duplicateaware approach to federated querying over the Web of Data can greatly reduce the number of queries while keeping high query recall values
The comparison shows that DAW can greatly reduce the number of queries while keeping high query recall values
the context is a set of tags
The contextual tag cloud system is a novel application
The contextual tag cloud system visualizes the patterns of instances
the data and ontology that is large enough to resolve the status of these tuples yet small enough so that the status of these tuples can be computed using a fullyfledged OWL 2 reasoner
the effort required to create One of the reasons why schemata are still rare
The evaluation has been performed on the Italian mappings
The evaluation of We approaches is carried out on three real datasets of different size and complexity
The extraction procedure requires to manually map Wikipedia infoboxes into the DBpedia ontology
the first scalable knowledge base enrichment approach is evaluated on a large set of knowledge bases with a quantitativeFor ontology reuse and integration a number of approaches have been devised that aim at identifying small sets of  relevant  axioms from ontologies
the first scalable knowledge base enrichment approach is evaluated on a large set of knowledge bases with qualitative result analysis
the font sizes reflect the number of instances
the group that developed Protege
the group that developed the most widely used ontology editor
the heuristic linkbased type inference mechanism SDType which can handle incorrect data
the heuristic linkbased type inference mechanism SDType which can handle noisy data
The key question we answer in this paper is how to scale to Linked Data in particular we use a dataset over 380000 tags
The key question we answer in this paper is how to scale to Linked Data in particular we use a dataset with 14 billion triples
the licensing terms associated to data
the lightweight description logic EL which forms a basis of some large ontologies like Galen
the lightweight description logic EL which forms a basis of some large ontologies like Gene Ontology
the lightweight description logic EL which forms a basis of some large ontologies like SNOMED
The main benefits of SKPs are that The main benefits of SKPs structure allows for both accurate query expansion and restriction The main benefits of SKPs are context dependent hence The main benefits of SKPs describe the usage and meaning of properties in the context of a particular class hence the equivalence among relations can be used efficiently at run timeOne of the main advantages of using semantically annotated data is that machines can reason on One of the main advantages of using semantically annotated data deriving implicit knowledge from explicit information
The main benefits of SKPs are that The main benefits of SKPs structure allows for both accurate query expansion and restriction The main benefits of SKPs are context dependent The main benefits of SKPs can be generated offline hence the equivalence among relations can be used efficiently at run time
the mapping methods detailed for ClioExperimentation is an important way to validate results of the Semantic Web in general
the mapping methods detailed for data exchange system
the mapping methods detailed for the stateoftheart relational data integration
the models produced using a state of the art system
The most challenging issue we have to deal with when designing such a decision procedure is to represent infinitely nontreeshaped models
the most interesting challenges facing the Semantic Web today
the most interesting challenges facing the Web of services today
the NELL project containing over 1M extractions
the NELL project containing over 70K ontological relations
The normative version of RDF Schema gives nonstandard  intensional  interpretations to some standard notions such as classes thus departing from standard setbased semantics
Then we compute the top k candidates and suggest to the user a ranked list of the semantic models for the new source
Then We introduce a new confidentiality model sensitive enough to detect several novel attacks and a method for constructing secure knowledge bases views
Then We present the space tiling approach implemented by Orchid
Then We present the space tiling approach prove that it is optimal with respect to the reduction ratio that Orchid can achieve
the ones obtained by a human annotator in term of precision
the ontology mapping algorithm compares the set of paths with a similar decomposition of a source ontology
the ontology mapping algorithm decomposes each query into a set of paths
the OWL constructs that are present in ontologies that users have uploaded to WebProtege
The preliminary evaluation provides evidence in favor of we approach regarding the completeness of data flowWith thousands of ontologies available today in many different domains ontology search has become an important problem
the Protege plugin NoHR that allows the user to add a set of logic programming  rules suitable eg to express defaults
the Protege plugin NoHR that allows the user to add a set of logic programming  rules suitable eg to express exceptions
the Protege plugin NoHR that allows the user to add a set of nonmonotonic  rules suitable eg to express defaults
the Protege plugin NoHR that allows the user to add a set of nonmonotonic  rules suitable eg to express exceptions
the Protege plugin NoHR that allows the user to take an EL  op ontology
the query recall when the query processing is limited to a subset of the sources
the reasons why schemata are still rare
Therefore It can significantly improve the performance of federated query processing engines
The resource is made available in an open formatDespite the large number and variety of services available today for exploring scholarly data current support focus instead on relating authors semantically
The resulting hybrid approach has enabled us to compute exact answers to queries over datasets where previously only approximate query answering was possible
The resulting hybrid approach has enabled us to compute exact answers to queries over ontologies where previously only approximate query answering was possibleOver the last years the Web of Data has developed into a large compendium of interlinked data sets from multiple domains
The resulting relationspecific subgraphs of BabelNet are used as semantic filters for estimating the adequacy of the extracted rules
the rules that can be written
These entities are therefore called  uncooperative 
These entities reside in different sources
The Semantic Web has matured from a mere theoretical vision to a variety of readytouse
these problems were noticeable
the seven semantic relations tested here
The successful application of ontology matching techniques is strongly tied to an effective exploration of the complex
the tags are classes 
the tags are ontological terms 
the tags are properties 
The tool The tool builds on the procedure SLG
The tool The tool  with the help of OWL 2 EL reasoner ELK preprocesses the ontology into rules
the topic learning towards structure information
The vision behind the Web of Data is to extend the current documentoriented Web with machinereadable facts thus creating a representation of general knowledge
the Web covering disparate
The Web of Data is a rich common resource with billions of triples available in thousands of datasets created by both expert
The Web of Data is a rich common resource with billions of triples available in thousands of datasets created by both nonexpert ontologists
The Web of Data is a rich common resource with billions of triples available in thousands of individual Web documents created by both expert
This allows us to handle role hierarchies without an exponential blowup
This is complicated by the fact that the calculation should when directed by the user consider the entailment of taxonomic andor domainrange axioms in the ontology
This is natural from an implementation perspective
this logic extended with the transitive closure of roles in a feature
this logic extended with the transitive closure of roles in concept axioms
This paper decreases the quality of data
This paper describes SKP  as a means to address this issue
This paper describes Statistical Knowledge Patterns  as a means to address this issue
This paper evaluates a deontic logic semantics
This paper may eventually hamper This paper usability over large scale
This paper proposes a deontic logic semantics
This paper shows how precision of relation extraction can be considerably improved by employing a widecoverage for effective semantic rule
This paper shows how precision of relation extraction can be considerably improved by employing BabelNet for effective semantic rule
This paper shows how precision of relation extraction can be considerably improved by employing generalpurpose lexical semantic network for effective semantic rule
this problem using various forms of bookkeeping to trace the consequences back to premises
This result can positively impact current implementations as reasoning in RDF Schema can be compatible with OWL extensions
This result can positively impact current implementations as reasoning in RDF Schema can be implemented following common setbased intuitionsaccess control required  if any 
This type of automated knowledge building requires a decent level of precision learned from unlabeled data by means of distant supervision
This type of automated knowledge building requires a decent level of precision learned from unlabeled data by means of minimal supervision
This work addresses the problem of effectively inferring data flow between services
This work presents a novel duplicateaware approach to federated querying over the Web of Data
This work presents DAW 
those attributes having hyperlinks in DBpedias values
Though subgraph matching has been extensively studied as a query paradigm in social network data environments a user can get a large number of answers in response to a query
Thus it is even more critical to ensure that the ranking is appropriate if Our want to have satisfied users
To address The most challenging issue we have to deal with when designing such a decision procedure we introduce a new
To address these challenges we present a novel hybrid query engine
To address these challenges we present FedSearch
To address this gap we have developed a novel tool Rexplore
To alleviate these problems we developed an iterative approach
To establish the feasibility of our approach our evaluated the algorithm effectiveness on a small suite of benchmarks
To establish the feasibility of our approach our have implemented the algorithmThe basic idea of the combined approach to query answering in the presence of ontologies is to materialize the consequences of the ontology in the data and then use a limited form of query rewriting to deal with infinite materializations
Tool development for require a wide variety of suitable ontologies as detailed characterisations of real ontologies
Tool development for require a wide variety of suitable ontologies as input for evaluation purposes
Tool development for require a wide variety of suitable ontologies as input for testing purposes
To remedy these limitations this paper presents an approach for elastically parallelizing the continuous execution of queries over Linked Stream Data
To this end we introduce the non standard Description Logic reasoning join aiming to provide a  constructive evidence  of why services can be connected and how many to many parameters  can be inferred in data flow
To this end we introduce the non standard Description Logic reasoning join aiming to provide a  constructive evidence  of why services can be connected and how non trivial links  can be inferred in data flow
To We mind the main reason for that is the absence of appropriate tools
uncertain extractions about entities form an extraction graph
uncertain extractions about their relations form an extraction graph
Unlike hypothesised
Up until now RDF data publishers have focused on exposing public data
Up until now RDF data publishers have focused on linking public data
Using a topic model we can show that We approach is effective in exploiting heterogeneous structure information outperforming a stateoftheart approachthe lightweight description logic EL which forms a basis of some large ontologies like NCI
Using test sets from three real world applications QODI achieves favorable results compared with AgreementMaker
Using test sets from three real world applications QODI achieves favorable results compared with a leading ontology matcher
Using test sets from three real world applications QODI achieves favorable results compared with an ontologybased implementation of the mapping methods
various forms of deduplications which allows random sampling of ontologies for a variety of empirical applications
various forms of manual cleaning which allows random sampling of ontologies for a variety of empirical applications
We also carry out an experimental evaluationIn our previous work our showed how a scalable OWL 2 RL reasoner can be used to compute both lower bound query answers over very large datasets
We also implement a novel Semantic Message Passing algorithm
We analyse the performance of Ontop in a series of experiments
we analysis validates we empirical design demonstrates that an easytouse webbased tool is sufficient for many users to start editing many users ontologiesType information is very valuable in knowledge bases
we analysis validates we empirical design suggests additional language constructors to explore
We apply Word Sense Disambiguation to the content words of the automatically extracted rules
we approach
we approach first identifies entity mentions in the given infoboxes
we approach leads to a significant improvement in recall
we approach leads to a significant improvement in speed
we approach then computes several features to estimate the possibilities that a given attribute value might link to a candidate entity
We approach uses the wellfounded semantics for MKNF knowledge bases as underlying formalism so no restriction other than DLsafety is imposed on the rules
We argue that the platform may be helpful for enriching user experience on modern online scientific collectionsThe discovery of links between resources within knowledge bases is of crucial importance to realize the vision of the Semantic Web
We argue that We present the first scalable knowledge base enrichment approach based on real schema usage patterns
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of facets to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of ontology versions to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of variants to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of facets to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of ontology versions to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of variants to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of facets to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of ontology versions to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of variants to random sampling
we can not confirm a statistically significant correlation between a papers citations
we combine a scalable preprocessing approach with a speciallyconstructed inverted index
we compared we results with the current mappings on a random sample reannotated by the authors
We compare We extensions with the original approaches
we compare we system with a stateoftheart triple store analyze we design choicesAutomation of service composition is one of the most interesting challenges
we compare we system with a stateoftheart triple store examine how pruning rules interact with inference
we conduct a controlled usability study with an emphasis on the effectiveness efficiency workload and satisfaction of two popular ontology visualization techniques indented tree and graph in the context of assisting users during evaluation of ontology mappings
We demonstrate that for standard ontologies queries and data Ontop is fast efficientWe present We work on developing a software platform for mining mathematical scholarly papers to obtain a Linked Data representation
We demonstrate that for standard ontologies queries and data Ontop is fast produces SQL rewritings of high quality
we demonstrate the power of we method on a synthetic Linked Data corpus
we describe the key features of the new tool
we describe the main innovative elements of a novel tool Rexplore
We describe work on automatically inferring the intended meaning of tables making the intended meaning of tables available for improving integration
We describe work on automatically inferring the intended meaning of tables making the intended meaning of tables available for improving search
We describe work on representing the intended meaning of tables as RDF linked data making the intended meaning of tables available for improving integration
We describe work on representing the intended meaning of tables as RDF linked data making the intended meaning of tables available for improving interoperability
We describe work on representing the intended meaning of tables as RDF linked data making the intended meaning of tables available for improving search
we designed this new version of the WebProtege user interface empirically by analysing the use of OWL constructs in a large corpus of publicly available ontologies
We discuss the problem of minimizing TBoxes
We discuss the theoretical foundations of Ontop the treewitness query rewriting optimisations based on database integrity constraints
We discuss the theoretical foundations of Ontop the treewitness query rewriting optimisations based on SQL features
We discuss the theoretical foundations of Ontop the treewitness query rewriting Tmappings based on database integrity constraints
We discuss the theoretical foundations of Ontop the treewitness query rewriting Tmappings based on SQL features
we empirical design approach
We employ statistical methods in combination with deduplication to create a knowledge base
We employ statistical methods in disambiguation to create a knowledge base
We employ statistical methods in supervised machine learning techniques to create a knowledge base
We employ statistical methods in unsupervised machine learning techniques to create a knowledge base
We evaluate a sample of the RDF We generate against a large corpus of news streams
We evaluate a sample of the RDF We show that We achieve a precision of more than 85Though subgraph matching has been extensively studied as a query paradigm in semantic web a user can get a large number of answers in response to a query
we evaluated we approach on the English Wikipedia data the experimental results show that we can effectively find the missing relations between entities and Our approach significantly outperforms the baseline methods in terms of both precision and recallThe normative version of RDF Schema gives nonstandard  intensional  interpretations to some standard notions such as properties thus departing from standard setbased semantics
we evaluate language coverage in WebProtege by assessing how well WebProtege covers the OWL constructs
We evaluate new methods to find the most relevant entity type based on collection statistics interconnecting entities
We evaluate new methods to find the most relevant entity type based on collection statistics interconnecting types
We evaluate new methods to find the most relevant entity type based on the graph structure interconnecting entities
We evaluate new methods to find the most relevant entity type based on the graph structure interconnecting types
we evaluate the usability of WebProtege through a usability survey
We evaluate We implemented techniques on tables from the WebWe present technologies underpinning the OBDA system Ontop
We evaluate We implemented techniques on tables from Wikipedia
We experiments show that We approach for federated entity search with onthefly consolidation improves upon the performance of a stateoftheart preference aggregation baseline and also benefits from consolidationaccess distributed linked data
We exploit the known source models to build a graph
We exploit the same domain ontology to build a graph
We extend the SPARQL algebra to apply novel optimization techniques to improve the query processing efficiency while maintaining a meaningful ranking of results
We extend the SPARQL algebra to incorporate keyword search clauses as firstclass citizens
We extend three wellknown federated query processing engines DARQ with DAW
We extend three wellknown federated query processing engines FedX with DAW
We extend three wellknown federated query processing engines SPLENDID with DAW
We first present two novel approaches for the efficient computation of Hausdorff distances
We formalise a natural adaptation of the Controlled Query Evaluation  CQE  framework to ontologies
We formally can not be satisfied without imposing restrictions on ontologies
We formally show that all three requirements are in conflict
We goal is to ensure all three requirements
We goal is to provide CQE algorithms
We have developed a holistic approach to analysis of mathematical documents including ontology
We identify safe approximations of the background knowledge safe approximations of the background knowledge can be used to reduce the complexity of constructing secure knowledge bases views
We illustrate several novel attacks to the confidentiality of knowledge bases
We observe that the amount and quality of experiments are steadily increasing over time
We perform experiments
We present implementation details of a joint inference module
We present technologies taking full advantage of storing data in relational databases
We present the architecture taking full advantage of storing data in relational databases
We present the architecture underpinning the OBDA system Ontop
We present the Protege plugin NoHR
we present the results from a taskcentric empirical evaluation
We propose an approach as a principled approach for automatically learning topics from both structure information
We propose an approach as a principled approach for automatically learning topics from both textual information
We propose a novel method for entity consolidation
we proposed approach uses probabilistic soft logic framework which easily scales to millions of facts
we proposed a recently introduced probabilistic modeling framework which easily scales to millions of facts
We propose new methods to find the most relevant entity type based on collection statistics interconnecting entities
We propose new methods to find the most relevant entity type based on collection statistics interconnecting types
We propose new methods to find the most relevant entity type based on the graph structure interconnecting entities
We propose new methods to find the most relevant entity type based on the graph structure interconnecting types
We propose novel approaches for exploring the different types of synonyms
We propose novel approaches for extending the lexical components of several biomedical ontologiesThe Web of Data is a rich common resource with billions of triples available in thousands of individual Web documents created by both nonexpert ontologists
We propose the identified fragment
We prove the correctness of the heuristics
we refer to the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification
we refer to the task of inferring missing information
we refer to the task of removing noise
we report results comparable to the ones
We results suggest that We approaches to the computation of Hausdorff distances require two orders of magnitude less orthodromic distances computations to compare geographical data
we show how existing data sources can be described with completeness statements
we show that compared to existing methods we approach is able to achieve improved AUC and F1 with significantly lower running timeWe describe a method for updating the classification of ontologies
We show that the heuristics provides optimal results for a class of ontologies
We show that the minimization of TBoxes is intractable  NPcomplete 
We study confidentiality enforcement in ontologybased information systems where ontologies are expressed in OWL 2 RL
We test We algorithms on multiple realworld graph data setsResearch effort in ontology visualization has largely focused on developing new visualization techniques
we then focus on the problem of the completeness of query answering over plain data sources augmented with completeness statements
we then focus on the problem of the completeness of query answering over RDFS data sources augmented with completeness statements
we use three approaches to prune unnecessary counts for faster intersection computations
When a term that a user searches for is available in multiple ontologies how do Our rank the results
When a user searches a collection of ontologies for her terms of interest there are often dozens of ontologies
When it comes to publishing data on the web the level of access control is highly dependent on the type of content
While incremental classification modulo additions is relatively straightforward handling deletions is more problematic since incremental classification modulo additions requires retracting logical consequences
While the combined approach to query answering in the presence of ontologies is efficient for ontologies the combined approach to query answering in the presence of ontologies incurs an exponential blowup during query rewriting
While the combined approach to query answering in the presence of ontologies is scalable for ontologies the combined approach to query answering in the presence of ontologies incurs an exponential blowup during query rewriting
While this looks like a bad news result We also provide a heuristic technique for minimizing TBoxes
With hundreds of ontologies available today in many different domains ontology search has become an important problem
With hundreds of ontologies available today in many different domains ontology search has become an timely problem
With hundreds of ontologies available today in many different domains ranking has become an important problem
With hundreds of ontologies available today in many different domains ranking has become an timely problem
With the advent of SPARQL 11 the linked data infrastructure can a means of publishing open data but also as a general mechanism for managing distributed graph data
With the advent of SPARQL 11 the linked data infrastructure can be used
With the resulting plugin even queries to very large ontologies such as SNOMED CT augmented with a large number of rules can be processed at an interactive response time after one initial brief preprocessing period
With thousands of ontologies available today in many different domains ontology search has become an timely problem
With thousands of ontologies available today in many different domains ranking has become an important problem
With thousands of ontologies available today in many different domains ranking has become an timely problem
writing these semantic descriptions by hand is a tedious task
writing these semantic descriptions by hand is a timeconsuming task
Yet so far little attention has been paid to the characteristics of geospatial data within the context of link discovery
Yet so far only little attention has been paid to the effect of duplicated data on federated querying
14 different languages for which mappings were not yet available
500 papers collected from the International Semantic Web Conferences over the past decade
