we illustrate several novel attacks to the confidentiality of knowledge bases
then we introduce a new confidentiality model sensitive enough to detect several novel attacks and a method for constructing secure knowledge bases views
the background knowledge exploited in the attacks
we identify safe approximations of the background knowledge safe approximations of the background knowledge can be used to reduce the complexity of constructing secure knowledge bases views
the background knowledge exploited in several novel attackstextrich structured data become on the enterprise databases by encoding heterogeneous structural information between the associated textual information
textrich structured data become more ubiquitous on the web databases by encoding heterogeneous structural information between entities such as people
textrich structured data become more ubiquitous on the web databases by encoding heterogeneous structural information between entities such as locations
textrich structured data become more ubiquitous on the web databases by encoding heterogeneous structural information between the associated textual information
textrich structured data become on the enterprise databases by encoding heterogeneous structural information between entities such as people
textrich structured data become more ubiquitous on the web databases by encoding heterogeneous structural information between the associated textual information
textrich structured data become more ubiquitous on the web databases by encoding heterogeneous structural information between entities such as organizations
textrich structured data become on the enterprise databases by encoding heterogeneous structural information between entities such as locations
textrich structured data become more ubiquitous on the web databases by encoding heterogeneous structural information between entities such as people
textrich structured data become more ubiquitous on the web databases by encoding heterogeneous structural information between entities such as locations
textrich structured data become on the enterprise databases by encoding heterogeneous structural information between entities such as organizations
textrich structured data become more ubiquitous on the web databases by encoding heterogeneous structural information between entities such as organizations
for analyzing this type of data existing topic modeling approaches require manuallydefined regularization terms to exploit
existing topic modeling approaches which are highly tailored toward document collections
the topic learning towards structure information
for analyzing this type of data existing topic modeling approaches require manuallydefined regularization terms to to bias the topic
we propose an approach as a principled approach for automatically learning topics from both textual information
an approach called topical relational model
we propose an approach as a principled approach for automatically learning topics from both structure information
a stateoftheart approach that requires manuallytuned regularization
using a topic model we can show that we approach is effective in exploiting heterogeneous structure information outperforming a stateoftheart approachthe lightweight description logic el which forms a basis of some large ontologies like snomed
the lightweight description logic el which forms a basis of some large ontologies like gene ontology
we discuss the problem of minimizing tboxes
tboxes expressed in the lightweight description logic el
the lightweight description logic el which forms a basis of some large ontologies like nci
the lightweight description logic el which forms a basis of some large ontologies like galen
we show that the minimization of tboxes is intractable  npcomplete 
while this looks like a bad news result we also provide a heuristic technique for minimizing tboxes
we show that the heuristics provides optimal results for a class of ontologies
we prove the correctness of the heuristics
ontologies which we define through an acyclicity constraint over a reference relation between equivalence classes of concepts
to establish the feasibility of our approach our evaluated the algorithm effectiveness on a small suite of benchmarks
to establish the feasibility of our approach our have implemented the algorithmthe basic idea of the combined approach to query answering in the presence of ontologies is to materialize the consequences of the ontology in the data and then use a limited form of query rewriting to deal with infinite materializations
while the combined approach to query answering in the presence of ontologies is scalable for ontologies the combined approach to query answering in the presence of ontologies incurs an exponential blowup during query rewriting
query rewriting when dllite is extended with the popular role hierarchies
ontologies that are formulated in the basic version of the description logic dllite
while the combined approach to query answering in the presence of ontologies is efficient for ontologies the combined approach to query answering in the presence of ontologies incurs an exponential blowup during query rewriting
in this paper we show how to replace the query rewriting with a filtering technique
this is natural from an implementation perspective
this allows us to handle role hierarchies without an exponential blowup
we also carry out an experimental evaluation
an experimental evaluation that demonstrates the scalability of this approachin our previous work our showed how a scalable owl 2 rl reasoner can be used to compute both upper bound query answers over arbitrary owl 2 ontologies
in our previous work our showed how a scalable owl 2 rl reasoner can be used to compute both lower bound query answers over very large datasets
in our previous work our showed how a scalable owl 2 rl reasoner can be used to compute both lower bound query answers over arbitrary owl 2 ontologies
in our previous work our showed how a scalable owl 2 rl reasoner can be used to compute both upper bound query answers over very large datasets
however when these bounds do not coincide there still remain a number of possible answer tuples
possible answer tuples whose status is not determined
our show how in the case of horn ontologies one can exploit the lower
in this paper our show computed by a scalable owl 2 rl reasoner to efficiently identify a subset of the data and ontology
the data and ontology that is large enough to resolve the status of these tuples yet small enough so that the status of these tuples can be computed using a fullyfledged owl 2 reasoner
our show how in the case of horn ontologies one can exploit upper bounds
the resulting hybrid approach has enabled us to compute exact answers to queries over ontologies where previously only approximate query answering was possible
the resulting hybrid approach has enabled us to compute exact answers to queries over datasets where previously only approximate query answering was possibleover the last years the web of data has developed into a large compendium of interlinked data sets from multiple domains
due to the decentralised architecture of this compendium several of the web of data contain duplicated data
yet so far only little attention has been paid to the effect of duplicated data on federated querying
this work presents a novel duplicateaware approach to federated querying over the web of data
this work presents daw 
a novel duplicateaware approach to federated querying over the web of data is based on a combination of minwise independent permutations
daw is based on a combination of compact data summaries
a novel duplicateaware approach to federated querying over the web of data is based on a combination of compact data summaries
daw is based on a combination of minwise independent permutations
it can be directly combined with existing federated query engines in order to achieve the same query recall values while querying fewer data sources
we extend three wellknown federated query processing engines splendid with daw
we compare we extensions with the original approaches
we extend three wellknown federated query processing engines darq with daw
we extend three wellknown federated query processing engines fedx with daw
the comparison shows that a novel duplicateaware approach to federated querying over the web of data can greatly reduce the number of queries while keeping high query recall values
the comparison shows that daw can greatly reduce the number of queries while keeping high query recall values
queries sent to the endpoints
therefore it can significantly improve the performance of federated query processing engines
moreover a novel duplicateaware approach to federated querying over the web of data provides a source selection mechanism
a source selection mechanism that maximises the query recall
moreover daw provides a source selection mechanism
the query recall when the query processing is limited to a subset of the sourceslinked stream data extends the linked data paradigm to dynamic data sources
linked stream data enables the joint processing of heterogeneous stream data with quasistatic data from the linked data cloud in nearrealtime
linked stream data enables the integration processing of heterogeneous stream data with quasistatic data from the linked data cloud in nearrealtime
stream update frequencies
several needs to be in improved in terms of dynamic data sizes number of concurrent queries
several linked stream data processing engines exist but several scalability still
several needs to be in improved in terms of static data sizes number of concurrent queries
several linked stream data processing engines
several linked stream data processing engines
so far none of several supports parallel processing in the linked data cloud in nearrealtime
to remedy these limitations this paper presents an approach for elastically parallelizing the continuous execution of queries over linked stream data
for this we have developed novel highly efficient for continuous query operators
for this we have developed scalable parallel algorithms for continuous query operators
algorithms are implemented in we cqels cloud system and we present extensive evaluations of our approach and algorithms superior performance on amazon ec2 demonstrating excellent elasticity in a real deployment
we approach
algorithms are implemented in we cqels cloud system and we present extensive evaluations of our approach and algorithms superior performance on amazon ec2 demonstrating high scalability in a real deploymentdata coming from heterogeneous distributed sources
in the domain of linked open data a need is emerging for developing automated frameworks able to generate the licensing terms
the licensing terms associated to data
this paper proposes a deontic logic semantics
this paper evaluates a deontic logic semantics
a deontic logic semantics which allows us to define the deontic components of obligations
a deontic logic semantics which allows us to generate a composite license compliant with the licensing items of the composed different licenses
a deontic logic semantics which allows us to define the deontic components of the licenses
a deontic logic semantics which allows us to define the deontic components of permissions
a deontic logic semantics which allows us to define the deontic components of prohibitions
some heuristics are proposed to support the data publisher in choosing the licenses composition strategy which better suits her needs wrt the data her is publishingtool development for require a wide variety of suitable ontologies as detailed characterisations of real ontologies
empirical experimentation in owl ontology engineering require a wide variety of suitable ontologies as input for evaluation purposes
tool development for require a wide variety of suitable ontologies as input for evaluation purposes
tool development for require a wide variety of suitable ontologies as input for testing purposes
empirical experimentation in owl ontology engineering require a wide variety of suitable ontologies as detailed characterisations of real ontologies
empirical experimentation in owl ontology engineering require a wide variety of suitable ontologies as input for testing purposes
empirical activities often resort to  somewhat arbitrarily  hand curated corpora such as the ncbo bioportal
empirical activities often resort to  somewhat arbitrarily  hand curated corpora such as the tones repository
empirical activities often resort to  somewhat arbitrarily  hand curated corpora manually selected sets of wellknown ontologies
empirical activities often resort to  somewhat arbitrarily  hand curated corpora available on the web
findings of results of benchmarking activities may be biased even heavily towards these datasets
findings of surveys of benchmarking activities may be biased even heavily towards these datasets
sampling from a large corpus of ontologies on the other hand may lead to more representative results
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of variants to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of variants to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of variants to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of facets to random sampling
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of variants to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of facets to random sampling
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of ontology versions to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of facets to random sampling
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of facets to random sampling
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of ontology versions to random sampling
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of facets to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of facets and therefore do not lend large numbers of ontology versions to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of ontology versions to random sampling
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of variants to random sampling
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of facets to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of variants and therefore do not lend large numbers of ontology versions to random sampling
current large scale repositories crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of ontology versions to random sampling
web crawls are mostly uncurated and suffer from duplication small and  for many purposes  uninteresting ontology files and contain large numbers of ontology versions and therefore do not lend large numbers of variants to random sampling
in this paper we survey ontologies as large numbers of facets describe the creation of a corpus of owl dl ontologies using strategies such as web crawling various forms of manual cleaning
in this paper we survey ontologies as large numbers of ontology versions describe the creation of a corpus of owl dl ontologies using strategies such as web crawling various forms of manual cleaning
in this paper we survey ontologies as large numbers of variants describe the creation of a corpus of owl dl ontologies using strategies such as web crawling various forms of manual cleaning
various forms of deduplications which allows random sampling of ontologies for a variety of empirical applications
in this paper we survey ontologies as large numbers of ontology versions exist on the web
in this paper we survey ontologies as large numbers of facets describe the creation of a corpus of owl dl ontologies using strategies such as web crawling various forms of deduplications
in this paper we survey ontologies as large numbers of ontology versions describe the creation of a corpus of owl dl ontologies using strategies such as web crawling various forms of deduplications
in this paper we survey ontologies as large numbers of variants describe the creation of a corpus of owl dl ontologies using strategies such as web crawling various forms of deduplications
in this paper we survey ontologies as large numbers of facets exist on the web
various forms of manual cleaning which allows random sampling of ontologies for a variety of empirical applications
in this paper we survey ontologies as large numbers of variants exist on the webextending large repositories of formalized knowledge
webscale relation extraction is a means for building
precision which is hard to achieve with automatically acquired rule sets
this type of automated knowledge building requires a decent level of precision learned from unlabeled data by means of minimal supervision
this type of automated knowledge building requires a decent level of precision learned from unlabeled data by means of distant supervision
this paper shows how precision of relation extraction can be considerably improved by employing generalpurpose lexical semantic network for effective semantic rule
this paper shows how precision of relation extraction can be considerably improved by employing a widecoverage for effective semantic rule
effective semantic rule filtering
this paper shows how precision of relation extraction can be considerably improved by employing babelnet for effective semantic rule
we apply word sense disambiguation to the content words of the automatically extracted rules
as a result a set of relationspecific relevant concepts is obtained
each of relationspecific relevant concepts is then used to represent the structured semantics of the corresponding relation
the resulting relationspecific subgraphs of babelnet are used as semantic filters for estimating the adequacy of the extracted rules
the seven semantic relations tested here
for the seven semantic relations the semantic filter consistently yields a higher precision at any relative recall value in the highrecall rangethe vision behind the web of data is to extend the current documentoriented web with machinereadable facts thus creating a representation of general knowledge
the vision behind the web of data is to extend the current documentoriented web with structured data thus creating a representation of general knowledge
encyclopedic knowledge describing entities
however most of the web of data is limited to being a large compendium of encyclopedic knowledge
a huge challenge the massive extraction of rdf facts from unstructured data has remained open so far
a huge challenge the timely extraction of rdf facts from unstructured data has remained open so far
the availability of such knowledge on the web of data would provide significant benefits to manifold
business intelligence
applications
the availability of such knowledge on the web of data would provide significant benefits to manifold applications
sentiment analysis
news retrieval
an approach that allows extracting rdf triples from unstructured data streams
in this paper we address the problem of the actuality of the web of data by presenting an approach
we employ statistical methods in supervised machine learning techniques to create a knowledge base
we employ statistical methods in disambiguation to create a knowledge base
a knowledge base that reflects the content of the input streams
we employ statistical methods in unsupervised machine learning techniques to create a knowledge base
we employ statistical methods in combination with deduplication to create a knowledge base
we evaluate a sample of the rdf we generate against a large corpus of news streams
we evaluate a sample of the rdf we show that we achieve a precision of more than 85though subgraph matching has been extensively studied as a query paradigm in social network data environments a user can get a large number of answers in response to a query
though subgraph matching has been extensively studied as a query paradigm in semantic web a user can get a large number of answers in response to a query
just like google does these answers can be shown to the user in accordance with an importance ranking
sparqlqueries denoted as importance queries
in this paper we present scalable algorithms to find the topk answers to a practically important subset of sparqlqueries via a suite of pruning techniques
algorithms on multiple realworld graph data sets showing that we algorithms are efficient even on networks with up to 6m vertices than popular triple stores
algorithms on multiple realworld graph data sets showing that we algorithms are efficient even on networks with up to 15m edges than popular triple stores
algorithms on multiple realworld graph data sets showing that we algorithms are far more efficient than popular triple stores
we test we algorithms on multiple realworld graph data setsresearch effort in ontology visualization has largely focused on developing new visualization techniques
at the same time researchers have paid less attention to investigating the usability of common visualization techniques that many practitioners regularly use to visualize ontological data
in this paper we focus on two popular ontology visualization techniques graph
in this paper we focus on two popular ontology visualization techniques indented tree
we conduct a controlled usability study with an emphasis on the effectiveness efficiency workload and satisfaction of two popular ontology visualization techniques indented tree and graph in the context of assisting users during evaluation of ontology mappings
findings from a controlled usability study with an emphasis on the effectiveness efficiency workload and satisfaction of these visualization techniques have revealed both strengths and weaknesses of each visualization technique
in particular while the indented tree visualization is more organized and familiar to novice users subjects found the graph visualization to be more controllable and intuitive without visual redundancy particularly for ontologies with multiple inheritanceto bring the life sciences domain closer to a semantic web realization it is fundamental to establish meaningful relations between biomedical ontologies
diverse biomedical terminology contained in biomedical ontologies
the successful application of ontology matching techniques is strongly tied to an effective exploration of the complex
in this paper we present an overview of the lexical components of several biomedical ontologies
in this paper we investigate how different approaches for the lexical components of several biomedical ontologies use can impact the performance of ontology matching techniques
we propose novel approaches for extending the lexical components of several biomedical ontologies
synonyms encoded by several biomedical ontologies
we propose novel approaches for exploring the different types of synonyms
several biomedical ontologies based both on internal synonym derivation and on external ontologiesthe web of data is a rich common resource with billions of triples available in thousands of individual web documents created by both expert
the web of data is a rich common resource with billions of triples available in thousands of datasets created by both nonexpert ontologists
the web of data is a rich common resource with billions of triples available in thousands of individual web documents created by both nonexpert ontologists
the web of data is a rich common resource with billions of triples available in thousands of datasets created by both expert
a common problem is the imprecision in the use of vocabularies annotators may not be able to find the right objects to annotate with
a common problem is the imprecision in the use of vocabularies annotators can misunderstand the semantics of a class or property
this paper may eventually hamper this paper usability over large scale
this paper decreases the quality of data
this paper describes statistical knowledge patterns  as a means to address this issue
this paper describes skp  as a means to address this issue
skps are automatically generated based on statistical data analysis
skps encapsulate key information about ontology classes including synonymous properties in datasets
skps can be effectively used to increase recall in querying
skps can be effectively used to automatically normalise data
both pattern extraction are completely automated
both pattern usage are completely automated
the main benefits of skps are that the main benefits of skps structure allows for both accurate query expansion and restriction the main benefits of skps are context dependent hence the main benefits of skps describe the usage and meaning of properties in the context of a particular class hence the equivalence among relations can be used efficiently at run time
the main benefits of skps are that the main benefits of skps structure allows for both accurate query expansion and restriction the main benefits of skps are context dependent the main benefits of skps can be generated offline hence the equivalence among relations can be used efficiently at run timeone of the main advantages of using semantically annotated data is that machines can reason on one of the main advantages of using semantically annotated data deriving implicit knowledge from explicit information
in this context materializing every possible implicit derivation from a given input can be computationally expensive especially when considering large data volumesontology engineering is a task
a task that is notorious for ontology engineering difficulty
the group that developed the most widely used ontology editor
as the group we are keenly aware of how difficult the users perceive this task to be
the group that developed protege
a tool that will be easy to use while still accounting for commonly used owl constructs social interaction around distributed ontology editing as part of the core tool design
a tool that will be easy to use while still accounting for commonly used owl constructs support collaboration around distributed ontology editing as part of the core tool design
in this paper we present the new version of webprotege that we designed with two main goals in mind create a tool
we designed this new version of the webprotege user interface empirically by analysing the use of owl constructs in a large corpus of publicly available ontologies
since the beta release of this new webprotege interface in january 2013 we users from around the world have uploaded 519 ontologies on we server
since the beta release of this new webprotege interface in january 2013 we users from around the world have created 519 ontologies on we server
in this paper
we empirical design approach
we describe the key features of the new tool
we evaluate language coverage in webprotege by assessing how well webprotege covers the owl constructs
the owl constructs that are present in ontologies that users have uploaded to webprotege
we evaluate the usability of webprotege through a usability survey
we analysis validates we empirical design demonstrates that an easytouse webbased tool is sufficient for many users to start editing many users ontologies
an easytouse webbased tool that covers most of the frequently used owl constructs
we analysis validates we empirical design suggests additional language constructors to exploretype information is very valuable in knowledge bases
however most large open knowledge bases contain incorrect data
however most large open knowledge bases are incomplete with respect to type information
however most large open knowledge bases contain noisy data
that makes classic type inference by reasoning difficult
in this paper we propose the heuristic linkbased type inference mechanism sdtype
the heuristic linkbased type inference mechanism sdtype which can handle incorrect data
the heuristic linkbased type inference mechanism sdtype which can handle noisy data
instead of leveraging tbox information from the schema sdtype takes the actual use of a schema into account
instead of leveraging tbox information from the schema sdtype is also robust to misused schema elementsinstances that use particular ontological terms
the contextual tag cloud system which can execute large volumes of queries about the number of instances
in this paper we present the infrastructure of the contextual tag cloud system
tags that defines a subset of instances
the font sizes reflect the number of instances
the contextual tag cloud system is a novel application
a novel application that helps users explore a large scale rdf dataset
the tags are ontological terms 
the context is a set of tags
the tags are classes 
instances that use each tag
the tags are properties 
the contextual tag cloud system visualizes the patterns of instances
instances specified by the context a user constructs
given a request with a specific context the contextual tag cloud system needs to quickly find what how many instances in the context use each tag
given a request with a specific context the contextual tag cloud system needs to quickly find what other tags the instances in the context use instances in the context use each tag
the key question we answer in this paper is how to scale to linked data in particular we use a dataset with 14 billion triples
the key question we answer in this paper is how to scale to linked data in particular we use a dataset over 380000 tags
this is complicated by the fact that the calculation should when directed by the user consider the entailment of taxonomic andor domainrange axioms in the ontology
we use three approaches to prune unnecessary counts for faster intersection computations
we combine a scalable preprocessing approach with a speciallyconstructed inverted index
we compare we system with a stateoftheart triple store examine how pruning rules interact with inference
we compare we system with a stateoftheart triple store analyze we design choicesautomation of service composition is one of the most interesting challenges
the most interesting challenges facing the semantic web today
the most interesting challenges facing the web of services today
despite approaches services data flow remains implicit and difficult to be automatically generated
approaches which are able to infer a partial order of services
enhanced with formal representations the semantic links between input parameters of services can be then exploited to infer services data flow
enhanced with formal representations the semantic links between output parameters of services can be then exploited to infer services data flow
this work addresses the problem of effectively inferring data flow between services
services based on this work representations
to this end we introduce the non standard description logic reasoning join aiming to provide a  constructive evidence  of why services can be connected and how non trivial links  can be inferred in data flow
to this end we introduce the non standard description logic reasoning join aiming to provide a  constructive evidence  of why services can be connected and how many to many parameters  can be inferred in data flow
the preliminary evaluation provides evidence in favor of we approach regarding the completeness of data flowwith hundreds of ontologies available today in many different domains ontology search has become an important problem
with thousands of ontologies available today in many different domains ontology search has become an important problem
with thousands of ontologies available today in many different domains ontology search has become an timely problem
with thousands of ontologies available today in many different domains ranking has become an timely problem
with hundreds of ontologies available today in many different domains ranking has become an timely problem
with hundreds of ontologies available today in many different domains ranking has become an important problem
with thousands of ontologies available today in many different domains ranking has become an important problem
with hundreds of ontologies available today in many different domains ontology search has become an timely problem
ontologies that contain her terms of interest
when a user searches a collection of ontologies for her terms of interest there are often dozens of ontologies
how does her know which ontology is the most relevant to her search
our research group hosts bioportal 
our research group hosts a public repository of more than 330 ontologies in the biomedical domain
when a term that a user searches for is available in multiple ontologies how do our rank the results
how do our measure how well our ranking works
in this paper our develop an evaluation framework
an evaluation framework that enables developers to analyze the performance of different ontologyranking methods
an evaluation framework that enables developers to compare the performance of different ontologyranking methods
our framework is based on processing search logs
our framework is determining how often users select the top link that the search engine offers
our evaluate our framework by analyzing the data on bioportal searches
our explore several different ranking algorithms
our measure the effectiveness of each ranking by measuring how often users click on the highest ranked ontology
our collected log data from more than 4800 bioportal searches
our results show that regardless of the ranking in more than half the searches users select the first link
thus it is even more critical to ensure that the ranking is appropriate if our want to have satisfied users
our further analysis demonstrates that ranking ontologies based on page view data significantly improves the user experience with an approximately 26we study confidentiality enforcement in ontologybased information systems where ontologies are expressed in a profile of owl 2
we study confidentiality enforcement in ontologybased information systems where ontologies are expressed in owl 2 rl
owl 2 that is becoming increasingly popular in semantic web applications
we formalise a natural adaptation of the controlled query evaluation  cqe  framework to ontologies
cqe algorithms that ensure confidentiality of sensitive information are efficiently implementable by means of rdf triple store technologies
we goal is to provide cqe algorithms
we goal is to ensure all three requirements
we formally can not be satisfied without imposing restrictions on ontologies
we formally show that all three requirements are in conflict
we propose the identified fragment
for the identified fragment we design a cqe algorithm
a cqe algorithm that has the same computational complexity as standard query answering
a cqe algorithm that can be implemented by relying on stateoftheart triple storesnowadays search on the web increasingly takes advantage of the growing amount of structured data
nowadays search on the web goes beyond the retrieval of textual web sites
entity search where the units of retrieval are structured these entities
of particular interest is entity search
these entities are therefore called  uncooperative 
different sources which may provide only limited information about these entities content
these entities reside in different sources
further different sources which may provide only limited information about their content capture complementary but also redundant information about entities
in this environment of uncooperative data sources we study the problem of federated entity search
entity consolidation performed at query time
federated entity search where redundant information about entities is reduced onthefly through entity consolidation
entity consolidation that is based on using language models hence more suitable for this onthefly uncooperative setting than stateoftheart methods
entity consolidation that is completely unsupervised hence more suitable for this onthefly uncooperative setting than stateoftheart methods
we propose a novel method for entity consolidation
stateoftheart methods that require training data
further we apply the same language model technique to deal with the federated search problem of ranking results returned from different sources
particular novel are the mechanisms we propose to incorporate consolidation results into this ranking
experiments using real web queries
experiments using data sources
we perform experiments
we experiments show that we approach for federated entity search with onthefly consolidation improves upon the performance of a stateoftheart preference aggregation baseline and also benefits from consolidationcombining structured queries with fulltext search provides a powerful means to access
access distributed linked data
however executing hybrid search queries in a federation of multiple data sources presents a number of challenges due to data source heterogeneity and lack of statistical data about keyword selectivity
a novel hybrid query engine based on the sparql federation framework fedx
to address these challenges we present a novel hybrid query engine
to address these challenges we present fedsearch
we extend the sparql algebra to incorporate keyword search clauses as firstclass citizens
we extend the sparql algebra to apply novel optimization techniques to improve the query processing efficiency while maintaining a meaningful ranking of results
by performing intelligent grouping of query clauses we are able to reduce significantly the communication costs making we approach suitable for topk hybrid search across multiple data sources
by performing onthefly adaptation of the query execution plan we are able to reduce significantly the communication costs making we approach suitable for topk hybrid search across multiple data sources
in experiments we demonstrate that we optimization techniques can lead to a substantial performance improvement reducing the execution time of hybrid queries by more than an order of magnitudethe web covering disparate
with thousands of rdf data sources available on the web and possibly overlapping knowledge domains the problem of providing highlevel descriptions  in the form of metadata  of thousands of rdf data content becomes crucial
in this paper we introduce a theoretical framework for describing data sources in terms of data sources completeness
completeness statements expressed in rdf
we show how existing data sources can be described with completeness statements
we then focus on the problem of the completeness of query answering over plain data sources augmented with completeness statements
we then focus on the problem of the completeness of query answering over rdfs data sources augmented with completeness statements
finally we present an extension of the completeness framework for federated data sourcesalthough an increasing number of rdf knowledge bases are published many of those lack sophisticated schemata
although an increasing number of rdf knowledge bases are published many of those consist primarily of instance data
having such schemata allows consistency checking
having such schemata allows debugging as well as improved inference
having such schemata allows more powerful querying
the reasons why schemata are still rare
one of the reasons is the effort
the effort required to create one of the reasons why schemata are still rare
a semiautomatic schemata construction approach addressing this problem the frequency of axiom patterns in existing knowledge bases
a semiautomatic schemata construction approach addressing this problem first in existing knowledge bases
in this article we propose a semiautomatic schemata construction approach is discovered
afterwards those patterns are converted to sparql based pattern detection algorithms
pattern detection algorithms which allow to enrich knowledge base schemata
we argue that we present the first scalable knowledge base enrichment approach based on real schema usage patterns
the first scalable knowledge base enrichment approach is evaluated on a large set of knowledge bases with a quantitative
the first scalable knowledge base enrichment approach is evaluated on a large set of knowledge bases with qualitative result analysisfor ontology reuse and integration a number of approaches have been devised that aim at identifying small sets of  relevant  axioms from ontologies
for ontology reuse and integration a number of approaches have been devised that aim at identifying modules of  relevant  axioms from ontologies
here we consider three logically sound notions of modules mex modules only applicable to inexpressive ontologies these modules a sound approximation of the first polynomial in the size of the ontology
modules based on syntactic locality a sound approximation of and thus the first  widely used since these modules can be extracted from owl dl ontologies in time
here we consider modules polynomial in the size of the ontology
modules based on syntactic locality a sound approximation of the second  widely used since these modules can be extracted from owl dl ontologies in timethe semantic web has matured from a mere theoretical vision to a variety of readytouse
readytouse linked open data sources currently available on the semantic web
still with respect to application development the web community is just starting to develop new paradigms
new paradigms in which data as the main driver of applications is promoted to first class status
relying on properties of resources as an indicator for the type propertybased typing is such a paradigm
in this paper we inspect the feasibility of propertybased typing for accessing data from the linked open data cloud
these problems were noticeable
to alleviate these problems we developed an iterative approach
an iterative approach that builds on human feedbackmuch of web search is today centered around entities
much of browsing activity is today centered around entities
for this reason search engine result pages increasingly contain information about the searched entities such as factual information
for this reason search engine result pages increasingly contain information about the searched entities such as pictures
for this reason search engine result pages increasingly contain information about the searched entities such as related entities
for this reason search engine result pages increasingly contain information about the searched entities such as short summaries
a key facet is instrumental for many applications is the entity type
a key facet that is often displayed on the engine result pages and that
however an entity is usually not associated to a single generic type in the background knowledge bases or not given the document context
however an entity is usually not associated to a set of more specific types or not given the document context
more specific types which may be relevant
for example one can find on the linked open data cloud the fact that tom hanks is a person
for example one can find on the linked open data cloud the fact that tom hanks is a person from concord california
for example one can find on the linked open data cloud the fact that tom hanks is an actor
some may be too general to be interesting  while other may be already known to the user 
some may be too general to be interesting  while other may be interesting
some may be too general to be person  while other may be already known to actor 
some may be too general to be person  while other may be irrelevant given the current browsing context 
some may be too general to be interesting  while other may be irrelevant given the current browsing context 
some may be too general to be person  while other may be irrelevant given person from concord california concord california 
some may be too general to be interesting  while other may be irrelevant given person from concord california concord california 
some may be too general to be interesting  while other may be already known to actor 
all those types are correct
some may be too general to be person  while other may be already known to the user 
some may be too general to be person  while other may be interesting
in this paper we define the new task of ranking entity types given an entity context
in this paper we define the new task of ranking entity types given an entity
we propose new methods to find the most relevant entity type based on collection statistics interconnecting types
we propose new methods to find the most relevant entity type based on the graph structure interconnecting types
we evaluate new methods to find the most relevant entity type based on collection statistics interconnecting types
we evaluate new methods to find the most relevant entity type based on the graph structure interconnecting types
we evaluate new methods to find the most relevant entity type based on collection statistics interconnecting entities
we propose new methods to find the most relevant entity type based on the graph structure interconnecting entities
we evaluate new methods to find the most relevant entity type based on the graph structure interconnecting entities
we propose new methods to find the most relevant entity type based on collection statistics interconnecting entities
an extensive experimental evaluation over several document collections at different levels of different type hierarchies  including freebase  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an extensive experimental evaluation over several document collections at different levels of granularity   including freebase  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an extensive experimental evaluation over several document collections at different levels of sentences paragraphs   including freebase  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an extensive experimental evaluation over several document collections at different levels of granularity   including schemaorg  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an extensive experimental evaluation over several document collections at different levels of sentences paragraphs   including schemaorg  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an extensive experimental evaluation over several document collections at different levels of different type hierarchies  including dbpedia  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an extensive experimental evaluation over several document collections at different levels of sentences paragraphs   including dbpedia  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an extensive experimental evaluation over several document collections at different levels of different type hierarchies  including schemaorg  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalable
an extensive experimental evaluation over several document collections at different levels of granularity   including dbpedia  shows that hierarchybased approaches provide more accurate results when picking entity types to be displayed to the enduser while still being highly scalablesemantic models of data sources provide support to automate many tasks such as source discovery
semantic models of services provide support to automate many tasks such as data integration
writing these semantic descriptions by hand is a timeconsuming task
writing these semantic descriptions by hand is a tedious task
semantic models of data sources provide support to automate many tasks such as service composition
semantic models of services provide support to automate many tasks such as source discovery
semantic models of data sources provide support to automate many tasks such as data integration
semantic models of services provide support to automate many tasks such as service composition
most of the related work focuses on automatic annotation with properties of source attributes
most of the related work focuses on automatic annotation with classes of source attributes
most of the related work focuses on automatic annotation with properties of output parameters
most of the related work focuses on automatic annotation with properties of input
most of the related work focuses on automatic annotation with classes of input
most of the related work focuses on automatic annotation with classes of output parameters
a source model that includes the relationships between the attributes in addition to the attributes in addition semantic types
however constructing a source model remains a largely unsolved problem
known sources that have been modeled over the same domain ontology
in this paper we present a graphbased approach to hypothesize a rich semantic description of a new target source from a set of known sources
a graph that represents the space of plausible source descriptions
we exploit the known source models to build a graph
we exploit the same domain ontology to build a graph
then we compute the top k candidates and suggest to the user a ranked list of the semantic models for the new source
known sources that have been modeled over the same domain ontology
a graphbased approach to hypothesize a rich semantic description of a new target source from a set of known sources takes into account user corrections to learn more accurate semantic descriptions of future data sources
the art system that does not learn from prior models
our evaluation shows that our method produces models
models that are twice as accurate than the models
the models produced using a state of the art systemthe semantic web makes an extensive use of the owl dl ontology language underlied by the shoiq description logic to formalize the semantic web resources
this logic extended with the transitive closure of roles in concept axioms
in this paper we propose a decision procedure for this logic
a feature needed in several application domains
this logic extended with the transitive closure of roles in a feature
the most challenging issue we have to deal with when designing such a decision procedure is to represent infinitely nontreeshaped models
infinitely nontreeshaped models which are different from those of shoiq ontologies
models which may have an infinite nontreeshaped part
a new blocking condition for characterizing models
to address the most challenging issue we have to deal with when designing such a decision procedure we introduce a newunfortunately transforming these candidate facts into useful knowledge is a formidable challenge
largescale information processing systems are able to extract massive collections of these candidate facts
in this paper we show how uncertain extractions about entities relations can be transformed into a knowledge graph
in this paper we show how uncertain extractions about entities relations can be transformed into a knowledge graph
we refer to the task of removing noise
uncertain extractions about entities form an extraction graph
we refer to the task of inferring missing information
uncertain extractions about their relations form an extraction graph
we refer to the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification
in order to perform the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification we must identify coreferent entities
in order to perform the task of inferring missing information we must reason jointly about candidate facts
in order to perform the task of inferring missing information we must identify coreferent entities
in order to perform the task of inferring missing information we must reason jointly about candidate facts
in order to perform the task of removing noise we must identify coreferent entities
in order to perform the task of removing noise we must reason jointly about candidate facts
in order to perform the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification we must reason jointly about candidate facts
in order to perform the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification we must incorporate ontological constraints
in order to perform the task of removing noise we must reason jointly about candidate facts
in order to perform the task of determining which candidate facts should be included into a knowledge graph as knowledge graph identification we must reason jointly about candidate facts
candidate facts associated extraction confidences
in order to perform the task of removing noise we must incorporate ontological constraints
candidate facts associated extraction confidences
in order to perform the task of inferring missing information we must incorporate ontological constraints
we proposed approach uses probabilistic soft logic framework which easily scales to millions of facts
we proposed a recently introduced probabilistic modeling framework which easily scales to millions of facts
we demonstrate the power of we method on a synthetic linked data corpus
a synthetic linked data corpus derived from the musicbrainz music community from the nell project
the nell project containing over 1m extractions
the nell project containing over 70k ontological relations
a synthetic linked data corpus derived from a realworld set of extractions from the nell project
we show that compared to existing methods we approach is able to achieve improved auc and f1 with significantly lower running timewe describe a method for updating the classification of ontologies
ontologies expressed in the el family of description logics after some axioms have been deleted
ontologies expressed in the el family of description logics after some axioms have been added
logical consequences that are no longer valid
while incremental classification modulo additions is relatively straightforward handling deletions is more problematic since incremental classification modulo additions requires retracting logical consequences
this problem using various forms of bookkeeping to trace the consequences back to premises
known algorithms address this problem
but such additional data can consume memory
but such additional data place an extra burden on the reasoner during application of inferences
a technique which avoids this extra cost while being very efficient for small incremental changes in ontologies
in this paper we present a technique
a technique is freely available as a part of the opensource el reasoner elk efficiency is demonstrated on synthetic data
a technique which avoids this extra cost while being very efficient for small incremental changes in ontologies
a technique is freely available as a part of the opensource el reasoner elk is demonstrated on naturally occurring
a technique is freely available as a part of the opensource el reasoner elk is demonstrated on synthetic data
a technique is freely available as a part of the opensource el reasoner elk efficiency is demonstrated on naturally occurringenglish wikipedia infoboxes contain rich structured information of various entities sets
various entities which have been explored by dbpedias to generate large scale linked data
entities which are important for creating rdf links between dbpedias instances
those attributes having hyperlinks in dbpedias values
among all the infobox attributes those attributes identify semantic relations between entities
infoboxes which causes lots of relations between entities
lots of relations between entities being missing in english wikipedia
however quite a few hyperlinks have not been anotated by editors in infoboxes
in this paper we propose an approach for automatically discovering the missing entity links in english wikipedia infoboxes so that the missing semantic relations between entities can be established
we approach then computes several features to estimate the possibilities that a given attribute value might link to a candidate entity
we approach first identifies entity mentions in the given infoboxes
a learning model is used to obtain the weights of different features
a learning model predict the destination entity for each attribute value
we evaluated we approach on the english wikipedia data the experimental results show that we can effectively find the missing relations between entities and our approach significantly outperforms the baseline methods in terms of both precision and recallthe normative version of rdf schema gives nonstandard  intensional  interpretations to some standard notions such as properties thus departing from standard setbased semantics
the normative version of rdf schema gives nonstandard  intensional  interpretations to some standard notions such as classes thus departing from standard setbased semantics
in this paper we develop a standard setbased  extensional  semantics for the rdf schema vocabulary while preserving the simplicity complexity of deduction of the intensional version
in this paper we develop a standard setbased  extensional  semantics for the rdf schema vocabulary while preserving the computational complexity of deduction of the intensional version
this result can positively impact current implementations as reasoning in rdf schema can be compatible with owl extensions
this result can positively impact current implementations as reasoning in rdf schema can be implemented following common setbased intuitionsaccess control required  if any 
when it comes to publishing data on the web the level of access control is highly dependent on the type of content
content exposed
up until now rdf data publishers have focused on linking public data
up until now rdf data publishers have focused on exposing public data
with the advent of sparql 11 the linked data infrastructure can be used
with the advent of sparql 11 the linked data infrastructure can a means of publishing open data but also as a general mechanism for managing distributed graph data
however such a decentralised architecture brings with such a decentralised architecture a number of additional challenges with respect to both data security and integrity
a general authorisation framework that can be used to cater for the secure manipulation of linked data
in this paper we propose a general authorisation framework
a general authorisation framework that can be used to deliver dynamic query results based on user credentials
specifically we describe how graph patterns can together be used to enforce consistent access control policies
specifically we describe how conflict resolution policies can together be used to enforce consistent access control policies
specifically we describe how integrity constraints can together be used to specify consistent access control policies
specifically we describe how conflict resolution policies can together be used to specify consistent access control policies
specifically we describe how graph patterns can together be used to specify consistent access control policies
specifically we describe how integrity constraints can together be used to enforce consistent access control policies
specifically we describe how propagation rules can together be used to specify consistent access control policies
specifically we describe how propagation rules can together be used to enforce consistent access control policiesqodi is an automatic ontologybased data integration system
qodi is distinguished in that the ontology mapping algorithm dynamically determines a partial mapping specific to the reformulation of each query
each query provides application context not available in the ontologies alone thereby the system is able to disambiguate mappings for different queries
the ontology mapping algorithm compares the set of paths with a similar decomposition of a source ontology
the ontology mapping algorithm decomposes each query into a set of paths
using test sets from three real world applications qodi achieves favorable results compared with a leading ontology matcher
using test sets from three real world applications qodi achieves favorable results compared with an ontologybased implementation of the mapping methods
using test sets from three real world applications qodi achieves favorable results compared with agreementmaker
the mapping methods detailed for data exchange system
the mapping methods detailed for clio
the mapping methods detailed for the stateoftheart relational data integrationexperimentation is an important way to validate results of the semantic web in general
experimentation is an important way to validate results of computer science research in general
in this paper we investigate the development
in this paper we investigate the current status of experimental work on the semantic web
500 papers collected from the international semantic web conferences over the past decade
based on a corpus of 500 papers we analyse the quality of experimental research compare the importance to general computer science
based on a corpus of 500 papers we analyse the quality of experimental research conducted
based on a corpus of 500 papers we analyse the importance
we observe that the amount and quality of experiments are steadily increasing over time
the amount of experimental work reported
we can not confirm a statistically significant correlation between a papers citations
unlike hypothesised
our analysis however shows that papers are more often cited than other papers
papers comparing our analysis to other systemsthe protege plugin nohr that allows the user to add a set of logic programming  rules suitable to express exceptions
the protege plugin nohr that allows the user to add a set of nonmonotonic  rules suitable to express defaults
the protege plugin nohr that allows the user to add a set of logic programming  rules suitable to express defaults
the protege plugin nohr that allows the user to query the combined knowledge base
we present the protege plugin nohr
the protege plugin nohr that allows the user to take an el  op ontology
the protege plugin nohr that allows the user to add a set of nonmonotonic  rules suitable to express exceptions
we approach uses the wellfounded semantics for mknf knowledge bases as underlying formalism so no restriction other than dlsafety is imposed on the rules
the rules that can be written
the tool the tool builds on the procedure slg
rules whose result together with the nonmonotonic rules serve as input for the topdown querying engine xsb prolog
the tool the tool  with the help of owl 2 el reasoner elk preprocesses the ontology into rules
with the resulting plugin even queries to very large ontologies such as snomed ct augmented with a large number of rules can be processed at an interactive response time after one initial brief preprocessing period
at the same time we system is able to deal with an ontology
at the same time we system is able to deal with possible inconsistencies between the nonmonotonic rules
an ontology that alone is consistenta largescale knowledge base that exploits wikipedia as primary data source
dbpedia is a largescale knowledge base
the extraction procedure requires to manually map wikipedia infoboxes into the dbpedia ontology
thanks to a large number of infoboxes has been mapped in the english dbpedia
thanks to crowdsourcing has been mapped in the english dbpedia
consequently the same procedure has been applied to other languages to create the localized versions of dbpedia
however the number of accomplished mappings is still small to most frequent infoboxes
however the number of accomplished mappings is still limited to most frequent infoboxes
furthermore mappings need maintenance due to the constant changes of wikipedia articles
furthermore mappings need maintenance due to the quick changes of wikipedia articles
in this paper we focus on the problem of automatically mapping infobox attributes to properties into the dbpedia ontology for extending the coverage of building from scratch versions for languages not covered in the current version
in this paper we focus on the problem of automatically mapping infobox attributes to properties into the dbpedia ontology for extending the coverage of the existing localized versions
the evaluation has been performed on the italian mappings
we compared we results with the current mappings on a random sample reannotated by the authors
we report results comparable to the ones
we approach leads to a significant improvement in recall
we approach leads to a significant improvement in speed
the ones obtained by a human annotator in term of precision
specifically we mapped 45978 wikipedia infobox attributes to dbpedia properties in 14 different languages
14 different languages for which mappings were not yet available
the resource is made available in an open formatsensemaking tasks which go beyond standard search and ranking of publications
despite the large number and variety of services available today for exploring scholarly data current support is still very limited in the context of sensemaking tasks
despite the large number and variety of services available today for exploring scholarly data current support focus instead on performing finegrained academic expert search along multiple dimensions
despite the large number and variety of services available today for exploring scholarly data current support focus instead on understanding the dynamics of research areas
sensemaking tasks which go beyond standard search and ranking of authors
despite the large number and variety of tools available today for exploring scholarly data current support focus instead on performing finegrained academic expert search along multiple dimensions
despite the large number and variety of services available today for exploring scholarly data current support focus instead on relating authors semantically
despite the large number and variety of tools available today for exploring scholarly data current support is still very limited in the context of sensemaking tasks
despite the large number and variety of tools available today for exploring scholarly data current support focus instead on relating authors semantically
despite the large number and variety of tools available today for exploring scholarly data current support focus instead on understanding the dynamics of research areas
rexplore which integrates visual analytics to provide effective support for making sense of scholarly data
rexplore which integrates semantic technologies to provide effective support for making sense of scholarly data
to address this gap we have developed a novel tool rexplore
rexplore which integrates visual analytics to provide effective support for exploring sense of scholarly data
rexplore which integrates statistical analysis to provide effective support for making sense of scholarly data
rexplore which integrates statistical analysis to provide effective support for exploring sense of scholarly data
rexplore which integrates semantic technologies to provide effective support for exploring sense of scholarly data
rexplore which integrates statistical analysis to provide effective support for exploring sense of scholarly data
rexplore which integrates statistical analysis to provide effective support for making sense of scholarly data
we present the results from a taskcentric empirical evaluation
rexplore which integrates semantic technologies to provide effective support for exploring sense of scholarly data
here
rexplore which integrates visual analytics to provide effective support for making sense of scholarly data
rexplore which integrates semantic technologies to provide effective support for making sense of scholarly data
rexplore which integrates visual analytics to provide effective support for exploring sense of scholarly data
a taskcentric empirical evaluation which shows that rexplore is highly effective at providing support for the aforementioned sensemaking tasks
we describe the main innovative elements of a novel tool rexplore
in addition the results are robust both also with respect to whether the aforementioned sensemaking tasks are selected by the evaluators or proposed by the users the aforementioned sensemaking tasks
in addition the results are robust both with respect to the background of the users themselves to whether the aforementioned sensemaking tasks are selected by the evaluators or proposed by the users the aforementioned sensemaking taskswe describe work on automatically inferring the intended meaning of tables making the intended meaning of tables available for improving integration
we describe work on representing the intended meaning of tables as rdf linked data making the intended meaning of tables available for improving interoperability
we describe work on representing the intended meaning of tables as rdf linked data making the intended meaning of tables available for improving search
we describe work on automatically inferring the intended meaning of tables making the intended meaning of tables available for improving interoperability
we describe work on representing the intended meaning of tables as rdf linked data making the intended meaning of tables available for improving integration
we describe work on automatically inferring the intended meaning of tables making the intended meaning of tables available for improving search
a joint inference module that uses knowledge from the linked open data cloud to jointly infer the semantics of column headers
a joint inference module that uses knowledge from the linked open data cloud to jointly infer relations between columns
a joint inference module that uses knowledge from the linked open data cloud to jointly infer table cell values 
a joint inference module that uses knowledge from the linked open data cloud to jointly infer strings 
we present implementation details of a joint inference module
a joint inference module that uses knowledge from the linked open data cloud to jointly infer numbers 
we also implement a novel semantic message passing algorithm
a novel semantic message passing algorithm which uses the linked open data knowledge to improve existing message
existing message passing schemes
we evaluate we implemented techniques on tables from the web
we evaluate we implemented techniques on tables from wikipediawe present the architecture taking full advantage of storing data in relational databases
we present technologies underpinning the obda system ontop
we present technologies taking full advantage of storing data in relational databases
we present the architecture underpinning the obda system ontop
we discuss the theoretical foundations of ontop the treewitness query rewriting tmappings based on sql features
we discuss the theoretical foundations of ontop the treewitness query rewriting optimisations based on sql features
we discuss the theoretical foundations of ontop the treewitness query rewriting optimisations based on database integrity constraints
we discuss the theoretical foundations of ontop the treewitness query rewriting tmappings based on database integrity constraints
standard ontologies queries and data stored in relational databases
we demonstrate that for standard ontologies queries and data ontop is fast efficient
we demonstrate that for standard ontologies queries and data ontop is fast produces sql rewritings of high quality
we analyse the performance of ontop in a series of experimentswe present we work on developing a software platform for mining mathematical scholarly papers to obtain a linked data representation
currently the linking open data cloud lacks detailed information on professional level mathematics
currently the linking open data cloud lacks uptodate information on professional level mathematics
to we mind the main reason for that is the absence of appropriate tools
appropriate tools that could analyze the underlying semantics in mathematical papers and effectively build mathematical papers consolidated representation
ontology based extraction conversion of the article body into rdf integration with some semantic search
ontology based the article body metadata into rdf integration with some existing linking open data data sets search
ontology based extraction conversion of the article body into rdf integration with some existing linking open data data sets search
ontology based the article body metadata into rdf integration with some semantic search
we have developed a holistic approach to analysis of mathematical documents including ontology
we argue that the platform may be helpful for enriching user experience on modern online scientific collectionsthe discovery of links between resources within knowledge bases is of crucial importance to realize the vision of the semantic web
addressing this task is especially challenging when dealing with geospatial datasets due to the potential complexity of single geospatial objects
addressing this task is especially challenging when dealing with geospatial datasets due to this task sheer size of single geospatial objects
yet so far little attention has been paid to the characteristics of geospatial data within the context of link discovery
in this paper we address this gap by presenting a reductionratiooptimal link discovery approach
a reductionratiooptimal link discovery approach designed especially for geospatial data
in this paper we address this gap by presenting orchid
orchid relies on a combination of the hausdorff to compute the distance between geospatial objects
orchid relies on a combination of orthodromic metrics to compute the distance between geospatial objects
we first present two novel approaches for the efficient computation of hausdorff distances
then we present the space tiling approach implemented by orchid
then we present the space tiling approach prove that it is optimal with respect to the reduction ratio that orchid can achieve
the evaluation of we approaches is carried out on three real datasets of different size and complexity
we results suggest that we approaches to the computation of hausdorff distances require two orders of magnitude less orthodromic distances computations to compare geographical data
moreover our approaches to the computation of hausdorff distances require two orders of magnitude less time than a naive approach to achieve this goal
finally our results indicate that orchid scales to large datasets while outperforming the state of the art significantly
