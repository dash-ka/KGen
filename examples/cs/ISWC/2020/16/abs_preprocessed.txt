Providing a plethora of entitycentric information Knowledge Graphs have become a vital building block for a variety of intelligent applications
Indeed modern knowledge graphs like Wikidata already capture several billions of RDF triples yet modern knowledge graphs like Wikidata still lack a good coverage for most relations
On the other hand recent developments in NLP research show that neural language models can easily be queried for relational knowledge without requiring massive amounts of training data
In this work we leverage this idea by creating a hybrid query answering system on top of knowledge graphs in combination with the masked language model BERT to complete query results
we thus incorporate valuable structural from knowledge graphs with textual knowledge from language models to achieve high precision query results
we thus incorporate semantic information from knowledge graphs with textual knowledge from language models to achieve high precision query results
relation extraction which requires massive amounts of training data
Standard techniques for dealing with incomplete knowledge graphs are either relation extraction
relation extraction which requires massive amounts of knowledge graph embeddings
knowledge graph embeddings which have problems to succeed beyond simple baseline datasets
training data which have problems to succeed beyond simple baseline datasets
we hybrid system KnowlyBERT requires only small amounts of training data while outperforming stateoftheart techniques by boosting Our hybrid system KnowlyBERT precision by over 30