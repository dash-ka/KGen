question answering systems are generally modelled as a pipeline
a pipeline consisting of a sequence of steps
in such a pipeline entity linking is often the first step
several entity perform entity disambiguation
several entity linking models first
several entity perform span detection
in such models errors from the span detection phase cascade to later steps
result in a drop of overall accuracy
moreover lack of gold entity spans in training data is a limiting factor for span detector training
hence the movement towards endtoend entity linking models began where no separate span detection step is involved
the popular pointer network model which achieves competitive performance
in this work we present a novel approach to endtoend entity linking by applying the popular pointer network model
we demonstrate this in we evaluation over three datasets on the wikidata knowledge grapha setting where the data are given as description logic aboxes with possibly anonymised
the privacy policies are expressed using sets of concepts of the description logic el
we adapt existing approaches for privacypreserving publishing of linked data to a setting  formally existentially quantified  individuals
we provide a chacterization of compliance of such aboxes wrt el policies
we provide a chacterization of compliance of such aboxes show how optimal compliant anonymisations of aboxes can be computed
aboxes that are noncompliant
this work extends previous work on privacypreserving ontology publishing
privacypreserving ontology publishing in which a very restricted form of aboxes called instance stores had been considered
this work restricts the attention to compliance
the approach developed here can easily be adapted to the problem of computing optimal repairs of quantified aboxeslanguage models are both active research areas in machine learning and semantic web
knowledge graphs are both active research areas in machine learning and semantic web
while language models have brought great improvements for many downstream tasks on language models own language models are often combined with knowledge graphs
knowledge graphs providing additionally aggregated well structured knowledge
usually this is done by leveraging knowledge graphs to improve language models
but what happens if we use language models to improve knowledge graphs
but what happens if we turn this aroundtables in scientific papers contain a wealth of valuable knowledge for the scientific enterprise
to help the many of us who frequently consult this type of knowledge we present a new endtoend system to build a knowledge base  kb  from tables in scientific papers
to help the many of us who frequently consult this type of knowledge we present tab2know 
tab2know addresses the challenge of automatically interpreting the tables in papers
tab2know addresses of disambiguating the entities that the entities contain
to solve these problems we propose a pipeline
a pipeline that employs both statisticalbased classifiers
a pipeline that employs logicbased reasoning
first we pipeline applies weakly supervised classifiers to recognize the type of tables with an ontology specifically designed for our purpose
first we pipeline applies weakly supervised classifiers to recognize the type of columns with an ontology specifically designed for our purpose
first we pipeline applies weakly supervised classifiers to recognize the type of columns with the help of a data labeling system
first we pipeline applies weakly supervised classifiers to recognize the type of tables with the help of a data labeling system
then logicbased reasoning is used to link equivalent entities  via sameas links  in different tables
an empirical evaluation of our approach has returned satisfactory performance
our approach using a corpus of papers in the computer science domain
this suggests that our is a promising step to create a largescale kb of scientific knowledgethe sparse data have caused widespread attention to knowledge representation learning  krl  technology
large computational overhead in the use of largescale knowledge graphs have caused widespread attention to knowledge representation learning  krl  technology
although many krl models have been proposed to embed structure information many krl models ability to accurately represent newly added entities is significantly insufficient
although many krl models have been proposed to embed structure information many krl models ability to accurately represent entities with few relations is significantly insufficient
in some studies the introduction of textual information has partially solved this problem
however most existing textenhanced models ignore the relation mention information between deep semantic information between words
entities which is not optimized for long texts supplementary information like wikipedia
deep semantic information between words which is not optimized for long texts supplementary information like wikipedia
deep semantic information between sentences which is not optimized for long texts supplementary information like wikipedia
however most existing textenhanced models only consider the shallow description information of the entities
however most existing textenhanced models ignore the relation mention information between deep semantic information between sentences
however most existing textenhanced models ignore the relation mention information between entitiesa description that uniquely identifies an instance
the generation of referring expressions is one of the most extensively explored tasks in natural language generation where a description is to be provided
some recent approaches aim to discover referring expressions in knowledge graphs
to limit the search space existing approaches define quality measures based on the intuitiveness and simplicity of the discovered expressions
data linking an algorithm
data linking task reminer
in this paper we focus on referring expressions of interest for data
an algorithm tailored to automatically discover minimal referring expressions for all instances of a class in a knowledge graph
an algorithm tailored to automatically discover diverse referring expressions for all instances of a class in a knowledge graph
data linking present reminer
tools referring expressions for data
data linking substantially improve the results especially the recall without decreasing the precision
we experimentally demonstrate on several benchmark datasets that compared to existing data
existing data linking tools
datasets containing millions of facts
we also show that the reminer algorithm can scale to datasetscollective entity is a core natural language processing task
collective entity linking
a text exploiting existing relations between entities within the the entities of a knowledge base
a core natural language processing task which consists in jointly identifying the entities of a knowledge base
a knowledge base that are mentioned in a text
stateoftheart methods typically combine local scores accounting for the similarity between mentions with a global score
stateoftheart methods typically combine local scores accounting for the similarity between entities with a global score
a global score measuring the coherence of the set of selected entities
the latter relies on the structure of a the entities of a knowledge base the hyperlink graph of the graph of an rdf the entities of a knowledge base and yago to benefit from the precise semantics of relationships between entities
the latter relies on the structure of a the entities of a knowledge base the hyperlink graph of the graph of an rdf the entities of a knowledge base and basekb to benefit from the precise semantics of relationships between entities
the latter relies on the structure of a the entities of a knowledge base the hyperlink graph of wikipedia in most cases the entities of a knowledge base and basekb to benefit from the precise semantics of relationships between entities
the latter relies on the structure of a the entities of a knowledge base the hyperlink graph of wikipedia in most cases the entities of a knowledge base and yago to benefit from the precise semantics of relationships between entities
in this paper we devise a novel rdfbased entity relatedness measure for global scores with important properties a novel rdfbased entity relatedness measure for global scores has a clear semantics a novel rdfbased entity relatedness measure for global scores can be calculated at a novel rdfbased entity relatedness measure for global scores accounts for the transitive aspects of entity relatedness through existing  bounded length  property paths between entities in an rdf the entities of a knowledge base
in this paper we devise a novel rdfbased entity relatedness measure for global scores with important properties a novel rdfbased entity relatedness measure for global scores has a clear semantics a novel rdfbased entity relatedness measure for global scores can be calculated at reasonable computational cost through existing  bounded length  property paths between entities in an rdf the entities of a knowledge base
the collective entity linking task
further we experimentally show with yago that a novel rdfbased entity relatedness measure for global scores provides significant improvement over stateoftheart entity relatedness measures for the collective entity
further we experimentally show with basekb that a novel rdfbased entity relatedness measure for global scores provides significant improvement over stateoftheart entity relatedness measures for the collective entity
further we experimentally show on the tackbp2017 dataset that a novel rdfbased entity relatedness measure for global scores provides significant improvement over stateoftheart entity relatedness measures for the collective entitythe shapes constraint language allows for formalizing constraints over rdf data graphs
a shape groups a set of constraints
constraints that may be fulfilled by nodes in the rdf graph
we investigate the problem of containment between the shapes constraint language shapes
one shape is contained in a second shape if every graph node also meets the constraints of the second
every graph node meeting the constraints of the first shape
to decide shape containment we map the shapes constraint language shape graphs into description logic axioms such that shape containment can be answered by description logic reasoning
the shapes constraint language for which this approach becomes complete
we identify several increasingly tight syntactic restrictions of the shapes constraint language
the shapes constraint language for which this approach becomes soundthe cold start problem caused by collaborative filtering in recommender systems
to alleviate the cold start problem knowledge graphs are increasingly employed by many methods as auxiliary resources
however existing work can not capture the explicit longrange semantics between users meanwhile consider various connectivity between items
however existing work can not capture the explicit longrange semantics between items meanwhile consider various connectivity between items
existing work incorporated with knowledge graphs
rgrec which combines rule learning  for recommendation
rgrec which combines graph neural networks  for recommendation
in this paper we propose rgrec
rgrec which combines gnns  for recommendation
rgrec first maps items to corresponding entities in knowledge graphs
rgrec first adds users as new entities
then it automatically learns rules to model the explicit longrange semantics and captures the connectivity between entities by aggregation to better encode various information
we show the effectiveness of rgrec on three realworld datasets
particularly the combination of rule learning and gnns achieves substantial improvement compared to methods only using either of rule learning and gnnsin recent years misinformation on the web has become increasingly rampant
the research community has responded by proposing systems
the research community has responded by proposing challenges
systems which are beginning to be useful for  various subtasks of  detecting misinformation
challenges which are beginning to be useful for  various subtasks of  detecting misinformation
results which are not machine readable
deep learning techniques which are finetuned to specific domains are difficult to interpret results
however most proposed systems are based on deep learning techniques
deep learning techniques which are finetuned to specific domains are difficult to produce results
this limits this applicability and adoption as this can only be used by a select expert audience in very specific settings
credibility reviews that can be used to build networks of distributed bots
distributed bots that collaborate for misinformation detection
in this paper we propose an architecture based on a core concept of credibility reviews
the credibility reviews serve as building blocks to compose graphs of web content existing credibility signals reputation reviews of websites
the credibility reviews serve as building computed reviews
the credibility reviews serve as building blocks to compose graphs of web content existing credibility signals factchecked claims of websites
top of lightweight extensions to schemaorg providing generic nlp tasks for semantic similarity
we implement this architecture on top of lightweight extensions to services
top of lightweight extensions to services providing generic nlp tasks for stance detection
we implement this architecture on top of lightweight extensions to schemaorg
top of lightweight extensions to schemaorg providing generic nlp tasks for stance detection
top of lightweight extensions to services providing generic nlp tasks for semantic similarity
evaluations on existing datasets of socialmedia posts demonstrates several advantages over existing systems extensibility via provenance
evaluations on existing datasets of fake news demonstrates several advantages over existing systems explainability via provenance
evaluations on existing datasets of political speeches demonstrates several advantages over existing systems composability via provenance
evaluations on existing datasets of fake news demonstrates several advantages over existing systems transparency via provenance
evaluations on existing datasets of socialmedia posts demonstrates several advantages over existing systems composability via provenance
evaluations on existing datasets of socialmedia posts demonstrates several advantages over existing systems explainability via provenance
evaluations on existing datasets of political speeches demonstrates several advantages over existing systems transparency via provenance
evaluations on existing datasets of socialmedia posts demonstrates several advantages over existing systems transparency via provenance
evaluations on existing datasets of political speeches demonstrates several advantages over existing systems domainindependence via provenance
evaluations on existing datasets of fake news demonstrates several advantages over existing systems extensibility via provenance
evaluations on existing datasets of political speeches demonstrates several advantages over existing systems extensibility via provenance
evaluations on existing datasets of fake news demonstrates several advantages over existing systems domainindependence via provenance
evaluations on existing datasets of political speeches demonstrates several advantages over existing systems explainability via provenance
evaluations on existing datasets of socialmedia posts demonstrates several advantages over existing systems domainindependence via provenance
evaluations on existing datasets of fake news demonstrates several advantages over existing systems composability via provenance
furthermore we obtain competitive results without requiring finetuning and establish a new state of the art on the clef 18 checkthat
factuality taskthe hyponymhypernym relation is an essential element in the hypernym cooccurrence network
identifying the hypernym from a definition is an important task in natural language processing
identifying the hypernym from a definition is an important task in semantic analysis
while a public dictionary such as wordnet works for common words a public dictionary such as wordnet application in domainspecific scenarios is limited
existing tools for hypernym extraction either rely on specific semantic patterns
the word representation which all demonstrate certain limitations
existing tools for hypernym extraction either focus on the word representation
here we propose a method by combining both the syntactic structure in definitions given by the words part of speech
here we propose a method by combining both the syntactic structure in definitions given by the bidirectional gated recurrent unit network as the learning kernel
the output can be further tuned by including other features such as a words centrality in the hypernym cooccurrence network
wikipedia featuring definition with high regularity
the method is tested in the corpus from wikipedia
stackoverflow whose definition is usually irregular
the method is tested in the corpus from the corpus from stackoverflow
the method shows enhanced performance compared with other tools in both corpora
taken together our work not only also gives an example of utilizing syntactic structures to learn semantic relationships  source code available at https 
taken together our work not only also gives an example of utilizing syntactic structures to learn semantic relationships  source code available at githubcomrestanhypernymextraction 
taken together our work not only also gives an example of utilizing syntactic structures to learn semantic relationships  data available at https 
taken together our work not only provides a useful tool for hypernym extraction
taken together our work not only also gives an example of utilizing syntactic structures to learn semantic relationships  data available at githubcomrestanhypernymextraction in this paper we study the problem of information disclosure in ontologybased data access
a function that alters answers to users queries to avoid the disclosure of protected data
following previous work on controlled query evaluation we introduce the framework of policyprotected obda 
policyprotected obda  which extends ontologybased data access with data protection policies enforced through a censor
following previous work on controlled query evaluation we introduce the framework of policyprotected ontologybased data access 
policyprotected ontologybased data access  which extends ontologybased data access with data protection policies specified over the ontology
following previous work on controlled query evaluation we introduce a function
policyprotected ontologybased data access  which extends ontologybased data access with data protection policies enforced through a censor
policyprotected obda  which extends ontologybased data access with data protection policies specified over the ontology
we consider policyprotected obda systems in which the policies are denial constraints
show that query answering under censors in such a setting can be reduced to standard query answering in ontologybased data access  without data protection policies 
we consider policyprotected obda systems in which the ontology is expressed in extsc owl2ql
the basic idea of we approach is to compile the policies of a policyprotected obda system into the mapping of a standard ontologybased data access system
to this aim we analyze some notions of censor show that some notions of are not suited for the abovementioned compilation
to this aim we analyze some notions of censor proposed in the literature
to this aim we analyze some notions of censor provide a new definition of censor that enables the effective realization of our idea
our evaluated our technique over the npd benchmark for ontologybased data access
our have implemented our technique
our results are very promising and show that controlled query evaluation in ontologybased data access can be realized in the practice by using offtheshelf ontologybased data access enginesshort text categorization is an important task in many nlp applications such as sentiment analysis news feed categorization due to the sparsity and shortness of the text many traditional classification models perform poorly if many traditional classification models are directly applied to short text
moreover supervised approaches require large amounts of manually labeled data
data which is a timeconsuming task
data which is a labor intensive task
data which is a costly task
categorization approach which does not require any manually
a timeconsuming task proposes a weakly supervised short text categorization approach
a costly task proposes a weakly supervised short text categorization approach
a weakly supervised short text categorization approach labeled data
a labor intensive task proposes a weakly supervised short text categorization approach
a classification model based on a wide deep learning approach
a data labeling module which leverages an external knowledge base to compute probabilistic labels for a classification model
the proposed model consists of two main modules a data labeling module
data set
a data labeling module which leverages an external knowledge base to compute probabilistic labels for a given unlabeled training data
the effectiveness of the proposed method is validated via evaluation on multiple datasets
the experimental results show that the proposed approach outperforms unsupervised stateoftheart classification approaches
the experimental results show that the proposed approach achieves comparable performance to supervised approachesclustering entities over knowledge graphs is an asset for knowledge discovery
clustering entities over knowledge graphs is an asset for explorative search
clustering entities over knowledge graph embeddings have been intensively investigated mostly for clustering entities over knowledge graph completion
clustering entities over knowledge graph embeddings have potential also for entity clustering
however embeddings do not convey userinterpretable labels for clusters
however embeddings are latent
a novel approach that combines clustering entities over knowledge graph embeddings with rule mining methods to compute informative clusters of entities along with comprehensible explanations
this work presents excut a novel approach
comprehensible explanations are in the form of concise combinations of entity relations
excut jointly enhances the quality of entity clusters in an iterative manner
an iterative manner that interleaves the learning of embeddings
an iterative manner that interleaves the learning of rules
excut jointly enhances the quality of entity clusters explanations in an iterative manner
experiments on realworld clustering entities over knowledge graphs demonstrate the effectiveness of excut for discovering entity clusters explanations
experiments on realworld clustering entities over knowledge graphs demonstrate the effectiveness of excut for discovering highquality clustersontology matching aims at making different ontologies interoperable
ontology
while most approaches have addressed the generation of simple correspondences more expressiveness is required to better address the different kinds of ontology heterogeneities
this paper presents an approach for generating complex correspondences
complex correspondences that relies on the notion of competency questions for alignment
a alignment expresses the user knowledge aims at reducing the alignment scope
a alignment expresses the user knowledge needs in terms of alignment
the approach takes as input a set of cqas as sparql queries over the source ontology
the generation of correspondences is performed by matching the subgraph from the source alignment to the lexically similar surroundings of the instances from the target ontology
evaluation of the approach has been carried out on both synthetically generated
evaluation of the approach has been carried out on realword datasetsit is sometimes claimed that adding inferred axioms the inferred class hierarchy to an ontology can improve reasoning performance
it is sometimes claimed that adding inferred axioms the inferred class hierarchy to an ontology can improve an ontologys usability in practice
while such beliefs may have an effect on how ontologies are published there is no conclusive empirical evidence to support such beliefs
to develop an understanding of the impact of this practice both for tools we survey to what extent published ontologies in bioportal already contain such beliefs the inferred class hierarchy
to develop an understanding of the impact of this practice both for tools we survey to what extent published ontologies in bioportal already contain such beliefs most specific class assertions
to develop an understanding of the impact of this practice both for ontology curators we survey to what extent published ontologies in bioportal already contain such beliefs the inferred class hierarchy
to develop an understanding of the impact of this practice both for ontology curators we survey to what extent published ontologies in bioportal already contain such beliefs most specific class assertions
added inferred axioms from these sets
furthermore we investigate how added can affect the performance of standard reasoning tasks such as realisation
furthermore we investigate how added can affect the performance of standard reasoning tasks such as classification
we find that most specific class assertions are highly prevalent in published biomedical ontologies
we find that axioms from the the inferred class hierarchy
we reasoning evaluation indicates that added are likely to be inconsequential for reasoning performance
added inferred axioms
negative effects that seem to depend
positive effects that seem to depend
however we observe instances of on the used reasoner for a given ontology
however we observe instances positive effects on the used reasoner for a given ontology
however we observe instances negative effects on the used reasoner for a given ontology
a taskspecific analysis that determines whether desired effects are obtained
these results suggest that the practice of adding inferred axioms during the release process of ontologies should be subject to a taskspecific analysiswith the everincreasing number of rdfbased knowledge graphs the number of interconnections between these graphs sameas property has exploded
the number of interconnections between these graphs using the owl
moreover as several works indicate the identity as defined by the semantics of owl sameas could be too rigid
moreover as several works indicate the identity as defined by the semantics of owl sameas property is therefore often misused
indeed identity must be seen as contextdependent
poor quality data when using sameas inference capabilities
these facts lead to poor quality data
poor quality data when using the owl
therefore contextual identity could be a possible path to better quality knowledge
unlike classical identity with contextual identity only certain properties can be propagated between contextually identical entities
an approach based on sentence embedding
continuing this work on contextual identity we propose an approach to find semiautomatically a set of properties for a given identity context
a given identity context that can be propagated between contextually identical entities
quantitative experiments against a gold standard show that our approach achieved promising results
the desired results that meet users needs when querying more complete and accurate answers
the use cases provided
the desired results that meet users needs when querying a knowledge graph
besides the use cases demonstrate that identifying the properties helps users achieve the desired results
the properties that can be propagatedthe number and size of rdf knowledge graphs grows continuously
efficient storage solutions for these graphs are indispensable for these graphs use in real applications
we present such a storage solution
a storage solution dubbed tentris
sparse order3 tensors using a novel data structure
a novel data structure which we dub hypertrie
we solution represents rdf knowledge graphs as sparse order3 tensors
a novel data structure which we dub hypertrie
a novel data structure then uses tensor algebra to carry out sparql queries by mapping sparql operations to einstein summation
by being able to compute einstein summations efficiently tentris outperforms the opensource rdf storage solutions evaluated in our experiments by at least 18 times with respect to the average number of queries tentris can serve per second on three datasets of up to 1 billion triples
by being able to compute einstein summations efficiently tentris outperforms the commercial rdf storage solutions evaluated in our experiments by at least 18 times with respect to the average number of queries tentris can serve per second on three datasets of up to 1 billion triples
our code evaluation setup results supplementary material are provided at https 
our code evaluation setup results supplementary material are provided at tentrisdiceresearch
our code evaluation setup results the datasets are provided at https 
our code evaluation setup results the datasets are provided at tentrisdiceresearch
orgiswc2020knowledge graphs are becoming essential to represent
knowledge graphs still rely heavily on humanlycurated structured data
knowledge graphs are becoming essential to store the worlds knowledge
knowledge graphs still rely heavily on humanlycurated structured data
knowledge graphs are becoming essential to organize
information extraction tasks like disambiguating entities from unstructured text are key to automate knowledge graphs population
information extraction tasks like disambiguating relations from unstructured text are key to automate knowledge graphs population
however natural language processing methods alone can not guarantee the validity of the facts
the facts extracted
however natural language processing methods alone may introduce erroneous information into the knowledge graphs
an endtoend system that combines semantic knowledge with natural language processing methods to provide knowledge graphs population of novel facts from clustered news events
this work presents an endtoend system
an endtoend system that combines validation techniques with natural language processing methods to provide knowledge graphs population of novel facts from clustered news events
these two contributions are twofold first we present a novel method for including entitytype knowledge into a relation extraction model improving f1score over the baseline with tacred datasets
these two contributions are twofold first we present a novel method for including entitytype knowledge into a relation extraction model improving f1score over the baseline with typere datasets
second we increase the precision by adding data validation on top of the relation extraction method
population over aggregated news demonstrating increased data validity when performing online learning from unstructured web data
these two contributions are combined in an industrial pipeline for automatic knowledge graphs population over aggregated news
finally the aggregatednewsre datasets build to benchmark these two contributions are also published to foster future research in this field
finally the typere datasets build to benchmark these two contributions are also published to foster future research in this fieldrdf is a recent w3c recommendation language for validating rdf data
constraints that enforce particular shapes on an rdf graph
specifically shacl documents are collections of constraints
previous work on the topic did not consider the standard decision problems of satisfiability
previous work on the topic has provided practical results for the validation problem
containment which are crucial for verifying the feasibility of the constraints
previous work on the topic has provided theoretical results for the validation problem
previous work on the topic did not important for design purposes
satisfiability which are crucial for verifying the feasibility of the constraints
previous work on the topic did not important for optimization purposes
previous work on the topic did not consider the standard decision problems of containment
new window that precisely captures the semantics of containment
new window that precisely captures the semantics of shacl wrt satisfiability
in this paper we undertake a thorough study of the different features of shacl by providing a translation to a new firstorder language
a new firstorder language called open image in new window
we study the interaction of shacl features in this logic
we provide the detailed map of decidability results of the aforementioned decision problems for different shacl sublanguages
we provide the detailed map of complexity results of the aforementioned decision problems for different shacl sublanguages
notably we prove that both problems are undecidable for the full language
notably we prove that we present decidable combinations of interesting featuresthere is a variety of available approaches to learn graph node embeddings
one of their common underlying task is the generation of  biased  random walks that are then fed into representation learning techniques
some techniques generate biased random walks by using structural information
other approaches also rely on some form of semantic information
while the former are purely structural thus not fully considering knowledge available in semantically rich networks the latter require leverage node types
leverage node types that may not be available
while the former are purely structural thus not fully considering knowledge available in semantically rich networks the latter require complex inputs 
while the former are purely structural thus not fully considering knowledge available in semantically rich networks the latter require metapaths 
the goal of this paper is to overcome these limitations by introducing node embeddings via semantic proximity 
the goal of this paper is to overcome these limitations by introducing nesp 
node embeddings via semantic proximity  which features two main components
nesp  which features two main components
the first provides four different ways of biasing random walks by leveraging semantic relatedness between predicates
the second component focuses on existing  embeddings by leveraging the notion of semantic proximity
the second component focuses on refining  embeddings by leveraging the notion of semantic proximity
node embeddings imposing the embeddings of semantic neighboring nodes of a node to lie within a sphere of fixed radius
the second component iteratively refines an initial set of node embeddings
we discuss an extensive experimental evaluation and comparison with related workdespite their largescale coverage largescale coverage crossdomain knowledge graphs invariably suffer from inherent incompleteness and sparsity
link prediction can alleviate this by inferring a target entity given a source entity
link prediction can alleviate this by inferring a target entity given a query relation
recent embeddingbased approaches operate in an uninterpretable latent semantic vector space of relations while pathbased approaches operate in the symbolic space making the inference process explainable
recent embeddingbased approaches operate in an uninterpretable latent semantic vector space of entities while pathbased approaches operate in the symbolic space making the inference process explainable
however pathbased approaches typically consider static snapshots of the knowledge graphs severely restricting pathbased approaches applicability for evolving knowledge graphs with newly emerging entities
an inductive representation learning framework
to overcome this issue we propose an inductive representation
framework that is able to learn representations of previously unseen entities
our method finds reasoning paths between source entities thereby providing support evidence for the inferred link
our method finds reasoning paths between source entities thereby making the link prediction for unseen entities interpretable
our method finds reasoning paths between target entities thereby providing support evidence for the inferred link
our method finds reasoning paths between target entities thereby making the link prediction for unseen entities interpretablewe study the problem of structurebased entity alignment between knowledge graphs
the recent mainstream solutions for the problem of structurebased entity alignment between knowledge graphs apply knowledge graph embedding techniques to map entities into a vector space where the similarity between entities could be measured accordingly
the recent mainstream solutions for the problem of structurebased entity alignment between knowledge graphs apply knowledge graph embedding techniques to map entities into a vector space where the similarity between entities could be measured accordingly
these methods which are mostly based on transe and the problem of structurebased entity alignment between knowledge graphs variants
these methods which are mostly based on transe and the problem of structurebased entity alignment between knowledge graphs variants
however these methods treat relation triples in knowledge graphs independently
entities that are implicit in the surrounding
these methods which are mostly based on its variants
entities that are far apart which we call as longterm dependencies
entities that are implicit in multihop entities
an entity which we call as shortterm differences while the other is the dependencies between entities
as a result these methods fail to capture some advanced interactions between entities one is the differences between the twohop neighborhood of an entity
as a result these methods fail to capture some advanced interactions between entities one is the differences between the onehop neighborhood of an entity
these methods which are mostly based on transe
entity alignment using graph neural networks respectively
a novel approach learning to capture both the longterm dependencies in knowledge graphs for entity alignment
a novel approach learning to capture both the shortterm differences in knowledge graphs for entity alignment
entity alignment using selfattention mechanisms respectively
based on the above observations this paper proposes a novel approach
our empirical study shows the superiority of our model compared with the stateoftheart methods
our empirical study conducted on four couples of realworld datasetsan rdfsparql setting where similarity in an rdf graph is measured with respect to a set of attributes
techniques that support the efficient computation of multidimensional similarity joins in an rdfsparql setting
attributes selected in the sparql query
we propose techniques
while similarity joins have been studied in other contexts rdf graphs present unique challenges
ways in which a similarity can be implemented
we discuss how a similarity join operator can investigate ways
ways in which a similarity can be optimised
we discuss how a similarity join operator can be included in the sparql language
we devise experiments to compare three similarity join algorithms over two datasets
a postgresql extension that supports similarity
we results reveal that we techniques outperform dbsimjoin a postgresql extension joinsbased on semantic web technologies knowledge graphs help users to discover information of interest by using live sparql services
answerseekers often modify sparql queries repeatedly in a search session
answerseekers often examine intermediate results iteratively
in this context understanding user behaviors is critical for effective intention prediction
in this context understanding user behaviors is critical for query optimization
however user behaviors have not yet been researched systematically at the sparql session level
this paper reveals secrets of session level user search behaviors by conducting a comprehensive investigation over massive realworld sparql query logs
query changes made by users regarding structural features of sparql queries
in particular we thoroughly assess query changes
query changes made by users regarding datadriven features of sparql queries
findings which might be valuable to devise efficient sparql caching
findings which might be valuable to devise relaxation techniques in the future
findings which might be valuable to devise approximation
findings which might be valuable to devise autocompletion
findings which might be valuable to devise query suggestion
to illustrate the potentiality of we findings we employ an application example of how to use we findingsproviding a plethora of entitycentric information knowledge graphs have become a vital building block for a variety of intelligent applications
indeed modern knowledge graphs like wikidata already capture several billions of rdf triples yet modern knowledge graphs like wikidata still lack a good coverage for most relations
on the other hand recent developments in nlp research show that neural language models can easily be queried for relational knowledge without requiring massive amounts of training data
in this work we leverage this idea by creating a hybrid query answering system on top of knowledge graphs in combination with the masked language model bert to complete query results
we thus incorporate semantic information from knowledge graphs with textual knowledge from language models to achieve high precision query results
we thus incorporate valuable structural from knowledge graphs with textual knowledge from language models to achieve high precision query results
relation extraction which requires massive amounts of knowledge graph embeddings
standard techniques for dealing with incomplete knowledge graphs are either relation extraction
training data which have problems to succeed beyond simple baseline datasets
relation extraction which requires massive amounts of training data
knowledge graph embeddings which have problems to succeed beyond simple baseline datasets
we hybrid system knowlybert requires only small amounts of training data while outperforming stateoftheart techniques by boosting our hybrid system knowlybert precision by over 30while learning new technical material a user faces difficulty
new concepts for which a user does not have the necessary prerequisite knowledge
difficulty encountering new concepts
determining the right set of prerequisites is challenging because the right set of prerequisites involves multiple searches on the web
although a number of techniques have been proposed to retrieve prerequisites none of a number of techniques consider grouping prerequisites into interesting facets
preface that automatically determines interesting facets for a given concept of interest
to address this issue we have developed a system called preface and determines prerequisites for the concept and facet
a retrieval model that balances the tradeoff between the relevance of the facets diversity
a retrieval model that balances the tradeoff between the relevance of the facets
the key component of preface is a retrieval model
we achieve this by representing each facet as a language model and a large corpus of research papers and ranking research papers
ranking research papers using a riskminimization framework
a language model estimated using a domainspecific knowledge base
we evaluation of the results over a benchmark set of queries shows that preface retrieves better facets and prerequisites than stateoftheart facet extraction techniqueswe introduce an approach to semantically represent raster data in a semantic web graph
we introduce an approach to semantically query raster data in a semantic web graph
we extend the geosparql vocabulary to support raster data as a new type of geospatial data
we extend query language to support raster data as a new type of geospatial data
we define new filter functions
we illustrate we approach using several use cases on realworld data sets
finally we validate the feasibility of we approach
finally we describe a prototypical implementationkeyword search has been a prominent approach to querying knowledge graphs
for exploratory search tasks existing methods commonly extract subgraphs
subgraphs that are group steiner trees as answers
a group steiner tree that connects all the query keywords
however a group steiner tree may inevitably have a large graph structure in contrast to users favor to a compact answer
however a group steiner tree may not exist
however a group steiner tree may inevitably have a unfocused graph structure in contrast to users favor to a compact answer
in this paper we aim at generating relaxable subgraphs as answers
we require a computed subgraph to have a
a bounded diameter
therefore
we allow a computed subgraph to only connect an incomplete subset of query keywords
in this paper we aim at generating compact subgraphs as answers
we formulate a computed subgraph as a new combinatorial optimization problem of computing a minimally relaxed answer with a compactness guarantee
we present a novel bestfirst search algorithm
extensive experiments showed that we approach efficiently computed compact answers of high completenessdata has exponentially grown in the last years
knowledge graphs constitute powerful formalisms to integrate a myriad of existing data sources
transformation functions specified with functionbased mapping languages like rmlfno can be applied to overcome interoperability issues across heterogeneous data sources
transformation functions specified with functionbased mapping languages like funul can be applied to overcome interoperability issues across heterogeneous data sources
these mapping languages global adoption
however the absence of engines to efficiently execute these mapping languages hinders these
we propose an interpreter of functionbased mapping languages an interpreter of functionbased mapping languages relies on a set of lossless rewriting rules to materialize the execution of functions in initial steps of knowledge graph creation
we propose an interpreter of functionbased mapping languages an interpreter of functionbased mapping languages relies on a set of lossless rewriting rules to push down
we propose funmap  an interpreter of functionbased mapping languages relies on a set of lossless rewriting rules to materialize the execution of functions in initial steps of knowledge graph creation
we propose funmap  an interpreter of functionbased mapping languages relies on a set of lossless rewriting rules to push down
any functionbased mapping language that supports
although applicable to any functionbased mapping language joins between mapping rules funmap feasibility is shown on rmlfno
funmap reduces data redundancy and duplicates and converts rmlfno mappings into a set of equivalent rules executable on rmlcompliant engines
funmap reduces data redundancy and unused attributes and converts rmlfno mappings into a set of equivalent rules executable on rmlcompliant engines
we evaluate funmap performance over realworld testbeds from the biomedical domain
the results indicate that funmap reduces the execution time of rmlcompliant engines by up to a factor of 18 furnishing thus a scalable solution for knowledge graph creationkg  embedding has attracted more attention in recent years
knowledge graph  embedding has attracted more attention in recent years
most kg embedding models learn from timeunaware triples
however the inclusion of temporal information besides triples would further improve the performance of a kge model
in this regard we propose a temporal kg embedding model
a temporal kg embedding model which incorporates time information into entityrelation representations by using additive time series decomposition
in this regard we propose atise
moreover considering the temporal uncertainty during the evolution of entityrelation representations over time we map the representations of temporal kgs into the space of multidimensional gaussian distributions
the mean of each entityrelation embedding at a time step shows the current expected position whereas the current expected position covariance  which is temporally stationary  represents the current expected position temporal uncertainty
experimental results show that atise significantly outperforms the existing temporal kge models on link prediction over four temporal kgs
experimental results show that atise significantly outperforms the stateoftheart kge models temporal kge models on link prediction over four temporal kgsclientside sparql query processing enables evaluating queries over rdf datasets
rdf datasets published on the web without producing high loads on the data providers servers
clients to evaluate sparql queries over sparql queries have been proposed
triple pattern fragment servers provide means to publish highly available rdf data on the web
for clients to devise efficient query plans it is key to accurately estimate join cardinalities to appropriately place physical join operators
requests submitted to the server
efficient query plans that minimize both the number of requests
requests submitted to the overall execution time
collecting accurate statistics from remote sources is a challenging task
the metadata provided by the triple pattern fragment server
clients typically rely on the metadata
however
collecting finegrained statistics from remote sources is a challenging task
addressing this shortcoming we propose a cost query optimizer to devise efficient plans
efficient plans combining both cost of query plans
addressing this shortcoming we propose a robustbased query optimizer to devise efficient plans
efficient plans combining both robustness of query plans
addressing this shortcoming we propose crop to devise efficient plans
the idea of robustness is determining the impact of join cardinality estimation errors on the cost of a query plan
the idea of robustness is determining the impact of to avoid plans where this impact is very high
in our experimental study our show that our concept of robustness improves the efficiency of query plans
in our experimental study our show that our concept of robustness complements the cost model
additionally our show that our approach outperforms existing triple pattern fragment clients in terms of overall runtime and number of requeststhe web provides a plethora of contents about symptoms
the web provides a plethora of contents about diseases
the web provides a plethora of contents about treatments
most notably users turn to health forums to seek advice from peers with similar cases
most notably users turn to health forums to seek advice from doctors
however the benefit of forums mostly lies in community qa and browsing
expressive querying for patientcentric needs is poorly supported by search engines
this paper overcomes this issue by enriching user queries with judiciously chosen entities
this paper overcomes this issue by enriching user queries with classes from a large knowledge graph
candidate entities are extracted from the full text of user posts
a novel method that computes a focused entity core for query expansion
topical drift that would arise from picking all entities
to counter topical drift we devise eco
to counter topical drift we devise a novel method
experiments with contents from clinical trials demonstrate substantial gains that eco achieves over stateoftheart baselines
experiments with contents from health forums demonstrate substantial gains that eco achieves over stateoftheart baselinesworks on graphbased data management often focus either on graph query languages where there has been little work in trying to combine both approaches
works on knowledge graphs often focus either on graph query languages where there has been little work in trying to combine both approaches
works on graphbased data management often focus either on frameworks for graph analytics where there has been little work in trying to combine both approaches
works on knowledge graphs often focus either on frameworks for graph analytics where there has been little work in trying to combine both approaches
the appropriate data which is then possibly combined again with other data by means of a query language
the appropriate data which is then enriched with analytics
however many realworld tasks conceptually involve combinations of these approaches a graph query can be used to select the appropriate data
the appropriate data which is then possibly filtered again with other data by means of a query language
a language that is wellsuited for both graph querying
a language that is wellsuited for analytical tasks
in this paper we propose a language
we propose a minimalistic extension of sparql to allow for expressing analytical tasks over existing sparql infrastructure in particular we propose to extend sparql with recursive features
we propose a minimalistic extension of sparql to allow for expressing analytical tasks over existing sparql infrastructure in particular we propose to provide a formal syntax and semantics for our language
we show that our language can express key analytical tasks on graphs  in fact our language is turing complete 
moreover queries in this language can also be compiled into sequences of iterations of sparql update statements
we show how procedures in we can be implemented over offtheshelf sparql engines with a specialised client
a specialised client that can leverage database operations to improve the performance of queries
results for we implementation show that procedures for popular analytics currently run in seconds for selective subgraphs  our target usecase 
results for we implementation show that procedures for popular analytics currently run in minutes for selective subgraphs  our target usecase knowledge base question answering systems
knowledge base question are linking modules
knowledge base question are heavily dependent on relation extraction
however the task of linking relations from text to knowledge bases faces two primary challenges the lack of training data
however the task of extracting relations from text to knowledge bases faces two primary challenges the ambiguity of natural language of training data
however the task of linking relations from text to knowledge bases faces two primary challenges the ambiguity of natural language of training data
however the task of extracting relations from text to knowledge bases faces two primary challenges the lack of training data
semantic parsing using abstract meaning representation
framework which leverages semantic parsing
to overcome two primary challenges the lack of training data we present sling
to overcome two primary challenges the ambiguity of natural language of training data we present sling
to overcome two primary challenges the lack of training data we present a relation
semantic parsing using distant supervision
to overcome two primary challenges the ambiguity of natural language of training data we present a relation
a relation linking framework
multiple approaches that capture complementary signals such as linguistic cues from the knowledge base
semantic parsing using abstract meaning representation rrb
semantic parsing using distant supervision
semantic parsing using abstract meaning representation rrb
a relation integrates multiple approaches
framework which leverages semantic parsing
multiple approaches that capture complementary signals such as rich semantic representation from the knowledge base
multiple approaches that capture complementary signals such as information from the knowledge base
a relation linking framework
sling integrates multiple approaches
relation linking using qald7
relation linking using three kbqa datasets
the experiments on relation demonstrate that the proposed approach achieves stateoftheart performance on all benchmarks
relation linking using lcquad 10
relation linking using qald9to quantify the impact of realworld events on online communities
automatically detecting meaning changes  of single words has recently received strong research attention
automatically detecting semantic shifts  of single words has recently received strong research attention
various measures which are intended to capture the somewhat elusive and undifferentiated concept of semantic shift
these computational approaches have introduced various measures
on the other hand there is a well established distinction in linguistics between a words paradigmatic 
on the other hand there is a longstanding distinction in syntagmatic associations 
on the other hand there is a longstanding distinction in linguistics between terms 
terms that typically occur next to a word
on the other hand there is a longstanding distinction in terms 
on the other hand there is a well established distinction in linguistics between terms 
terms that can replace a word
on the other hand there is a well established distinction in terms 
on the other hand there is a longstanding distinction in linguistics between a words paradigmatic 
on the other hand there is a well established distinction in syntagmatic associations 
a method that captures a measures sensitivity for paradigmatic andor syntagmatic  association  shifts
in this work we join these two lines of research by introducing a method
for this purpose we perform synthetic distortions on textual corpora
textual corpora that in turn induce shifts in word embeddings
word embeddings trained on them
we find that the local neighborhood is sensitive to paradigmatic
the global semantic displacement is sensitive to syntagmatic shift in word embeddings
words that undergo syntagmatic shift both separately
the newly validated syntagmatic measures on amazon reddit 
words that undergo paradigmatic shift both at the same time
the newly validated paradigmatic measures on three realworld datasets 
words that undergo syntagmatic shift both at the same time
the newly validated syntagmatic measures on amazon wikipedia 
the newly validated syntagmatic measures on three realworld datasets 
the newly validated paradigmatic measures on amazon reddit 
the newly validated paradigmatic measures on amazon wikipedia 
by applying the newly we find examples of words
words that undergo paradigmatic shift both separately
with this more nuanced understanding of semantic shift on word embeddings we hope to analyze a similar concept of semantic shift on rdf graph embeddings in the futurethe rdftotext task has recently gained substantial attention due to continuous growth of linked data
neural models which are now able to convert a set of rdf triples into text in an endtoend style with promising results
in contrast to traditional pipeline models recent studies have focused on neural models
however english is the only language widely targeted
a multilingual graphbased neural model that verbalizes rdf data to russian
a multilingual graphbased neural model that verbalizes rdf data to english
a multilingual graphbased neural model that verbalizes rdf data to german
we address this research gap by presenting nabu a multilingual graphbased neural model
an encoder inspired by graph attention networks as decoder
a multilingual graphbased neural model that verbalizes rdf data to german russian
an encoder inspired by a transformer as decoder
a multilingual graphbased neural model that verbalizes rdf data to english is based on an encoderdecoder architecture
nabu a multilingual graphbased neural model uses an encoder
we approach relies on the fact that knowledge graphs are languageagnostic
knowledge graphs hence can be used to generate multilingual text
we evaluate nabu in multilingual settings on standard benchmarking webnlg datasets
we evaluate nabu in monolingual settings on standard benchmarking webnlg datasets
a multilingual graphbased neural model that verbalizes rdf data to russian
a multilingual graphbased neural model that verbalizes rdf data to english
a multilingual graphbased neural model that verbalizes rdf data to german
we results show that nabu a multilingual graphbased neural model achieves consistent results across all languages on the multilingual scenario with 5604 bleu
we results show that nabu a multilingual graphbased neural model outperforms stateoftheart approaches on english with 6621 bleuduring the last few years several knowledge graph embedding models have been devised in order to handle machine learning problems for knowledge graphs
some of the models such as symmetry show lower performance in practice than those not allowing to infer those patterns
the models which were proven to be capable of inferring relational patterns
some of the models such as transitivity show lower performance in practice than those not allowing to infer those patterns
it is often unknown what factors contribute to such performance differences among kge models in the inference of particular patterns
we develop the concept of a solution space as a factor to infer relational patterns
a factor that has a direct influence on the practical performance of knowledge graph embedding models
a factor that has knowledge graph embedding models capability
we showcase the effect of solution space on a newly proposed model
a newly proposed model dubbed spacess
we evaluate we model against stateoftheart models on a set of standard benchmarks namely freebase
we describe the theoretical considerations behind the solution space
we evaluate we model against stateoftheart models on a set of standard benchmarks namely wordnet
